<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Activation Function - JasminePP</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="JasminePP"><meta name="msapplication-TileImage" content="/img/avatar.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="JasminePP"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="I. What is an activation function?In contact with the deep learning, especially in the neural network, we will find that in each layer of the neural network output will use a function (such as sigmoid"><meta property="og:type" content="blog"><meta property="og:title" content="Activation Function"><meta property="og:url" content="http://example.com/2023/08/15/Activation/"><meta property="og:site_name" content="JasminePP"><meta property="og:description" content="I. What is an activation function?In contact with the deep learning, especially in the neural network, we will find that in each layer of the neural network output will use a function (such as sigmoid"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*ZafDv3VUm60Eh10OeJu1vw.png"><meta property="article:published_time" content="2023-08-14T23:00:00.000Z"><meta property="article:modified_time" content="2023-09-24T14:28:13.937Z"><meta property="article:author" content="Tianhao Peng"><meta property="article:tag" content="deeplearning"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://cdn-images-1.medium.com/max/1600/1*ZafDv3VUm60Eh10OeJu1vw.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2023/08/15/Activation/"},"headline":"Activation Function","image":["https://cdn-images-1.medium.com/max/1600/1*ZafDv3VUm60Eh10OeJu1vw.png"],"datePublished":"2023-08-14T23:00:00.000Z","dateModified":"2023-09-24T14:28:13.937Z","author":{"@type":"Person","name":"Tianhao Peng"},"publisher":{"@type":"Organization","name":"JasminePP","logo":{"@type":"ImageObject","url":{"text":"Tianhao'Site"}}},"description":"I. What is an activation function?In contact with the deep learning, especially in the neural network, we will find that in each layer of the neural network output will use a function (such as sigmoid"}</script><link rel="canonical" href="http://example.com/2023/08/15/Activation/"><link rel="icon" href="/img/avatar.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="jasmine"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="jasmine"></script><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Tianhao&#039;Site</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://cdn-images-1.medium.com/max/1600/1*ZafDv3VUm60Eh10OeJu1vw.png" alt="Activation Function"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-14T23:00:00.000Z" title="15/08/2023, 00:00:00">2023-08-15</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></span><span class="level-item">12 minutes read (About 1761 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Activation Function</h1><div class="content"><h3 id="I-What-is-an-activation-function"><a href="#I-What-is-an-activation-function" class="headerlink" title="I. What is an activation function?"></a>I. What is an activation function?</h3><p>In contact with the deep learning, especially in the neural network, we will find that in each layer of the neural network output will use a function (such as sigmoid, tanh, Relu, etc.) on the results of the operation, this function is the <strong>activation function</strong> . So <em>why do we need to add an activation function? And what are the problems if you don’t add it?</em></p>
<span id="more"></span>

<p>Firstly, we know that neural networks mimic the workings of human neurons, and an activation function is a function added to an artificial neural network to help it <strong>learn complex patterns</strong> in the data. In a neuron, the input is weighted and summed in a series of weighted sums and then applied to another function, which is the activation function in this case. Similar to the neuron-based model of the human brain, the activation function ultimately determines whether or not a signal is delivered and what is to be fired to the next neuron. In an artificial neural network, the activation function of a node defines the output of that node for a given input or set of inputs. </p>
<p>Activation functions can be classified as <strong>linear activation functions</strong> (linear equations controlling the mapping of inputs to outputs, e.g., f(x) &#x3D; x, etc.) and <strong>nonlinear activation functions</strong> (nonlinear equations controlling the mapping of inputs to outputs, e.g., Sigmoid, Tanh, ReLU, LReLU, PReLU, Swish, etc.)</p>
<h4 id="why-do-we-need-to-use-the-activation-function"><a href="#why-do-we-need-to-use-the-activation-function" class="headerlink" title="why do we need to use the activation function?"></a>why do we need to use the activation function?</h4><p>Because the input and output of each layer of a neural network is a linear summation process, the output of the next layer is just a linear transformation of the input function of the previous layer, so if there is no activation function, then no matter how complex the neural network you constructed, how many layers, the final output is a linear combination of inputs, and the <strong>purely linear combination is not capable of solving more complex problems</strong>. After introducing the activation function, we will find that the common activation function is non-linear, so it also introduces a non-linear element to the neuron, which allows the neural network to approximate any other non-linear function, which can make the neural network applied to more non-linear models.</p>
<p><img src="/2023/08/15/Activation/img1.png"></p>
<p>In general, the activation function is an important part in neurons, and in order to enhance the representation and learning ability of the network, the activation functions of neural networks are nonlinear and usually have the following properties:</p>
<ul>
<li><strong>Continuous and differentiable</strong> (allowing for non-derivable at a few points), a differentiable activation function can be used directly to learn the network parameters using numerical optimisation methods;</li>
<li>The activation function and its derivatives should be as <strong>simple</strong> as possible; too much complexity is not conducive to improving the network computation rate;</li>
<li>The <strong>value domain</strong> of the derivative of the activation function should be in a suitable interval, not too big or too small, otherwise it will affect the efficiency and stability of training</li>
</ul>
<h4 id><a href="#" class="headerlink" title></a></h4><h3 id="II-Common-activation-functions"><a href="#II-Common-activation-functions" class="headerlink" title="II. Common activation functions"></a>II. Common activation functions</h3><ol>
<li><h5 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h5><p>Sigmoid function, also called <strong>Logistic function</strong>, is used for the output of the hidden layer neuron, the value range is (0,1), it can map a real number to the interval (0,1), can be used to do binary classification, the effect is better when the feature difference is more complex or the difference is not particularly large. sigmoid is a very common activation function, the function expression is as follows:</p>
<p><img src="/2023/08/15/Activation/img2.png"></p>
</li>
</ol>
<p>​       The image resembles an S-curve</p>
<p><img src="/2023/08/15/Activation/img3.jfif"></p>
<p><strong>The condition to use the Sigmoid activation function:</strong></p>
<ul>
<li><strong>The Sigmoid function has an output range of 0 to 1</strong>. Since the output value is limited to 0 to 1, it normalises the output of each neuron;</li>
<li><strong>A model used to take the predicted probability as an output</strong>. The Sigmoid function is well suited since the probabilities take values from 0 to 1;</li>
<li><strong>Gradient smoothing</strong> to avoid ‘jumpy’ output values;</li>
<li><strong>The function is differentiable</strong>. This means that the slope of the sigmoid curve can be found for any two points;</li>
<li><strong>Clear prediction</strong>, i.e. very close to 1 or 0.</li>
</ul>
<p><strong>Shortcomings of the Sigmoid activation function:</strong></p>
<ul>
<li><strong>The gradient vanishes</strong>: note that the rate of change of the sigmoid function flattens out as it approaches 0 and 1, i.e. the gradient of the sigmoid tends to 0. When a neural network is backpropagated using the sigmoid activation function, neurons with outputs close to 0 or 1 have their gradient approaching 0. These neurons are called <strong>saturated neurons</strong>. Therefore, the weights of these neurons <u>are not updated</u>. In addition, the weights of the neurons connected to such neurons are also updated very slowly. The problem is called <strong>gradient vanishing</strong>. Thus, imagine a large neural network containing Sigmoid neurons, many of which are saturated, then the network cannot perform backpropagation.</li>
<li><strong>Non-zero centred</strong>: Sigmoid outputs are not zero centred, the output is constantly greater than 0. Non-zero centred outputs cause the inputs of the neurons in the subsequent layer to undergo a Bias Shift, which further slows down the convergence of the gradient descent.</li>
<li><strong>Computationally expensive</strong>: The exp() function is computationally expensive and slower to run on a computer than other nonlinear activation functions.</li>
</ul>
<ol start="2">
<li><h5 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h5></li>
</ol>
<p>  The Tanh activation function is also called <strong>hyperbolic tangent activation function</strong>. Similar to the Sigmoid function, the Tanh function uses the truth value, but the Tanh function compresses it into the interval -1 to 1. Unlike the Sigmoid, the output of the Tanh function is centred on zero, since the interval is between -1 and 1.</p>
<p>Function expressions:</p>
<p><img src="/2023/08/15/Activation/img4.png"></p>
<p>We can find that the Tanh function can be viewed as <strong>a zoomed and shifted Logistic function</strong> with a value range of (-1, 1). <strong>tanh</strong> is related to the <strong>sigmoid</strong> as follows:</p>
<p><img src="/2023/08/15/Activation/img5.gif"></p>
<p>The image of the tanh activation function is also S-shaped. As a hyperbolic tangent function, the tanh function has a relatively similar curve to the sigmoid function. But it has some <strong>advantages</strong> over the sigmoid function.</p>
<p>In practice, the Tanh function is used in preference to the Sigmoid function. Negative inputs are treated as negative values, zero input values are mapped close to zero, and positive inputs are treated as positive values:</p>
<ul>
<li><p>When the input is large or small, the output is almost smooth and has a small gradient, which is not favourable for weight updating. The difference between the two is in the output spacing, tanh has an output spacing of 1 and the whole function is centred on 0, which is better than a sigmoid function;</p>
</li>
<li><p>In tanh graph, negative inputs will be strongly mapped as negative and zero inputs are mapped as close to zero.</p>
</li>
</ul>
<p><strong>Shortcomings of tanh</strong>:</p>
<ul>
<li><p>Similar to the sigmoid, the tanh function suffers from <strong>vanishing gradients</strong>, and thus ‘kills’ gradients at saturation (when x is large or small).</p>
<p>Note: <em>In general binary classification problems, the tanh function is used in the <strong>hidden layer</strong> and the sigmoid function is used in the output layer, but this is not fixed and needs to be adapted to the particular problem.</em></p>
</li>
</ul>
<ol start="3">
<li><h5 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h5></li>
</ol>
<p>  ReLU function, also known as <strong>Rectified Linear Unit (RLU)</strong>, is a segmented linear function, which makes up for the gradient vanishing problem of the sigmoid function as well as the tanh function, and is widely used in current deep neural networks.The ReLU function is essentially a <strong>ramp function</strong>, and the formula and the function image are as follows:</p>
<p>  <img src="/2023/08/15/Activation/img6.png"></p>
<p>ReLU function is a popular activation function in deep learning, compared with sigmoid function and tanh function, it has the following <strong>advantages</strong>: </p>
<ul>
<li>When the input is positive, the derivative is 1, which <strong>improves the gradient vanishing problem</strong> to some extent and <strong>accelerates the convergence of gradient descent</strong>;</li>
<li><strong>It is much faster to compute</strong>. there are only linear relationships in the ReLU function, so it is faster to compute than sigmoid and tanh.</li>
<li><strong>Considered biologically plausible</strong>, e.g., unilateral inhibition, wide excitability bounds (i.e., excitability can be very high).</li>
</ul>
<p><strong>Shortcomings of the ReLU function:</strong></p>
<ul>
<li><p><strong>Dead ReLU problem</strong>. <em>When the input is negative, ReLU fails completely</em>, which is not a problem during forward propagation. Some regions are sensitive and some are not. <strong>But during back propagation, if the input is negative, the gradient will be completely zero;</strong></p>
</li>
<li><p><strong>Not zero-centred</strong>: Similar to the Sigmoid activation function, the output of the ReLU function is not zero-centred, the output of the ReLU function is zero or positive, introducing a bias offset to the neural network in the later layers can affect the efficiency of gradient descent.</p>
</li>
</ul>
<ol start="4">
<li><strong>Leaky ReLU</strong><br>To solve the problem of vanishing gradient in the ReLU activation function when x &lt; 0, we use <strong>Leaky ReLU</strong> - a function that tries to fix the dead ReLU problem. Let’s take a closer look at Leaky ReLU.</li>
</ol>
<p>The function expression and image are shown below:</p>
<p><img src="/2023/08/15/Activation/img7.png"></p>
<p><img src="/2023/08/15/Activation/img8.png"></p>
<h5 id="Why-would-using-Leaky-ReLU-work-better-than-ReLU"><a href="#Why-would-using-Leaky-ReLU-work-better-than-ReLU" class="headerlink" title="Why would using Leaky ReLU work better than ReLU?"></a>Why would using Leaky ReLU work better than ReLU?</h5><ul>
<li>Leaky ReLU adjusts for the problem of negative zero gradients by giving a negative input (0.01x) to a very small linear component of x, <strong>which gives a positive gradient of 0.1 when x &lt; 0</strong>. This function alleviates the dead ReLU problem to some extent.</li>
<li>leak helps to <strong>extend the range of the ReLU function</strong>, usually with a value of 0.01 or so;</li>
<li>The function range of Leaky ReLU is (negative infinity to positive infinity).</li>
</ul>
<p>Although Leaky ReLU has all the characteristics of ReLU activation function (such as computationally efficient, fast convergence, not saturated in the positive region), <strong>it does not completely prove that Leaky ReLU is always better than ReLU in practice</strong>.</p>
<ol start="5">
<li><strong>Softmax</strong><br>Softmax is an activation function for <strong>multi-class classification</strong> problems where class membership is required for more than two class labels. For any real vector of length K, Softmax can compress it into a real vector of length K with values in the range (0, 1) and the sum of the elements in the vector is 1.</li>
</ol>
<p>The expression of the function is as follows:</p>
<p><img src="/2023/08/15/Activation/img9.png"></p>
<p><img src="/2023/08/15/Activation/img10.jpg"></p>
<p>Softmax is different from the normal max function: the max function only outputs the maximum value, but Softmax ensures that smaller values have a smaller probability and are not discarded outright. We can think of it as a probabilistic or “soft” version of the argmax function.</p>
<p>The denominator of the Softmax function combines all the factors of the original output value, which means that the various probabilities obtained by the Softmax function are related to each other.</p>
<p><strong>Shortcomings of the Softmax activation function:</strong></p>
<ul>
<li><strong>It is not differentiable at zero</strong>;</li>
<li><strong>Negative inputs have a gradient of zero</strong>, which means that for activations in that region, the weights are not updated during backpropagation, thus creating dead neurons that never activate</li>
</ul>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/deeplearning/">deeplearning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2023/08/30/ModelCompressionPruning/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Model Compression - Pruning</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/07/30/BasicsofNeuralNetworksII/"><span class="level-item">Basics of Neural NetworksII</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">32</span></span></a></li><li><a class="level is-mobile" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Machine-learning/"><span class="level-start"><span class="level-item">Machine learning</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/Video-Compression/"><span class="level-start"><span class="level-item">Video Compression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-29T23:00:00.000Z">2023-09-30</time></p><p class="title"><a href="/2023/09/30/KD02/">Knowledge DistillationII</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-14T23:00:00.000Z">2023-09-15</time></p><p class="title"><a href="/2023/09/15/KD01/">Knowledge DistillationI</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-29T23:00:00.000Z">2023-08-30</time></p><p class="title"><a href="/2023/08/30/ModelCompressionPruning/">Model Compression - Pruning</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-14T23:00:00.000Z">2023-08-15</time></p><p class="title"><a href="/2023/08/15/Activation/">Activation Function</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-29T23:00:00.000Z">2023-07-30</time></p><p class="title"><a href="/2023/07/30/BasicsofNeuralNetworksII/">Basics of Neural NetworksII</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Array/"><span class="tag">Array</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BackTracking/"><span class="tag">BackTracking</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BinarySearchTree/"><span class="tag">BinarySearchTree</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BinaryTree/"><span class="tag">BinaryTree</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C-C/"><span class="tag">C/C++</span><span class="tag">32</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Combination/"><span class="tag">Combination</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CrossValidation/"><span class="tag">CrossValidation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DecisionTree/"><span class="tag">DecisionTree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepLearning/"><span class="tag">DeepLearning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DoublePointers/"><span class="tag">DoublePointers</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DynamicProgramming/"><span class="tag">DynamicProgramming</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeatureExtraction/"><span class="tag">FeatureExtraction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeaturePreprocessing/"><span class="tag">FeaturePreprocessing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GreedyAlgorithm/"><span class="tag">GreedyAlgorithm</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Iteration/"><span class="tag">Iteration</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KNN/"><span class="tag">KNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KnowledgeDistillation/"><span class="tag">KnowledgeDistillation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinearRegression/"><span class="tag">LinearRegression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinkedList/"><span class="tag">LinkedList</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linkedlist/"><span class="tag">Linkedlist</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NaiveBayes/"><span class="tag">NaiveBayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pruning/"><span class="tag">Pruning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Resursion/"><span class="tag">Resursion</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RidgeRegression/"><span class="tag">RidgeRegression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/String/"><span class="tag">String</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TraversalOfBinaryTree/"><span class="tag">TraversalOfBinaryTree</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VideoCompression/"><span class="tag">VideoCompression</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deeplearning/"><span class="tag">deeplearning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/loop-invariant/"><span class="tag">loop invariant</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machinelearning/"><span class="tag">machinelearning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/model-compression/"><span class="tag">model compression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/modelcompression/"><span class="tag">modelcompression</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quantization/"><span class="tag">quantization</span><span class="tag">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="Tianhao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Tianhao</p><p class="is-size-6 is-block">a MATH and CS student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Bristol, United Kingdom</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">46</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/jasminepp/jasminepp.github.io" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/jasminepp/jasminepp.github.io"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">September 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Tianhao&#039;Site</a><p class="is-size-7"><span>&copy; 2023 Tianhao Peng</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="jasmine"></script><script src="jasmine"></script><script src="jasmine" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="jasmine" defer></script><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>