<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Model Compression - Pruning - JasminePP</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="JasminePP"><meta name="msapplication-TileImage" content="/img/avatar.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="JasminePP"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Why is the Neural Network Pruning (Why?)Much of the success of convolutional neural networks stems from larger and larger network architectures and more and more neurons. While this allows neural netw"><meta property="og:type" content="blog"><meta property="og:title" content="Model Compression - Pruning"><meta property="og:url" content="http://example.com/2023/09/15/ModelCompressionPruning/"><meta property="og:site_name" content="JasminePP"><meta property="og:description" content="Why is the Neural Network Pruning (Why?)Much of the success of convolutional neural networks stems from larger and larger network architectures and more and more neurons. While this allows neural netw"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://miro.medium.com/max/1400/1*rw2zAHw9Xlm7nSq1PCKbzQ.png"><meta property="article:published_time" content="2023-09-14T23:00:00.000Z"><meta property="article:modified_time" content="2023-10-02T20:08:38.324Z"><meta property="article:author" content="Tianhao Peng"><meta property="article:tag" content="Pruning"><meta property="article:tag" content="DeepLearning"><meta property="article:tag" content="modelcompression"><meta property="article:tag" content="Optimization"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://miro.medium.com/max/1400/1*rw2zAHw9Xlm7nSq1PCKbzQ.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2023/09/15/ModelCompressionPruning/"},"headline":"Model Compression - Pruning","image":["https://miro.medium.com/max/1400/1*rw2zAHw9Xlm7nSq1PCKbzQ.png"],"datePublished":"2023-09-14T23:00:00.000Z","dateModified":"2023-10-02T20:08:38.324Z","author":{"@type":"Person","name":"Tianhao Peng"},"publisher":{"@type":"Organization","name":"JasminePP","logo":{"@type":"ImageObject","url":{"text":"Jasmine'Site"}}},"description":"Why is the Neural Network Pruning (Why?)Much of the success of convolutional neural networks stems from larger and larger network architectures and more and more neurons. While this allows neural netw"}</script><link rel="canonical" href="http://example.com/2023/09/15/ModelCompressionPruning/"><link rel="icon" href="/img/avatar.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="jasmine"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="jasmine"></script><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Jasmine&#039;Site</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://miro.medium.com/max/1400/1*rw2zAHw9Xlm7nSq1PCKbzQ.png" alt="Model Compression - Pruning"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-09-14T23:00:00.000Z" title="15/09/2023, 00:00:00">2023-09-15</time></span><span class="level-item"><a class="link-muted" href="/categories/Deep-Learning/">Deep Learning</a></span><span class="level-item">7 minutes read (About 1110 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Model Compression - Pruning</h1><div class="content"><h5 id="Why-is-the-Neural-Network-Pruning-Why"><a href="#Why-is-the-Neural-Network-Pruning-Why" class="headerlink" title="Why is the Neural Network Pruning (Why?)"></a>Why is the Neural Network Pruning (Why?)</h5><p>Much of the success of convolutional neural networks stems from larger and larger network architectures and more and more neurons. While this allows neural networks to perform better in many tasks, it also puts more demands on computer hardware:  <strong>more arithmetic power</strong>, <strong>more storage space</strong>. However,  this makes the cost of using neural networks skyrocket, a problem that cannot be ignored in edge devices.</p>
<span id="more"></span>

<p>Model compression looks to <strong>reduce model size</strong> while minimising the loss of accuracy. Neural network pruning achieves model compression by removing <strong>unnecessary weights</strong> (e.g., weights below a certain threshold) and <strong>neurons</strong> (ReLU, Dropout, and other nonlinear functions that bring about zero-setting operations, etc.)</p>
<h4 id="What-is-the-Neural-Network-Pruning-What"><a href="#What-is-the-Neural-Network-Pruning-What" class="headerlink" title="What is the Neural Network Pruning (What?)"></a>What is the Neural Network Pruning (What?)</h4><p><strong>Pruning</strong> is the process of removing redundant <em><u>channels</u></em>, <em><u>filters</u></em>, <em><u>neurons</u></em>, and <u><em>layers</em></u> from a network in order to obtain a lighter weight network without compromising performance.</p>
<p><img src="/2023/09/15/ModelCompressionPruning/img1.png"></p>
<h4 id="Pruning-classification"><a href="#Pruning-classification" class="headerlink" title="Pruning classification"></a>Pruning classification</h4><ul>
<li>Depending on the type of network elements, it can be classified as <strong>neuron pruning</strong> and <strong>connection pruning</strong>;</li>
<li>Depending on whether the network structure is changed before and after pruning, it can be classified as <strong>structured pruning</strong> and <strong>unstructured pruning</strong>;</li>
<li>Depending on whether there is pruning in the inference phase, it can be classified as <strong>static pruning</strong> and <strong>dynamic pruning</strong>;</li>
</ul>
<p><strong>Structured pruning</strong> can include neuron pruning and connection pruning, but is changing the structure of the network before pruning. <strong>Unstructured pruning</strong> <em>does not</em> change the structure of the network, but rather changes the branch weights to 0, such that a branch with a weight of 0 has no effect on the whole network, and thus is equivalent to subtracting that branch. If it is observed that a branch with a weight of 0 at some iterations has a large effect on the current iteration (affecting performance), the zero weight caused by unstructured pruning can be <strong>dynamically restored</strong> to provide network performance. </p>
<h4 id="1-Structured-pruning"><a href="#1-Structured-pruning" class="headerlink" title="1. Structured pruning"></a>1. Structured pruning</h4><ul>
<li><h4 id="neuron-pruning"><a href="#neuron-pruning" class="headerlink" title="neuron pruning"></a>neuron pruning</h4></li>
</ul>
<p>Neurons are the basic nodes of a neural network. Neuron pruning is to cut out the network nodes and their related connections together that do not have much effect on the inputs and outputs, so this pruning method will change the structure of the network belongs to <strong>structural pruning</strong>.</p>
<p>Neuron pruning based on neuron pruning more coarse-grained pruning, such as <strong>channel</strong>, <strong>filter</strong>, and <strong>layer pruning</strong>, also change the structure of the network will change the structure of the network.</p>
<h5 id="channel-wise-pruning"><a href="#channel-wise-pruning" class="headerlink" title="- channel-wise pruning"></a>- channel-wise pruning</h5><p>Suppose we want to try to <strong>reduce the computation</strong> of a particular individual conv layer. The following information is known about it:</p>
<h5 id="Shape-of-Tensor"><a href="#Shape-of-Tensor" class="headerlink" title="Shape of  Tensor:"></a>Shape of  Tensor:</h5><p><em><strong>Input feature map X</strong></em>: <em>N x c x IH X IW</em>,  where <em>N</em> is the batch size, <em>c</em> is the number of input feature maps, and <em>IH</em> and <em>IW</em> are the length and width of the input feature map respectively<br><em><strong>Convolutional filters W</strong></em>: <em>n x c x Kh x Kw</em>, where <em>n</em> and <em>c</em> are the number of input and output feature map channels for this conv layer, and <em>Kh</em> and <em>Kw</em> are the length and width of the conv kernel</p>
<p><em><strong>Output feature map Y</strong></em>:  <em>N x n x OH X OW</em>, where <em>N</em> is the batch size, <em>n</em> is the number of output feature maps, and <em>OH</em> and <em>OW</em> are the length and width of the output feature map respectively.</p>
<p>The problem we want to solve is how to reduce some channel information of <em><strong>X</strong></em> while ensuring that the overall information of <em><strong>Y</strong></em> is still not lost too much. The following equation can represent this <strong>optimisation</strong> problem.</p>
<p><img src="/2023/09/15/ModelCompressionPruning/img2.png"></p>
<h4 id="Step-by-step-implementation-of-the-channel-pruning"><a href="#Step-by-step-implementation-of-the-channel-pruning" class="headerlink" title="Step-by-step implementation of the channel pruning"></a>Step-by-step implementation of the channel pruning</h4><p>Relaxation of the original optimisation problem<br>The <strong>L0</strong> optimisation problem discussed above is essentially an <strong>NP-hard</strong> problem. We give it some more constraints and thus turn it into an <strong>L1</strong> norm optimisation problem as shown below:</p>
<p><img src="/2023/09/15/ModelCompressionPruning/img3.png"></p>
<p>Firstly, the normalisation factor of the parameter <em><strong>β</strong></em> is increased so that it looks for the right set of parameters in the direction we care about (i.e., so that β has as many zeros as possible, thus pruning out a certain amount of information about the input channels).<br>In addition, we add a strong constraint that the norm of <em><strong>W</strong></em> is 1 so that the W solution we get is not too simple </p>
<h4 id="The-sub-optimisation-problem-for-finding-the-β"><a href="#The-sub-optimisation-problem-for-finding-the-β" class="headerlink" title="The sub-optimisation problem for finding the  β"></a>The sub-optimisation problem for finding the  β</h4><p>First, we can fix W constant and seek a combination of parameters β, i.e., decide which input channels of the input feature map can be discarded. This is clearly an NP-hard combinatorial optimisation problem, and we can use the classical heuristic <strong>LASSO approach</strong> to iteratively find the optimal β value, as shown in the following equation:</p>
<p><img src="/2023/09/15/ModelCompressionPruning/img4.png"></p>
<h4 id="The-sub-optimisation-problem-of-finding-the-W"><a href="#The-sub-optimisation-problem-of-finding-the-W" class="headerlink" title="The sub-optimisation problem of finding the W"></a>The sub-optimisation problem of finding the W</h4><p>We then fix the constant <em><strong>β</strong></em> obtained above and solve for the <strong>optimal W</strong> parameter values, essentially solving the following <strong>MSE</strong> problem.</p>
<p><img src="/2023/09/15/ModelCompressionPruning/img5.png"></p>
<h4 id="Overall-solution-scheme"><a href="#Overall-solution-scheme" class="headerlink" title="Overall solution scheme"></a>Overall solution scheme</h4><p> In practice, there are two ways to follow.</p>
<p>One is to optimise problem one and two alternately, i.e., at the very beginning, initialise the <em><strong>W</strong></em> values with the trainee model weights and set λ to 0, i.e., without any normalization penality factor, and ||β|| to c, i.e., to retain all the input channels; then gradually increase the value of λ as the iteration proceeds; and every time we change the value of λ, we will keep solving β and W <strong>alternatively</strong> until the final ||β|| is stable. By the time the final value of ||β|| satisfies the value of less than or equal to c’, we will be able to solve for <strong>β and W alternately</strong> is less than or equal to c’, then we stop and get the final value of W as {βiWi}.</p>
<p>The other one is to perform two optimisation problems <strong>separately</strong>. That is, first continuously optimise to get the value of <strong>β</strong> so that it satisfies ||β|| &lt;&#x3D; c’, and then fix β and optimise once to get the <strong>W</strong> value.</p>
<p>Observations show that the results of these two methods are similar, while the second one is clearly <strong>less computationally</strong> intensive and more practical</p>
<h4 id="Pruning-for-the-whole-model"><a href="#Pruning-for-the-whole-model" class="headerlink" title="Pruning for the whole model"></a>Pruning for the whole model</h4><p>The method described above is for pruning a <strong>single conv layer</strong>. When applying this method to the whole CNN model, the method is similar, and it is only necessary to apply it to each layer sequentially<br>Below is the optimised pruning for each layer when applied to the whole model.</p>
<p><img src="/2023/09/15/ModelCompressionPruning/img6.png"></p>
<p>In contrast to equation (1), it replaces <em><strong>Y</strong></em> with <em><strong>Y’</strong></em>, where <em><strong>Y’</strong></em> refers to the output feature map on the original model, so we’ll take into account the effect of <u>accumulated error</u> when tuning the parameters for the whole model weigth.</p>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Pruning/">Pruning</a><a class="link-muted mr-2" rel="tag" href="/tags/DeepLearning/">DeepLearning</a><a class="link-muted mr-2" rel="tag" href="/tags/modelcompression/">modelcompression</a><a class="link-muted mr-2" rel="tag" href="/tags/Optimization/">Optimization</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2023/09/30/KD01/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Knowledge DistillationI</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/08/30/Activation/"><span class="level-item">Activation Function</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">32</span></span></a></li><li><a class="level is-mobile" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Machine-learning/"><span class="level-start"><span class="level-item">Machine learning</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/Video-Compression/"><span class="level-start"><span class="level-item">Video Compression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-29T23:00:00.000Z">2023-09-30</time></p><p class="title"><a href="/2023/09/30/KD01/">Knowledge DistillationI</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-14T23:00:00.000Z">2023-09-15</time></p><p class="title"><a href="/2023/09/15/ModelCompressionPruning/">Model Compression - Pruning</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-29T23:00:00.000Z">2023-08-30</time></p><p class="title"><a href="/2023/08/30/Activation/">Activation Function</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-14T23:00:00.000Z">2023-08-15</time></p><p class="title"><a href="/2023/08/15/BasicsofNeuralNetworksII/">Basics of Neural NetworksII</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-29T23:00:00.000Z">2023-07-30</time></p><p class="title"><a href="/2023/07/30/BasicsofNeuralNetworksI/">Basics of Neural NetworksI</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Array/"><span class="tag">Array</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BackTracking/"><span class="tag">BackTracking</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BinarySearchTree/"><span class="tag">BinarySearchTree</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BinaryTree/"><span class="tag">BinaryTree</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C-C/"><span class="tag">C/C++</span><span class="tag">32</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Combination/"><span class="tag">Combination</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CrossValidation/"><span class="tag">CrossValidation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DecisionTree/"><span class="tag">DecisionTree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepLearning/"><span class="tag">DeepLearning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DoublePointers/"><span class="tag">DoublePointers</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DynamicProgramming/"><span class="tag">DynamicProgramming</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeatureExtraction/"><span class="tag">FeatureExtraction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeaturePreprocessing/"><span class="tag">FeaturePreprocessing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GreedyAlgorithm/"><span class="tag">GreedyAlgorithm</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Iteration/"><span class="tag">Iteration</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KNN/"><span class="tag">KNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KnowledgeDistillation/"><span class="tag">KnowledgeDistillation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinearRegression/"><span class="tag">LinearRegression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinkedList/"><span class="tag">LinkedList</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linkedlist/"><span class="tag">Linkedlist</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NaiveBayes/"><span class="tag">NaiveBayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pruning/"><span class="tag">Pruning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Resursion/"><span class="tag">Resursion</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RidgeRegression/"><span class="tag">RidgeRegression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/String/"><span class="tag">String</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TraversalOfBinaryTree/"><span class="tag">TraversalOfBinaryTree</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VideoCompression/"><span class="tag">VideoCompression</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deeplearning/"><span class="tag">deeplearning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/loop-invariant/"><span class="tag">loop invariant</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machinelearning/"><span class="tag">machinelearning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/modelcompression/"><span class="tag">modelcompression</span><span class="tag">4</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="Tianhao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Tianhao</p><p class="is-size-6 is-block">a MATH and CS student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Bristol, United Kingdom</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">44</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">36</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/jasminepp/jasminepp.github.io" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/jasminepp/jasminepp.github.io"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">September 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Jasmine&#039;Site</a><p class="is-size-7"><span>&copy; 2023 Tianhao Peng</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="jasmine"></script><script src="jasmine"></script><script src="jasmine" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="jasmine" defer></script><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>