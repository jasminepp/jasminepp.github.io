{"posts":[{"title":"BackTracking Algorithm Basic","text":"The Definition of Back Tracking Algorithm The BackTracking can also be called the backtracking search method, which is a way of searching. The Efficiency of Back Tracking AlgorithmBackTracking is not an efficient algorithm because backtracking is essentially an exhaustive process of enumerating all possibilities and selecting the answer we want. The Problems can be solved by Back Tracking AlgorithmThe BackTracking, in general, can solve the following kinds of problems: Combination problem: a set of k numbers inside N numbers is found according to certain rules Cutting problem: how many ways to cut a string according to certain rules Subset problem: how many eligible subsets are there in a set of N numbers Arrange problem: how many ways to arrange N numbers in full according to certain rules Checkerboard problem: N queens, solving sudoku, etc How to understand Back Tracking AlgorithmThe problems solved by BackTracking can all be abstracted into a tree structure.The size of the set constitutes the width of the tree, and the depth of the recursion constitutes the depth of the tree. It must be a tree of finite height (N-fork tree) since recursion must have a termination condition The Template Backtracking Backtracking function template return values and parameters The return value of a function in the backtracking algorithm is generally void. The backtracking function pseudo-code is as follows: 1void backtracking(parameter) Termination conditions for backtracking functions Since it is a tree structure, when traversing the tree structure we know that there must be a termination condition, so backtracking also has a termination condition. Generally speaking, if we find a leaf node and we have found an answer that satisfies the conditions, stored the answer, and ended the recursion at this level. So the backtracking function termination condition pseudo-code is as follows： 1234 if (termination condition) { save result; return;} Traversal process for backtracking search As we mentioned above, backtracking is generally a recursive search in sets, where the size of the set constitutes the width of the tree and the depth of the recursion constitutes the depth of the tree. As shown: The backtracking function traverses the following pseudo-code: 12345 for (select：the elements of this level set（the number of children of tree node == size of set）) { process node; backtracking(path, select table); // recursion backtracking, withdraw result} A for loop is a traversal of the set interval: the number of children of a node == the times of the for loop executes (horizontal traversal) Backtracking here calls itself and implements recursion. (vertical traversal) The backtracking algorithm template is as follows: 12345678910 void backtracking(parameter) { if (temination condition) { save result; return; } for (select：the elements of this level set（the number of children of tree node == size of set）) { process node; backtracking(path, select table); // recursion backtracking, withdraw result}","link":"/2022/08/15/BackTrackingAlgorithmBasic/"},{"title":"Binary Tree Basic","text":"What is Binary Tree Data Structure?Binary Tree is defined as a Tree data structure with at most 2 children. Since each element in a binary tree can have only 2 children, we typically name them the left and right child. The Type of Binary Trees1. Full Binary Tree k-ary tree: A (rooted) tree is k-ary if every node has at most k children. If k = 2 then the tree is called binary. A k ary tree is full if every internal node has exactly k children 2. Complete Binary Tree complete if all levels except possibly the last is entirely filled (and last level is filled from left to right) 3. Binary Search TreeThe trees described earlier have no values, while a binary search tree has values. A binary search tree is an ordered tree. if its left subtree is not empty, the values of all nodes in the left subtree are less than the value of its root node. if its right subtree is not empty, the values of all nodes in the right subtree are greater than the value of its root node. Its left and right subtrees are also each binary sorted trees The following two trees are both search trees: 4. Balanced Binary TreeBalanced Binary Tree：also known as an AVL (Adelson-Velsky and Landis) tree and has the following properties: it is an empty tree or the difference in height between its left and right subtrees does not exceed 1in absolute value xxxxxxxxxx class Solution: def combine(self, n: int, k: int) -&gt; List[List[int]]: res=[] path=[] def backtrack(n,k,startIndex): if len(path) == k: res.append(path[:]) return for i in range(startIndex,n-(k-len(path))+2): #optimized for loop path.append(i) #process node backtrack(n,k,i+1) # recursion path.pop() #backtrack backtrack(n,k,1) return rescpp as shown： The last one is not a balanced binary tree because the absolute value of the difference between the heights of its left and right subtrees exceeds 1. How binary trees are storedThere are two types of storage for binary trees. Linked storage(pointer) Sequential storage(array) Sequential storage has a continuous distribution of elements in memory, whereas linked storage has a series of nodes distributed at scattered addresses by means of pointers. How does using an array to store a binary tree traverse? If the parent node’s array subscript is i, then its left child is i * 2 + 1 and its right child is i * 2 + 2. Traversing Binary TreeThere are two main types of traversal of a binary tree. Depth-first traversal: go deeper first, then back when you encounter a leaf node. Pre-order traversal (recursive, iterative ) In-order traversal (recursive, iterative) Post-order traversal (recursive, iterative) Breadth-first traversal: go through layer by layer. Level Order traversal (iterative) The Definition of Binary Tree&amp;C++123456struct TreeNode { int val; TreeNode *left; TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) {}}; &amp;Java123456789101112public class TreeNode { int val; TreeNode left; TreeNode right; TreeNode() {} TreeNode(int val) { this.val = val; } TreeNode(int val, TreeNode left, TreeNode right) { this.val = val; this.left = left; this.right = right; }} &amp;Python12345class TreeNode: def __init__(self, value): self.value = value self.left = None self.right = None","link":"/2022/03/28/BinaryTreeBasic/"},{"title":"Binary Search","text":"Given an array of integers nums which is sorted in ascending order, and an integer target, write a function to search target in nums. If target exists, then return its index. Otherwise, return -1. 123Input: nums = [-1,0,3,5,9,12], target = 9Output: 4Explanation: 9 exists in nums and its index is 4 HINTThere are two pre-conditions for Binary Search here: The array is sorted in ascending order There are no duplicate elements in the array In addition, We also need to focus on the validity of the two ends of the range：[left, right] is according to while (left &lt;= right) or ***while (left &lt; right) *** The definition of an interval is an invariant, so insisting on bounds in a loop based on the definition of a lookup interval is the loop invariant rule C++ 12345678910111213141516171819202122//Type1:[left, right] class Solution {public: int search(vector&lt;int&gt;&amp; nums, int target) { int left = 0; int right = nums.size() - 1; // [left, right] while (left &lt;= right) { // when left==right，set[left, right] still works，so we use &lt;= int middle = left + ((right - left) / 2);// prevent override==(left + right)/2 if (nums[middle] &gt; target) { right = middle - 1; // target is in left set，so[left, middle - 1] } else if (nums[middle] &lt; target) { left = middle + 1; // target is in right set，so[middle + 1, right] } else { // nums[middle] == target return middle; // we find the target, return index } } // no target return -1; }}; Time Complexity : O(nlogn) 12345678910111213141516171819202122//Type2:[left, right) class Solution {public: int search(vector&lt;int&gt;&amp; nums, int target) { int left = 0; int right = nums.size() - 1; // [left, right) while (left &lt; right) { // when left==right，set[left, right] is meaningless，so we use &lt; int middle = left + ((right - left) / 2);// prevent override==(left + right)/2 if (nums[middle] &gt; target) { right = middle - 1; // target is in left set，so[left, middle) } else if (nums[middle] &lt; target) { left = middle + 1; // target is in right set，so[middle + 1, right) } else { // nums[middle] == target return middle; // we find the target, return index } } // no target return -1; }}; Java 12345678910111213141516171819class Solution { public int search(int[] nums, int target) { // To avoid multiple while loop when target is less than nums[0] nums[nums.length - 1] if (target &lt; nums[0] || target &gt; nums[nums.length - 1]) { return -1; } int left = 0, right = nums.length - 1; while (left &lt;= right) { int mid = left + ((right - left) &gt;&gt; 1); if (nums[mid] == target) return mid; else if (nums[mid] &lt; target) left = mid + 1; else if (nums[mid] &gt; target) right = mid - 1; } return -1; }}","link":"/2021/10/02/BinarySearch/"},{"title":"Convert Sorted Array to Binary Search Tree","text":"TEST YOURSELFGiven an integer array nums where the elements are sorted in ascending order, convert it to a height-balanced binary search tree. A height-balanced binary tree is a binary tree in which the depth of the two subtrees of every node never differs by more than one. Example 1: 123Input: nums = [-10,-3,0,5,9]Output: [0,-3,9,-10,null,5] Explanation: [0,-10,5,null,-3,null,9] is also accepted: HINTThe focus is on finding the split point, which is used as the current node, and then recursively the left interval and right interval. The split point is the node in the middle of the array. If the array is of even length and there are two intermediate nodes, which one is taken? Either one can be taken, except that it forms a different balanced binary search tree. Example: Input: [-10,-3,0,5,9] The following two trees, both of which are balanced binary search trees for this array, are as follows. If the length of the array to be partitioned is even and the middle element is two, the left element is tree 1 and the right element is tree 2 RecursionDetermine the parameters and return value of the recursive function:We want to construct a binary tree, still using the return value of the recursive function to construct the left and right children of the middle node. Parameters: first the array is passed in, then the left subscript left and the right subscript right 12 // [left, right]TreeNode* traversal(vector&lt;int&gt;&amp; nums, int left, int right) Determine the termination condition: When the interval left &gt; right, the node is empty. 1if (left &gt; right) return nullptr; The logic for determining the single-level recursion: First take the position of the middle element of the array: int mid = left + ((right - left) / 2); Once the middle position is taken, start constructing nodes with the elements in the middle position: TreeNode* root = new TreeNode(nums[mid]); Then divide the interval, root’s left child picks up the next layer of left interval construction node, the right child picks up the next layer of right interval construction node. Finally, the root node is returned: 12345 int mid = left + ((right - left) / 2);TreeNode* root = new TreeNode(nums[mid]);root-&gt;left = traversal(nums, left, mid - 1);root-&gt;right = traversal(nums, mid + 1, right);return root; &amp;C++ 12345678910111213141516class Solution {private: TreeNode* traversal(vector&lt;int&gt;&amp; nums, int left, int right) { if (left &gt; right) return nullptr; int mid = left + ((right - left) / 2); TreeNode* root = new TreeNode(nums[mid]); root-&gt;left = traversal(nums, left, mid - 1); root-&gt;right = traversal(nums, mid + 1, right); return root; }public: TreeNode* sortedArrayToBST(vector&lt;int&gt;&amp; nums) { TreeNode* root = traversal(nums, 0, nums.size() - 1); return root; }}; Iteration&amp;C++ 12345678910111213141516171819202122232425262728293031323334353637383940class Solution {public: TreeNode* sortedArrayToBST(vector&lt;int&gt;&amp; nums) { if (nums.size() == 0) return nullptr; TreeNode* root = new TreeNode(0); // initial root node queue&lt;TreeNode*&gt; nodeQue; // save traversal nodes queue&lt;int&gt; leftQue; // save left subscript queue&lt;int&gt; rightQue; // save right subscript nodeQue.push(root); // put root node in queue leftQue.push(0); // 0 is the initial position of left subscript rightQue.push(nums.size() - 1); // nums.size() - 1 is the initial position of right subscript while (!nodeQue.empty()) { TreeNode* curNode = nodeQue.front(); nodeQue.pop(); int left = leftQue.front(); leftQue.pop(); int right = rightQue.front(); rightQue.pop(); int mid = left + ((right - left) / 2); curNode-&gt;val = nums[mid]; // let middle node = mid value if (left &lt;= mid - 1) { // deal with left side curNode-&gt;left = new TreeNode(0); nodeQue.push(curNode-&gt;left); leftQue.push(left); rightQue.push(mid - 1); } if (right &gt;= mid + 1) { // deal with right side curNode-&gt;right = new TreeNode(0); nodeQue.push(curNode-&gt;right); leftQue.push(mid + 1); rightQue.push(right); } } return root; }}; &amp;JavaRecursion 1234567891011121314151617class Solution { public TreeNode sortedArrayToBST(int[] nums) { TreeNode root = traversal(nums, 0, nums.length - 1); return root; } // [left, right] private TreeNode traversal(int[] nums, int left, int right) { if (left &gt; right) return null; int mid = left + ((right - left) &gt;&gt; 1); TreeNode root = new TreeNode(nums[mid]); root.left = traversal(nums, left, mid - 1); root.right = traversal(nums, mid + 1, right); return root; }} Iteration 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution { public TreeNode sortedArrayToBST(int[] nums) { if (nums.length == 0) return null; //initial root node TreeNode root = new TreeNode(-1); Queue&lt;TreeNode&gt; nodeQueue = new LinkedList&lt;&gt;(); Queue&lt;Integer&gt; leftQueue = new LinkedList&lt;&gt;(); Queue&lt;Integer&gt; rightQueue = new LinkedList&lt;&gt;(); // put root node in queue nodeQueue.offer(root); leftQueue.offer(0); rightQueue.offer(nums.length - 1); while (!nodeQueue.isEmpty()) { TreeNode currNode = nodeQueue.poll(); int left = leftQueue.poll(); int right = rightQueue.poll(); int mid = left + ((right - left) &gt;&gt; 1); currNode.val = nums[mid]; if (left &lt;= mid - 1) { currNode.left = new TreeNode(-1); nodeQueue.offer(currNode.left); leftQueue.offer(left); rightQueue.offer(mid - 1); } if (right &gt;= mid + 1) { currNode.right = new TreeNode(-1); nodeQueue.offer(currNode.right); leftQueue.offer(mid + 1); rightQueue.offer(right); } } return root; }} &amp;PythonRecursion 1234567891011121314151617181920class Solution: def sortedArrayToBST(self, nums: List[int]) -&gt; TreeNode: root = self.traversal(nums, 0, len(nums)-1) return root def traversal(self, nums: List[int], left: int, right: int) -&gt; TreeNode: # Base Case if left &gt; right: return None mid = left + (right - left) // 2 mid_root = TreeNode(nums[mid]) mid_root.left = self.traversal(nums, left, mid-1) mid_root.right = self.traversal(nums, mid+1, right) return mid_root","link":"/2022/07/31/ConvertSortedArraytoBinarySearchTree/"},{"title":"Design Linked List","text":"Implement the MyLinkedList class: int get(int index) Get the value of the indexth node in the linked list. If the index is invalid, return -1. void addAtHead(int val) Add a node of value val before the first element of the linked list. After the insertion, the new node will be the first node of the linked list. void addAtTail(int val) Append a node of value val as the last element of the linked list. void addAtIndex(int index, int val) Add a node of value val before the indexth node in the linked list. If index equals the length of the linked list, the node will be appended to the end of the linked list. If index is greater than the length, the node will not be inserted. void deleteAtIndex(int index) Delete the indexth node in the linked list, if the index is valid. 1234567891011121314151617Example:Input[&quot;MyLinkedList&quot;, &quot;addAtHead&quot;, &quot;addAtTail&quot;, &quot;addAtIndex&quot;, &quot;get&quot;, &quot;deleteAtIndex&quot;, &quot;get&quot;][[], [1], [3], [1, 2], [1], [1], [1]]Output[null, null, null, null, 2, null, 3]ExplanationMyLinkedList myLinkedList = new MyLinkedList();myLinkedList.addAtHead(1);myLinkedList.addAtTail(3);myLinkedList.addAtIndex(1, 2); // linked list becomes 1-&gt;2-&gt;3myLinkedList.get(1); // return 2myLinkedList.deleteAtIndex(1); // now the linked list is 1-&gt;3myLinkedList.get(1); // return 3 Two ways of manipulating a linked list: 1. Use the original linked list 2. Setting a dummy head node The following is used to set a dummy head node (this is more convenient, as you can see by looking at the code) &amp;C++123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293class MyLinkedList {public: // Define the linked list node structure struct LinkedNode { int val; LinkedNode* next; LinkedNode(int val):val(val), next(nullptr){} }; // Initialise a linked list MyLinkedList() { _dummyHead = new LinkedNode(0); // a dummy node _size = 0; } // Get the value of the index node, return -1 if index is illegal, note that index starts at 0 and the 0th node is the head node int get(int index) { if (index &gt; (_size - 1) || index &lt; 0) { return -1; } LinkedNode* cur = _dummyHead-&gt;next; while(index--){ // if--index we will fall into loop cur = cur-&gt;next; } return cur-&gt;val; } // Insert a node at the top of the linked list, and when the insertion is complete, the newly inserted node is the new head node of the linked list void addAtHead(int val) { LinkedNode* newNode = new LinkedNode(val); newNode-&gt;next = _dummyHead-&gt;next; _dummyHead-&gt;next = newNode; _size++; } // Add a node to the end of the list void addAtTail(int val) { LinkedNode* newNode = new LinkedNode(val); LinkedNode* cur = _dummyHead; while(cur-&gt;next != nullptr){ cur = cur-&gt;next; } cur-&gt;next = newNode; _size++; } // A new node is inserted before the index node, e.g. if index is 0, then the new inserted node is the new head node of the chain. // If index is equal to the length of the list, then the newly inserted node is the end node of the list. // if index is greater than the length of the list, then return null // If index is less than 0, set to 0 as the new head node of the linked list. void addAtIndex(int index, int val) { if (index &gt; _size || index &lt; 0) { return; } LinkedNode* newNode = new LinkedNode(val); LinkedNode* cur = _dummyHead; while(index--) { cur = cur-&gt;next; } newNode-&gt;next = cur-&gt;next; cur-&gt;next = newNode; _size++; } // Delete the index node, if the index is larrger than or equal to the length of the linked list, return directly, note that the index starts from 0 void deleteAtIndex(int index) { if (index &gt;= _size || index &lt; 0) { return; } LinkedNode* cur = _dummyHead; while(index--) { cur = cur -&gt;next; } LinkedNode* tmp = cur-&gt;next; cur-&gt;next = cur-&gt;next-&gt;next; delete tmp; _size--; } // print linked list void printLinkedList() { LinkedNode* cur = _dummyHead; while (cur-&gt;next != nullptr) { cout &lt;&lt; cur-&gt;next-&gt;val &lt;&lt; &quot; &quot;; cur = cur-&gt;next; } cout &lt;&lt; endl; }private: int _size; LinkedNode* _dummyHead;}; &amp;C123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121typedef struct { int val; struct MyLinkedList* next;}MyLinkedList;/** Initialize your data structure here. */MyLinkedList* myLinkedListCreate() { //a dummy node MyLinkedList* head = (MyLinkedList *)malloc(sizeof (MyLinkedList)); head-&gt;next = NULL; return head;}/** Get the value of the index-th node in the linked list. If the index is invalid, return -1. */int myLinkedListGet(MyLinkedList* obj, int index) { MyLinkedList *cur = obj-&gt;next; for (int i = 0; cur != NULL; i++){ if (i == index){ return cur-&gt;val; } else{ cur = cur-&gt;next; } } return -1;}/** Add a node of value val before the first element of the linked list. After the insertion, the new node will be the first node of the linked list. */void myLinkedListAddAtHead(MyLinkedList* obj, int val) { MyLinkedList *nhead = (MyLinkedList *)malloc(sizeof (MyLinkedList)); nhead-&gt;val = val; nhead-&gt;next = obj-&gt;next; obj-&gt;next = nhead;}/** Append a node of value val to the last element of the linked list. */void myLinkedListAddAtTail(MyLinkedList* obj, int val) { MyLinkedList *cur = obj; while(cur-&gt;next != NULL){ cur = cur-&gt;next; } MyLinkedList *ntail = (MyLinkedList *)malloc(sizeof (MyLinkedList)); ntail-&gt;val = val; ntail-&gt;next = NULL; cur-&gt;next = ntail;}/** Add a node of value val before the index-th node in the linked list. If index equals to the length of linked list, the node will be appended to the end of linked list. If index is greater than the length, the node will not be inserted. */void myLinkedListAddAtIndex(MyLinkedList* obj, int index, int val) { if (index == 0){ myLinkedListAddAtHead(obj, val); return; } MyLinkedList *cur = obj-&gt;next; for (int i = 1 ;cur != NULL; i++){ if (i == index){ MyLinkedList* newnode = (MyLinkedList *)malloc(sizeof (MyLinkedList)); newnode-&gt;val = val; newnode-&gt;next = cur-&gt;next; cur-&gt;next = newnode; return; } else{ cur = cur-&gt;next; } }}/** Delete the index-th node in the linked list, if the index is valid. */void myLinkedListDeleteAtIndex(MyLinkedList* obj, int index) { if (index == 0){ MyLinkedList *tmp = obj-&gt;next; if (tmp != NULL){ obj-&gt;next = tmp-&gt;next; free(tmp) } return; } MyLinkedList *cur = obj-&gt;next; for (int i = 1 ;cur != NULL &amp;&amp; cur-&gt;next != NULL; i++){ if (i == index){ MyLinkedList *tmp = cur-&gt;next; if (tmp != NULL) { cur-&gt;next = tmp-&gt;next; free(tmp); } return; } else{ cur = cur-&gt;next; } } }void myLinkedListFree(MyLinkedList* obj) { while(obj != NULL){ MyLinkedList *tmp = obj; obj = obj-&gt;next; free(tmp); }}/** * Your MyLinkedList struct will be instantiated and called as such: * MyLinkedList* obj = myLinkedListCreate(); * int param_1 = myLinkedListGet(obj, index); * myLinkedListAddAtHead(obj, val); * myLinkedListAddAtTail(obj, val); * myLinkedListAddAtIndex(obj, index, val); * myLinkedListDeleteAtIndex(obj, index); * myLinkedListFree(obj);*/ &amp;Java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697//single linked list class ListNode { int val; ListNode next; ListNode(){} ListNode(int val) { this.val=val; }}class MyLinkedList { //size of this linked list int size; //dummy node ListNode head; //initialise linked list public MyLinkedList() { size = 0; head = new ListNode(0); } //get the value of index node public int get(int index) { //if index is illgel return -1 if (index &lt; 0 || index &gt;= size) { return -1; } ListNode currentNode = head; // the list contains a dummy node, so we find the node of index+1 for (int i = 0; i &lt;= index; i++) { currentNode = currentNode.next; } return currentNode.val; } //Insert a node at the top of the list public void addAtHead(int val) { addAtIndex(0, val); } //Insert a node at the end of the list public void addAtTail(int val) { addAtIndex(size, val); } // Insert a new node before the index node, e.g. if index is 0, then the newly inserted node is the new head node of the list. // If index is equal to the length of the list, then the newly inserted node is the end node of the list // If index is larger than the length of the list, then return null public void addAtIndex(int index, int val) { if (index &gt; size) { return; } if (index &lt; 0) { index = 0; } size++; //find the fronter of the inserted node ListNode pred = head; for (int i = 0; i &lt; index; i++) { pred = pred.next; } ListNode toAdd = new ListNode(val); toAdd.next = pred.next; pred.next = toAdd; } //detele the node of index public void deleteAtIndex(int index) { if (index &lt; 0 || index &gt;= size) { return; } size--; if (index == 0) { head = head.next; return; } ListNode pred = head; for (int i = 0; i &lt; index ; i++) { pred = pred.next; } pred.next = pred.next.next; }}/** * Your MyLinkedList object will be instantiated and called as such: * MyLinkedList obj = new MyLinkedList(); * int param_1 = obj.get(index); * obj.addAtHead(val); * obj.addAtTail(val); * obj.addAtIndex(index,val); * obj.deleteAtIndex(index); */","link":"/2021/12/09/DesignLinkedList/"},{"title":"Insert into a Binary Search Tree","text":"TEST YOURSELFYou are given the root node of a binary search tree (BST) and a value to insert into the tree. Return the root node of the BST after the insertion. It is guaranteed that the new value does not exist in the original BST. Notice that there may exist multiple valid ways for the insertion, as long as the tree remains a BST after insertion. You can return any of them. Example 1: 123Input: root = [4,2,7,1,3], val = 5Output: [4,2,7,1,3,5] Explanation: Another accepted tree is: HINTWe just need to follow the rules of the binary search tree to traverse and insert nodes when we encounter empty nodes. For example, insert element 10, we need to find the last node to insert it, the same reasoning to insert element 15, insert element 0, insert element 6. The next step is the process of traversing the binary search tree: Recursion Determine the parameters and return value of the recursive function: The parameters are the root node pointer, and the element to be inserted.We can use the return value to complete the assignment of the newly added node to its parent nodeThe return type of the recursive function is the node type TreeNode* 1TreeNode* insertIntoBST(TreeNode* root, int val) Determine the termination condition:The termination condition is to find the position of the node to be inserted when the traversed node is null, and to return the inserted node 1234if (root == NULL) { TreeNode* node = new TreeNode(val); return node;} The logic for determining the single-level recursion: We do not need to traverse the whole search tree.Because the search tree is directed, the direction of recursion can be determined by the value of the inserted elements. 1234 if (root == NULL) { TreeNode* node = new TreeNode(val); return node;} &amp;C++ 123456789101112class Solution {public: TreeNode* insertIntoBST(TreeNode* root, int val) { if (root == NULL) { TreeNode* node = new TreeNode(val); return node; } if (root-&gt;val &gt; val) root-&gt;left = insertIntoBST(root-&gt;left, val); if (root-&gt;val &lt; val) root-&gt;right = insertIntoBST(root-&gt;right, val); return root; }}; Iteration&amp;C++ 1234567891011121314151617181920class Solution {public: TreeNode* insertIntoBST(TreeNode* root, int val) { if (root == NULL) { TreeNode* node = new TreeNode(val); return node; } TreeNode* cur = root; TreeNode* parent = root; // we need to record last node, otherise we cannot give value to new node while (cur != NULL) { parent = cur; if (cur-&gt;val &gt; val) cur = cur-&gt;left; else cur = cur-&gt;right; } TreeNode* node = new TreeNode(val); if (val &lt; parent-&gt;val) parent-&gt;left = node;// Use parent node to give value else parent-&gt;right = node; return root; }}; &amp;JavaRecursion 12345678910111213141516171819202122class Solution { public TreeNode insertIntoBST(TreeNode root, int val) { if (root == null) return new TreeNode(val); TreeNode newRoot = root; TreeNode pre = root; while (root != null) { pre = root; if (root.val &gt; val) { root = root.left; } else if (root.val &lt; val) { root = root.right; } } if (pre.val &gt; val) { pre.left = new TreeNode(val); } else { pre.right = new TreeNode(val); } return newRoot; }} Iteration 1234567891011121314class Solution { public TreeNode insertIntoBST(TreeNode root, int val) { if (root == null) // if this node is empty, which means val finds the suitable position, return new node return new TreeNode(val); if (root.val &lt; val){ root.right = insertIntoBST(root.right, val); // create right subtree }else if (root.val &gt; val){ root.left = insertIntoBST(root.left, val); // create left subtree } return root; }} &amp;PythonRecursion 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode:# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution: def insertIntoBST(self, root: TreeNode, val: int) -&gt; TreeNode: # Base Case if not root: return TreeNode(val) if val &lt; root.val: root.left = self.insertIntoBST(root.left, val) if root.val &lt; val: root.right = self.insertIntoBST(root.right, val) return root Iteration 123456789101112131415161718192021222324class Solution: def insertIntoBST(self, root: TreeNode, val: int) -&gt; TreeNode: if not root: return TreeNode(val) parent = None cur = root #use while loop to find the parent of new node while cur: if cur.val &lt; val: parent = cur cur = cur.right elif cur.val &gt; val: parent = cur cur = cur.left # the parent of new node has been found if parent.val &gt; val: parent.left = TreeNode(val) else: parent.right = TreeNode(val) return root","link":"/2022/07/15/InsertintoaBinarySearchTree/"},{"title":"Letter Combinations of a Phone Number","text":"TEST YOURSELFGiven a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. Return the answer in any order. A mapping of digits to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters. Example 1: 12Input: digits = &quot;23&quot;Output: [&quot;ad&quot;,&quot;ae&quot;,&quot;af&quot;,&quot;bd&quot;,&quot;be&quot;,&quot;bf&quot;,&quot;cd&quot;,&quot;ce&quot;,&quot;cf&quot;] HINTIn terms of examples, if we type “23”, the most straightforward idea would be a two-level for loop traversal, which would output all the combinations. But if we enter “233”, then it’s a three-level for loop, and if “2333”, then it’s a four-level for loop ……. There are three problems to solve: How to map numbers and letters n letters n for loops Input 1 * # keystrokes and so on exceptions 1. How to map numbers and lettersWe can use a map or define a two-dimensional array, e.g. string letterMap[10], to do the mapping. 123456789101112const string letterMap[10] = { &quot;&quot;, // 0 &quot;&quot;, // 1 &quot;abc&quot;, // 2 &quot;def&quot;, // 3 &quot;ghi&quot;, // 4 &quot;jkl&quot;, // 5 &quot;mno&quot;, // 6 &quot;pqrs&quot;, // 7 &quot;tuv&quot;, // 8 &quot;wxyz&quot;, // 9}; 2. Solve n-for loop by BackTrackingFor example, enter: “23” and abstract to a tree structure as shown in the figure. The graph shows the depth of the traversal, which is the length of the input “23”, and the leaf nodes are the results we want to collect, outputting [“ad”, “ae”, “af”, “bd”, “be”, “bf”, “cd”, “ce”, “cf”]. Backtracking function template return values and parameters string s to collect the results of the leaf nodes and then save them in a string array result The index is a record of the number of digits traversed, which is used to traverse digits and the index also indicates the depth of the tree. 123vector&lt;string&gt; result;string s;void backtracking(const string&amp; digits, int index) Termination conditions for backtracking functions For example, if the input case is “23”, two digits, then the root node is recursive down two levels and the leaf nodes are the result set to be collected. The termination condition is if index is equal to the number of digits (digits.size) entered (index was originally used to traverse the digits). The result is then collected, ending the recursion at this level. 1234 if (index == digits.size()) { result.push_back(s); return;} Traversal process for backtracking search we take the number pointed to by index and find the corresponding character set (the character set of the phone keyboard) 1234567 int digit = digits[index] - '0'; //change the number pointed by index to intstring letters = letterMap[digit]; // take the correponding character setfor (int i = 0; i &lt; letters.size(); i++) { s.push_back(letters[i]); // process node backtracking(digits, index + 1); // recursion，index+1 s.pop_back(); // backtracking} &amp;C++ 123456789101112131415161718192021222324252627282930313233343536373839 class Solution {private: const string letterMap[10] = { &quot;&quot;, // 0 &quot;&quot;, // 1 &quot;abc&quot;, // 2 &quot;def&quot;, // 3 &quot;ghi&quot;, // 4 &quot;jkl&quot;, // 5 &quot;mno&quot;, // 6 &quot;pqrs&quot;, // 7 &quot;tuv&quot;, // 8 &quot;wxyz&quot;, // 9 };public: vector&lt;string&gt; result; string s; void backtracking(const string&amp; digits, int index) { if (index == digits.size()) { result.push_back(s); return; } int digit = digits[index] - '0'; //change the number pointed by index to intstring letters = letterMap[digit]; // take the correponding character setfor (int i = 0; i &lt; letters.size(); i++) { s.push_back(letters[i]); // process node backtracking(digits, index + 1); // recursion，index+1 s.pop_back(); // backtracking}vector&lt;string&gt; letterCombinations(string digits) { s.clear(); result.clear(); if (digits.size() == 0) { return result; } backtracking(digits, 0); return result; }}; &amp;Java 12345678910111213141516171819202122232425262728293031323334353637 class Solution { // set global list to save final result List&lt;String&gt; list = new ArrayList&lt;&gt;(); public List&lt;String&gt; letterCombinations(String digits) { if (digits == null || digits.length() == 0) { return list; } String[] numString = {&quot;&quot;, &quot;&quot;, &quot;abc&quot;, &quot;def&quot;, &quot;ghi&quot;, &quot;jkl&quot;, &quot;mno&quot;, &quot;pqrs&quot;, &quot;tuv&quot;, &quot;wxyz&quot;}; //Interation backTracking(digits, numString, 0); return list; } StringBuilder temp = new StringBuilder(); public void backTracking(String digits, String[] numString, int num) { if (num == digits.length()) { list.add(temp.toString()); return; } String str = numString[digits.charAt(num) - '0']; for (int i = 0; i &lt; str.length(); i++) { temp.append(str.charAt(i)); //c backTracking(digits, numString, num + 1); temp.deleteCharAt(temp.length() - 1); } }} &amp;Python 123456789101112131415161718192021222324252627282930313233class Solution: def __init__(self): self.answers: List[str] = [] self.answer: str = '' self.letter_map = { '2': 'abc', '3': 'def', '4': 'ghi', '5': 'jkl', '6': 'mno', '7': 'pqrs', '8': 'tuv', '9': 'wxyz' } def letterCombinations(self, digits: str) -&gt; List[str]: self.answers.clear() if not digits: return [] self.backtracking(digits, 0) return self.answers def backtracking(self, digits: str, index: int) -&gt; None: # the backtacking funtion has no return value # Base Case if index == len(digits): self.answers.append(self.answer) return letters: str = self.letter_map[digits[index]] for letter in letters: self.answer += letter self.backtracking(digits, index + 1) self.answer = self.answer[:-1] # backtracking","link":"/2022/09/30/LetterCombinationsofaPhoneNumber/"},{"title":"Combinations-Pruning","text":"Pruning optimization There is the following code in the traversal process: 12345for (int i = startIndex; i &lt;= n; i++) { path.push_back(i); backtracking(n, k, i + 1); path.pop_back();} The range of this traversal can be pruned and optimised, how? For example, if n = 4 and k = 4, then at the first level of the for loop, all traversals from element 2 are meaningless. At the second level of the for loop, the traversal from element 3 onwards is meaningless. This is a little abstract, as shown in the diagram: Each node in the diagram (rectangle in the diagram) represents a for-loop at this level, so each level of for-loops starting from the second number is meaningless and is an invalid traversal. Thus, the place to prune is at the starting position chosen by the for loop at each level of the recursion. If the number of elements after the start of the for loop is already less than the number of elements we need, then there is no need to search. Note that i in the code is the starting position selected in the for loop. Next, look at the optimization process as follows: Number of elements already selected: path.size(); The number of elements still needed is: k - path.size(); The set n must be traversed from at most the starting position : n - (k - path.size()) + 1, starting traversal The reason for the +1 is: we want a left-closed setincluding the starting position. For example, n = 4, k = 3, the element already selected is 0 (path.size is 0), n - (k - 0) + 1 i.e. 4 - ( 3 - 0) + 1 = 2.It is reasonable to search all from 2, which can be the combination [2, 3, 4]. So the optimized for loop is: 12345678910111213141516171819202122class Solution {private: vector&lt;vector&lt;int&gt;&gt; result; vector&lt;int&gt; path; void backtracking(int n, int k, int startIndex) { if (path.size() == k) { result.push_back(path); return; } for (int i = startIndex; i &lt;= n - (k - path.size()) + 1; i++) { // optimized for loop path.push_back(i); // process node backtracking(n, k, i + 1); path.pop_back(); // backtracking } }public: vector&lt;vector&lt;int&gt;&gt; combine(int n, int k) { backtracking(n, k, 1); return result; }}; &amp;Java 1234567891011121314151617181920class Solution { List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;(); LinkedList&lt;Integer&gt; path = new LinkedList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; combine(int n, int k) { combineHelper(n, k, 1); return result; } private void combineHelper(int n, int k, int startIndex){ //termination if (path.size() == k){ result.add(new ArrayList&lt;&gt;(path)); return; } for (int i = startIndex; i &lt;= n - (k - path.size()) + 1; i++){ path.add(i); combineHelper(n, k, i + 1); path.removeLast(); } }} &amp;Python 1234567891011121314class Solution: def combine(self, n: int, k: int) -&gt; List[List[int]]: res=[] path=[] def backtrack(n,k,startIndex): if len(path) == k: res.append(path[:]) return for i in range(startIndex,n-(k-len(path))+2): #optimized for loop path.append(i) #process node backtrack(n,k,i+1) # recursion path.pop() #backtrack backtrack(n,k,1) return res","link":"/2022/09/15/Combinations-Pruning/"},{"title":"Level Order Traversal of Binary Tree","text":"Given the root of a binary tree, return the level order traversal of its nodes’ values. (i.e., from left to right, level by level). Example1: 123Input: root = [3,9,20,null,null,15,7]Output: [[3],[9,20],[15,7]] Example2: 12Input: root = [1]Output: [[1]] HINTLevel order Traversal: This means traversing the binary tree one layer at a time from left to right.This is achieved by borrowing an auxiliary data structure: the queue, which is FIFO(first-in-first-out), in line with the logic of layer-by-layer traversal. This level order traversal is the breadth-first traversal in graph theory, except that we apply it to binary trees. A breadth-first traversal of a binary tree using a queue is as follows. &amp;C++123456789101112131415161718192021class Solution {public: vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) { queue&lt;TreeNode*&gt; que; if (root != NULL) que.push(root); vector&lt;vector&lt;int&gt;&gt; result; while (!que.empty()) { int size = que.size(); vector&lt;int&gt; vec; // fixed siize(since que.size is changing) for (int i = 0; i &lt; size; i++) { TreeNode* node = que.front(); que.pop(); vec.push_back(node-&gt;val); if (node-&gt;left) que.push(node-&gt;left); if (node-&gt;right) que.push(node-&gt;right); } result.push_back(vec); } return result; }}; Iteration 1234567891011121314151617class Solution {public: void order(TreeNode* cur, vector&lt;vector&lt;int&gt;&gt;&amp; result, int depth) { if (cur == nullptr) return; if (result.size() == depth) result.push_back(vector&lt;int&gt;()); result[depth].push_back(cur-&gt;val); order(cur-&gt;left, result, depth + 1); order(cur-&gt;right, result, depth + 1); } vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) { vector&lt;vector&lt;int&gt;&gt; result; int depth = 0; order(root, result, depth); return result; }}; &amp;Java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Solution { public List&lt;List&lt;Integer&gt;&gt; resList = new ArrayList&lt;List&lt;Integer&gt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) { //checkFun01(root,0); checkFun02(root); return resList; } //DFS--Resursion public void checkFun01(TreeNode node, Integer deep) { if (node == null) return; deep++; if (resList.size() &lt; deep) { //when number of level is larger,the item of list is larger. we use the index of list to distinguish diffrent levels List&lt;Integer&gt; item = new ArrayList&lt;Integer&gt;(); resList.add(item); } resList.get(deep - 1).add(node.val); checkFun01(node.left, deep); checkFun01(node.right, deep); } //BFS--Iteration--use queue public void checkFun02(TreeNode node) { if (node == null) return; Queue&lt;TreeNode&gt; que = new LinkedList&lt;TreeNode&gt;(); que.offer(node); while (!que.isEmpty()) { List&lt;Integer&gt; itemList = new ArrayList&lt;Integer&gt;(); int len = que.size(); while (len &gt; 0) { TreeNode tmpNode = que.poll(); itemList.add(tmpNode.val); if (tmpNode.left != null) que.offer(tmpNode.left); if (tmpNode.right != null) que.offer(tmpNode.right); len--; } resList.add(itemList); } } }","link":"/2022/04/30/LevelOrderTraversalofBinaryTree/"},{"title":"Minimum Size Subarray Sum","text":"Given an array of positive integers nums and a positive integer target, return the minimal length of a contiguous subarray [numsl, numsl+1, …, numsr-1, numsr] of which the sum is greater than or equal to target. If there is no such subarray, return 0 instead. 123Input: target = 7, nums = [2,3,1,2,4,3]Output: 2Explanation: The subarray [4,3] has the minimal length under the problem constraint. First Solutoinwe can directly use two for loop but it has bad Runtime O(n^2) 1234567891011121314151617181920class Solution {public: int minSubArrayLen(int s, vector&lt;int&gt;&amp; nums) { int result = INT32_MAX; // final result int sum = 0; // the sum of subaray int subLength = 0; // the length of subaray for (int i = 0; i &lt; nums.size(); i++) { // set the start of subarray is i sum = 0; for (int j = i; j &lt; nums.size(); j++) { // set the start of subarray is j sum += nums[j]; if (sum &gt;= s) { // update result when sum of subarray is bigger than s subLength = j - i + 1; // update the length of subarray result = result &lt; subLength ? result : subLength; break; // once we find the result, break } } } // no relative result return result == INT32_MAX ? 0 : result; }}; Time Complexity : O(n^2) Space Complexity : O(1) Better Solutionwe can choose double pointer to solve this question.Imagine a double pointer as a sliding window: The window is the smallest continuous subarray of its length that satisfies the sum ≥ s. How the start of the window is moved?: if the current window is larger than s, the window is moved forward (i.e. it is time to shrink). How to move the end of the window: The end of the window is the pointer to the traversal array, which is the index in the for loop. C++ 1234567891011121314151617181920class Solution {public: int minSubArrayLen(int s, vector&lt;int&gt;&amp; nums) { int result = INT32_MAX; //set initial result as an infitine maxinum int sum = 0; // the sum of sliding window int i = 0; // the start of sliding window int subLength = 0; // the length of sliding window for (int j = 0; j &lt; nums.size(); j++) { sum += nums[j]; while (sum &gt;= s) { subLength = (j - i + 1); // update the length of sliding window result = result &lt; subLength ? result : subLength; sum -= nums[i++]; // update i, which means shrink sliding window } } // return result == INT32_MAX ? 0 : result; }}; Time Complexity : O(n) Space Complexity : O(1) Java 12345678910111213141516class Solution { public int minSubArrayLen(int s, int[] nums) { int left = 0; int sum = 0; int result = Integer.MAX_VALUE; for (int right = 0; right &lt; nums.length; right++) { sum += nums[right]; while (sum &gt;= s) { result = Math.min(result, right - left + 1); sum -= nums[left++]; } } return result == Integer.MAX_VALUE ? 0 : result; }}","link":"/2021/10/21/Minimum%20Size%20Subarray%20Sum/"},{"title":"Maximum Depth of Binary Tree","text":"TEST YOURSELFGiven the root of a binary tree, return its maximum depth. A binary tree’s maximum depth is the number of nodes along the longest path from the root node down to the farthest leaf node. Example1: 123Input: root = [3,9,20,null,null,15,7]Output: 3 HINTResursionThe problem can be solved using either preorder (parent left and right) or postorder traversal (left and right parent), using preorder to find the depth and postorder to find the height. The depth of a binary tree node: the number of edges or nodes of the longest simple path from the root node to that node (depending on whether the depth starts at 0 or 1) The height of a binary tree node: the number of edges of the longest simple path from that node to the leaf node (depending on whether the height starts at 0 or 1) The height of the root node is the maximum depth of the binary tree, so in this problem we find the maximum depth of the binary tree by the height of the root node in the post-order. we start with a post-order traversal (left and right middle) to calculate the height of the tree. Determine the parameters and return value of the recursive function: the parameter is the root node of the incoming tree, and the return returns the depth of the tree, so the return value is of type int.The code is as follows. 1int getdepth(treenode* node) Determine the termination condition: if the node is empty, 0 is returned, indicating a height of 0. 1if (node == NULL) return 0; The logic for determining the single-level recursion: first find the depth of its left subtree, then the depth of its right subtree, and finally take the largest value of the left and right depths and add 1 (plus 1 because the current middle node is included) to the depth of the tree whose current node is the root node. 1234int leftdepth = getdepth(node-&gt;left); // leftint rightdepth = getdepth(node-&gt;right); // rightint depth = 1 + max(leftdepth, rightdepth); // parentreturn depth; &amp;C++ 12345678910111213class solution {public: int getdepth(treenode* node) { if (node == NULL) return 0; int leftdepth = getdepth(node-&gt;left); // left int rightdepth = getdepth(node-&gt;right); // right int depth = 1 + max(leftdepth, rightdepth); // parent return depth; } int maxdepth(treenode* root) { return getdepth(root); }}; IterationUsing the iterative method, it is most appropriate to use a level order traversal, because the **maximum depth **is the number of layers in the binary tree, which fits perfectly with the level order traversal. As shown in the figure: &amp;C++1234567891011121314151617181920class solution {public: int maxdepth(treenode* root) { if (root == NULL) return 0; int depth = 0; queue&lt;treenode*&gt; que; que.push(root); while(!que.empty()) { int size = que.size(); depth++; // record depth for (int i = 0; i &lt; size; i++) { treenode* node = que.front(); que.pop(); if (node-&gt;left) que.push(node-&gt;left); if (node-&gt;right) que.push(node-&gt;right); } } return depth; }}; &amp;Java 12345678910111213class solution { /** * Resursion */ public int maxDepth(TreeNode root) { if (root == null) { return 0; } int leftDepth = maxDepth(root.left); int rightDepth = maxDepth(root.right); return Math.max(leftDepth, rightDepth) + 1; }} 123456789101112131415161718192021222324252627class solution { /** * Iteration by level order traversal */ public int maxDepth(TreeNode root) { if(root == null) { return 0; } Deque&lt;TreeNode&gt; deque = new LinkedList&lt;&gt;(); deque.offer(root); int depth = 0; while (!deque.isEmpty()) { int size = deque.size(); depth++; for (int i = 0; i &lt; size; i++) { TreeNode node = deque.poll(); if (node.left != null) { deque.offer(node.left); } if (node.right != null) { deque.offer(node.right); } } } return depth; }} &amp;PythonResursion 12345class solution: def maxdepth(self, root: treenode) -&gt; int: if not root: return 0 return 1 + max(self.maxdepth(root.left), self.maxdepth(root.right)) Iteration 123456789101112131415161718import collectionsclass solution: def maxdepth(self, root: treenode) -&gt; int: if not root: return 0 depth = 0 #record depth queue = collections.deque() queue.append(root) while queue: size = len(queue) depth += 1 for i in range(size): node = queue.popleft() if node.left: queue.append(node.left) if node.right: queue.append(node.right) return depth","link":"/2022/05/31/MaximumDepthofBinaryTree/"},{"title":"Merge Two Binary Trees","text":"TEST YOURSELFYou are given two binary trees root1 and root2. Imagine that when you put one of them to cover the other, some nodes of the two trees are overlapped while the others are not. You need to merge the two trees into a new binary tree. The merge rule is that if two nodes overlap, then sum node values up as the new value of the merged node. Otherwise, the NOT null node will be used as the node of the new tree. Return the merged tree. Note: The merging process must start from the root nodes of both trees. Example 1: 123Input: root1 = [1,3,2,5], root2 = [2,1,3,null,4,null,7]Output: [3,4,5,5,4,null,7] HINTIt’s actually the same logic as traversing a tree, except that the nodes of both trees are passed in and operated on simultaneously. ResursionUse pre-order as an example: Determine the parameters and return value of the recursive function: To merge two binary trees, then the argument is passed at least the root node of both binary trees, and the return value is the root node of the merged binary tree. 1TreeNode* mergeTrees(TreeNode* t1, TreeNode* t2) { Determine the termination condition: Since two trees are passed in, there will be two trees traversing nodes t1 and t2. If t1 == NULL, the two trees will be merged into t2 (it doesn’t matter if t2 is also NULL, it will be NULL after the merge). If t2 == NULL, then the two numbers are merged into t1 (it doesn’t matter if t1 is also NULL, the merge is NULL). 12if (t1 == NULL) return t2; if (t2 == NULL) return t1; The logic for determining the single-level recursion: Adding the elements of two trees together 1t1-&gt;val += t2-&gt;val; The left subtree of t1 is: the left subtree after merging the left subtree of t1 and the left subtree of t2. The right subtree of t1 is: the right subtree after merging the right subtree of t1 and the right subtree of t2. The final t1 is the root node after the merge. 123t1-&gt;left = mergeTrees(t1-&gt;left, t2-&gt;left);t1-&gt;right = mergeTrees(t1-&gt;right, t2-&gt;right);return t1; &amp;C++ Complete code 12345678910111213 class Solution {public: TreeNode* mergeTrees(TreeNode* t1, TreeNode* t2) { if (t1 == NULL) return t2; if (t2 == NULL) return t1; // re-define new node, don't modity structure of the 2 old tree TreeNode* root = new TreeNode(0); root-&gt;val = t1-&gt;val + t2-&gt;val; root-&gt;left = mergeTrees(t1-&gt;left, t2-&gt;left); root-&gt;right = mergeTrees(t1-&gt;right, t2-&gt;right); return root; }}; Iteration&amp;C++ 12345678910111213141516171819202122232425262728293031323334353637class Solution {public: TreeNode* mergeTrees(TreeNode* t1, TreeNode* t2) { if (t1 == NULL) return t2; if (t2 == NULL) return t1; queue&lt;TreeNode*&gt; que; que.push(t1); que.push(t2); while(!que.empty()) { TreeNode* node1 = que.front(); que.pop(); TreeNode* node2 = que.front(); que.pop(); // add val node1-&gt;val += node2-&gt;val; // if thel left node of 2 trees are both not empty, enter queue if (node1-&gt;left != NULL &amp;&amp; node2-&gt;left != NULL) { que.push(node1-&gt;left); que.push(node2-&gt;left); } // if thel right node of 2 trees are both noe empty, enter queue if (node1-&gt;right != NULL &amp;&amp; node2-&gt;right != NULL) { que.push(node1-&gt;right); que.push(node2-&gt;right); } // when left node of t1=null, t2 !=null, set val if (node1-&gt;left == NULL &amp;&amp; node2-&gt;left != NULL) { node1-&gt;left = node2-&gt;left; } // when right node of t1=null, t2 !=null, set val if (node1-&gt;right == NULL &amp;&amp; node2-&gt;right != NULL) { node1-&gt;right = node2-&gt;right; } } return t1; }}; xxxxxxxxxx import collectionsclass solution: def maxdepth(self, root: treenode) -&gt; int: if not root: return 0 depth = 0 #record depth queue = collections.deque() queue.append(root) while queue: size = len(queue) depth += 1 for i in range(size): node = queue.popleft() if node.left: queue.append(node.left) if node.right: queue.append(node.right) return depthcpp Recursion 1234567891011class Solution { public TreeNode mergeTrees(TreeNode root1, TreeNode root2) { if (root1 == null) return root2; if (root2 == null) return root1; root1.val += root2.val; root1.left = mergeTrees(root1.left,root2.left); root1.right = mergeTrees(root1.right,root2.right); return root1; }} Iteration 1234567891011121314151617181920212223242526272829303132333435 class Solution { // using queue public TreeNode mergeTrees(TreeNode root1, TreeNode root2) { if (root1 == null) return root2; if (root2 ==null) return root1; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root1); queue.offer(root2); while (!queue.isEmpty()) { TreeNode node1 = queue.poll(); TreeNode node2 = queue.poll(); node1.val = node1.val + node2.val; if (node1.left != null &amp;&amp; node2.left != null) { queue.offer(node1.left); queue.offer(node2.left); } if (node1.right != null &amp;&amp; node2.right != null) { queue.offer(node1.right); queue.offer(node2.right); } if (node1.left == null &amp;&amp; node2.left != null) { node1.left = node2.left; } if (node1.right == null &amp;&amp; node2.right != null) { node1.right = node2.right; } } return root1; }} &amp;Python Recursion 12345678910111213141516171819# Definition for a binary tree node.# class TreeNode:# def __init__(self, val=0, left=None, right=None):# self.val = val# self.left = left# self.right = rightclass Solution: def mergeTrees(self, root1: TreeNode, root2: TreeNode) -&gt; TreeNode: if not root1: return root2 if not root2: return root1 root1.val += root2.val # 中 root1.left = self.mergeTrees(root1.left, root2.left) #left root1.right = self.mergeTrees(root1.right, root2.right) # right return root1 Iteration 1234567891011121314151617181920212223242526272829303132class Solution: def mergeTrees(self, root1: TreeNode, root2: TreeNode) -&gt; TreeNode: if not root1: return root2 if not root2: return root1 queue = deque() queue.append(root1) queue.append(root2) while queue: node1 = queue.popleft() node2 = queue.popleft() # update queue if node1.left and node2.left: queue.append(node1.left) queue.append(node2.left) if node1.right and node2.right: queue.append(node1.right) queue.append(node2.right) node1.val += node2.val if not node1.left and node2.left: node1.left = node2.left if not node1.right and node2.right: node1.right = node2.right return root1","link":"/2022/06/30/MergeTwoBinaryTrees/"},{"title":"Remove Linked List Elements","text":"Given the head of a linked list and an integer val, remove all the nodes of the linked list that has Node.val == val, and return the new head. 123Example1:Input: head = [1,2,6,3,4,5,6], val = 6Output: [1,2,3,4,5] 123Example2:Input: head = [7,7,7,7], val = 7Output: [] HINT If we use C, after deleting the target node, we also need to delete the original memory of the deleted node If we use Java or Python we don’t need to manage memory manually The following two ways of manipulating a linked list are involved here. Using the original linked list Set a dummy head node 1. Using the original linked listRemoving the head node is not the same as removing other nodes, as the other nodes in the chain are removed from the current node by the previous node, whereas the head node does not have a previous node. So how do you remove the head node? In fact, all you have to do is move the head node back one position, which removes a head node from the chain. Don’t forget to delete the original head node from memory. 2. Set a dummy head nodeRemoving the head node in a single linked list is not the same as removing other nodes, so could there be a uniform logic for removing nodes in a linked list? It is actually possible to set up a dummy head node so that all nodes of the original chain list can be removed in a uniform manner.To show how to set up a dummy head node. we are still in this linked list, remove element 1.Here’s how to add a dummy head node as a new head node to the linked list, removing this old head node element 1 at this point. &amp;C 1234567891011121314151617181920212223242526#Type1: Use Original Linked listclass Solution {public: ListNode* removeElements(ListNode* head, int val) { // delete head node while (head != NULL &amp;&amp; head-&gt;val == val) { // NOtice:not if here ListNode* tmp = head; head = head-&gt;next; delete tmp; } // delete non-head node ListNode* cur = head; while (cur != NULL &amp;&amp; cur-&gt;next!= NULL) { if (cur-&gt;next-&gt;val == val) { ListNode* tmp = cur-&gt;next; cur-&gt;next = cur-&gt;next-&gt;next; delete tmp; } else { cur = cur-&gt;next; } } return head; }}; 12345678910111213141516171819202122#Type2: Set a dummy head nodeclass Solution {public: ListNode* removeElements(ListNode* head, int val) { ListNode* dummyHead = new ListNode(0); // Set a dummy head node dummyHead-&gt;next = head; // let the dummy head node point to head so that we can delete target later ListNode* cur = dummyHead; while (cur-&gt;next != NULL) { if(cur-&gt;next-&gt;val == val) { ListNode* tmp = cur-&gt;next; cur-&gt;next = cur-&gt;next-&gt;next; delete tmp; } else { cur = cur-&gt;next; } } head = dummyHead-&gt;next; delete dummyHead; return head; }}; &amp;Java 1234567891011121314151617181920#Type1: set dummy nodepublic ListNode removeElements(ListNode head, int val) { if (head == null) { return head; } // since the delete operation refers to head node, so we set a dummy node ListNode dummy = new ListNode(-1, head); ListNode pre = dummy; ListNode cur = head; while (cur != null) { if (cur.val == val) { pre.next = cur.next; } else { pre = cur; } cur = cur.next; } return dummy.next;} 12345678910111213141516171819202122#Type2: set a pre nodepublic ListNode removeElements(ListNode head, int val) { while (head != null &amp;&amp; head.val == val) { head = head.next; } if (head == null) { return head; } ListNode pre = head; ListNode cur = head.next; while (cur != null) { if (cur.val == val) { pre.next = cur.next; } else { pre = cur; } cur = cur.next; } return head;} 123456789101112131415#Type3: Do not set a dummy node or pre nodepublic ListNode removeElements(ListNode head, int val) { while(head!=null &amp;&amp; head.val==val){ head = head.next; } ListNode curr = head; while(curr!=null){ while(curr.next!=null &amp;&amp; curr.next.val == val){ curr.next = curr.next.next; } curr = curr.next; } return head;} Time Complexity: O(n)Space Complexty: O(1)","link":"/2021/11/28/RemoveLinkedListElements/"},{"title":"LinkedList Basic","text":"1.The type of LinkedlistWhat is a linked list? A linked list is a linear data structure linked together by pointers. Each node consists of two parts, a data field and a pointer field (which holds a pointer to the next node), with the last node having a pointer field pointing to null (meaning a null pointer) The start node of the link is called the head node of the chain. 1. Single Linked List A pointer field in a single-linked list can only point to its next node. 2. Double Linked List Each node has two pointer fields, one pointing to the next node and one pointing to the previous node. 3. Circular Linked List The circular linked list is a linked list where all nodes are connected to form a circle. In a circular linked list, the first node and the last node are connected to each other which forms a circle. There is no NULL at the end. ![Example of Circular Linked List)(Circular.jpg) 2.Memory of Linked listA linked list is linked to nodes in memory by pointers in the pointer field.Therefore, the nodes are not distributed continuously in memory, but are scattered at certain addresses in memory, and the allocation mechanism depends on the memory management of the operating system. shown as follow: 3. Definition of Linked listC/C++ 123456// Single Linked Liststruct ListNode { int val; // value of this node ListNode *next; // the pointer ListNode(int x) : val(x), next(NULL) {} // struct function}; Java 1234567891011121314151617181920212223public class ListNode { // the value of node int val; // next nohde ListNode next; // the struct function of node(no parameter) public ListNode() { } // the struct function of node(one parameter) public ListNode(int val) { this.val = val; } //the struct function of node(two parameter) public ListNode(int val, ListNode next) { this.val = val; this.next = next; }}}; 3.The Operation of Linked List Detele Node![] if we want to delete node D, we should only change the pointer of C to Node E. Add Node","link":"/2021/11/07/LinkedList%20Basic/"},{"title":"Minimum Depth of Binary Tree","text":"TEST YOURSELFGiven a binary tree, find its minimum depth. The minimum depth is the number of nodes along the shortest path from the root node down to the nearest leaf node. Note: A leaf is a node with no children. Example 1: 12Input: root = [3,9,20,null,null,15,7]Output: 2 Example 2: 12Input: root = [2,null,3,null,4,null,5,null,6]Output: 5 HINTIntuitively, it seems to be similar to finding the maximum depth, but in fact it is still quite different. In the following explanation, we still use post-ordertraversal in the order of traversal (because we want to compare the results after the recursive returns) We first need to understand the definition of minimum depth: the minimum depth is the number of nodes on the shortest path from the root node to the nearest leaf node Recursion Determine the parameters and return value of the recursive function: The parameter is the root node of the binary tree to be passed in, and the depth returned is of type int. 1int getDepth(TreeNode* node) Determine the termination condition: if the node is empty, 0 is returned, indicating a height of 0. 1if (node == NULL) return 0; The logic for determining the single-level recursion: xxxxxxxxxx class Solution {​ public int minSubArrayLen(int s, int[] nums) { int left = 0; int sum = 0; int result = Integer.MAX_VALUE; for (int right = 0; right &lt; nums.length; right++) { sum += nums[right]; while (sum &gt;= s) { result = Math.min(result, right - left + 1); sum -= nums[left++]; } } return result == Integer.MAX_VALUE ? 0 : result; }}cpp Conversely, if the right subtree is empty and the left subtree is not empty, the minimum depth is: 1 + the depth of the left subtree. Finally, if neither the left nor the right subtree is empty, return the minimum depth of the left or right subtree+1. 12345678910// it is not the most deep node when the left subtree is empty and the right subtree is not emptyif (node-&gt;left == NULL &amp;&amp; node-&gt;right != NULL) { return 1 + rightDepth;} // it is not the most deep node when the left subtree is not empty and the right subtree is emptyif (node-&gt;left != NULL &amp;&amp; node-&gt;right == NULL) { return 1 + leftDepth;}int result = 1 + min(leftDepth, rightDepth);return result; &amp;C++ 1234567891011121314class Solution {public: int minDepth(TreeNode* root) { if (root == NULL) return 0; if (root-&gt;left == NULL &amp;&amp; root-&gt;right != NULL) { return 1 + minDepth(root-&gt;right); } if (root-&gt;left != NULL &amp;&amp; root-&gt;right == NULL) { return 1 + minDepth(root-&gt;left); } return 1 + min(minDepth(root-&gt;left), minDepth(root-&gt;right)); }}; Iteration &amp;C++ 123456789101112131415161718192021222324class Solution {public: int minDepth(TreeNode* root) { if (root == NULL) return 0; int depth = 0; queue&lt;TreeNode*&gt; que; que.push(root); while(!que.empty()) { int size = que.size(); depth++; // reord for (int i = 0; i &lt; size; i++) { TreeNode* node = que.front(); que.pop(); if (node-&gt;left) que.push(node-&gt;left); if (node-&gt;right) que.push(node-&gt;right); if (!node-&gt;left &amp;&amp; !node-&gt;right) { return depth; } } } return depth; }}; &amp;Java Resursion 1234567891011121314151617181920class Solution { public int minDepth(TreeNode root) { if (root == null) { return 0; } int leftDepth = minDepth(root.left); int rightDepth = minDepth(root.right); if (root.left == null) { return rightDepth + 1; } if (root.right == null) { return leftDepth + 1; } // both left node and right node are not empty return Math.min(leftDepth, rightDepth) + 1; }} Iteration 1234567891011121314151617181920212223242526272829class Solution { public int minDepth(TreeNode root) { if (root == null) { return 0; } Deque&lt;TreeNode&gt; deque = new LinkedList&lt;&gt;(); deque.offer(root); int depth = 0; while (!deque.isEmpty()) { int size = deque.size(); depth++; for (int i = 0; i &lt; size; i++) { TreeNode poll = deque.poll(); if (poll.left == null &amp;&amp; poll.right == null) {# // when iterate leaf node ,return directly. since it is level oreder traversal, this value is minimum return depth; } if (poll.left != null) { deque.offer(poll.left); } if (poll.right != null) { deque.offer(poll.right); } } } return depth; }} &amp;Python Resursion 1234567891011121314class Solution: def minDepth(self, root: TreeNode) -&gt; int: if not root: return 0 if not root.left and not root.right: return 1 min_depth = 10**9 if root.left:# min_depth = min(self.minDepth(root.left), min_depth) # the minimum depth of left subtree if root.right: min_depth = min(self.minDepth(root.right), min_depth) # the minimum depth of right subtree return min_depth + 1 Iteration 12345678910111213class Solution: def minDepth(self, root: TreeNode) -&gt; int: if not root: return 0 if not root.left and not root.right: return 1 min_depth = 10**9 if root.left: min_depth = min(self.minDepth(root.left), min_depth) # the minimum depth of left subtree if root.right: min_depth = min(self.minDepth(root.right), min_depth) # the minimum depth of right subtree return min_depth + 1","link":"/2022/06/14/MinimumDepthofBinaryTree/"},{"title":"Repeated Substring Pattern(2)","text":"There is a problem with move matching: we end up with the process of determining whether a string (s + s) has an occurrence of s. We could just use libraries like contains, find and so on. This ignores the time complexity of implementing these functions (the brute force solution is m * n, and the general library implementation is O(m + n))","link":"/2022/03/11/RepeatedSubstringPattern(2)/"},{"title":"Replace spaces","text":"TEST YOURSELFPlease implement a function that replaces each space in the string s with “%20”. 1234Example 1:Input: s = &quot;We are happy.&quot;Output: &quot;We%20are%20happy.&quot; HINTThe highlight of this puzzle is that no extra auxiliary space is used. Step1: Expand the array to the size after each space has been replaced with “%20”. Step2: Replace the spaces from back to front, i.e. the double pointer method, as follows: i points to the end of the new length and j points to the end of the old length. Why do we need to fill from back to front?Filling from front to back is an O(n^2) algorithm, because each time an element is added, all the elements after the added element are moved backwards In fact, for many array padding problems, we can pre-fill the array with the size of the padding, and then work backwards and forwards. Doing this has two advantages: Don’t need to request new arrays. By filling the elements backwards and forwards, we avoid having to move all the elements backwards after the added elements each time they are added. &amp;C++12345678910111213141516171819202122232425262728class Solution {public: string replaceSpace(string s) { int count = 0; // count the number of spaces int sOldSize = s.size(); for (int i = 0; i &lt; s.size(); i++) { if (s[i] == ' ') { count++; } } // expand the size of string s(after replacing 20%) s.resize(s.size() + count * 2); int sNewSize = s.size(); // replace spaces with 20% from back to front for (int i = sNewSize - 1, j = sOldSize - 1; j &lt; i; i--, j--) { if (s[j] != ' ') { s[i] = s[j]; } else { s[i] = '0'; s[i - 1] = '2'; s[i - 2] = '%'; i -= 2; } } return s; }}; Time Complexity：O(n) Space Complexity：O(1) &amp;C12345678910111213141516171819202122232425262728char* replaceSpace(char* s){ int count = 0; int len = strlen(s); for (int i = 0; i &lt; len; i++) { if (s[i] == ' ') { count++; } } int newLen = len + count * 2; char* result = malloc(sizeof(char) * newLen + 1); for (int i = len - 1, j = newLen - 1; i &gt;= 0; i--, j--) { if (s[i] != ' ') { result[j] = s[i]; } else { result[j--] = '0'; result[j--] = '2'; result[j] = '%'; } } result[newLen] = '\\0'; return result;} &amp;Java1234567891011121314151617181920212223242526272829303132public String replaceSpace(String s) { if(s == null || s.length() == 0){ return s; } //expand size, double number of spaces StringBuilder str = new StringBuilder(); for (int i = 0; i &lt; s.length(); i++) { if(s.charAt(i) == ' '){ str.append(&quot; &quot;); } } // no space, return s if(str.length() == 0){ return s; } //double pointer int left = s.length() - 1;//left： point to end of the old string s += str.toString(); int right = s.length()-1;//right：point to end of the new string char[] chars = s.toCharArray(); while(left&gt;=0){ if(chars[left] == ' '){ chars[right--] = '0'; chars[right--] = '2'; chars[right] = '%'; }else{ chars[right] = chars[left]; } left--; right--; } return new String(chars);}","link":"/2022/02/11/Replacespaces/"},{"title":"Reverse Linked List","text":"Given the head of a singly linked list, reverse the list, and return the reversed list. 123Input: head = [1,2,3,4,5]Output: [5,4,3,2,1] HINTIt would actually be a waste of memory space to define a new chain table and implement the inversion of the elements of the chain table. In fact, you only need to change the pointing of the next pointer of the chain table to invert it directly without redefining a new chain table, as shown in the figure: Previously the head node of the chain was element 1, after reversing it the head node is element 5, no nodes are added or removed here, just the direction of the next pointer. Reverse flow: Firstly, define a cur pointer to the head node, and a pre pointer, initialized to null. Then it’s time to reverse the process, first by saving the cur-&gt;next node with the tmp pointer, that is, by saving this node. Why save this node? Because the next step is to change the cur-&gt;next pointer, so that cur-&gt;next points to pre, which has already reversed the first node. The next step is to loop through the following code logic, continuing to move the pre and cur pointers. Finally, the cur pointer is pointing to null, the loop is over and the chain is reversed. At this point we return the pre pointer and the pre pointer points to the new head node. &amp;C++1.Double Pointer 123456789101112131415161718class Solution {public: ListNode* reverseList(ListNode* head) { ListNode* temp; // 保存cur的下一个节点 ListNode* cur = head; ListNode* pre = NULL; while(cur) { temp = cur-&gt;next; // 保存一下 cur的下一个节点，因为接下来要改变cur-&gt;next cur-&gt;next = pre; // 翻转操作 // 更新pre 和 cur指针 pre = cur; cur = temp; } return pre; }}; 2.Recursion 12345678910111213141516171819public: ListNode* reverse(ListNode* pre,ListNode* cur){ if(cur == NULL) return pre; ListNode* temp = cur-&gt;next; cur-&gt;next = pre; // 可以和双指针法的代码进行对比，如下递归的写法，其实就是做了这两步 // pre = cur; // cur = temp; return reverse(cur,temp); } ListNode* reverseList(ListNode* head) { // 和双指针法初始化是一样的逻辑 // ListNode* cur = head; // ListNode* pre = NULL; return reverse(NULL, head); }}; &amp;Java1.Double Pointer 12345678910111213141516class Solution { public ListNode reverseList(ListNode head) { ListNode prev = null; ListNode cur = head; ListNode temp = null; while (cur != null) { temp = cur.next;// 保存下一个节点 cur.next = prev; prev = cur; cur = temp; } return prev; }} 2.Recursion 12345678910111213141516171819class Solution { public ListNode reverseList(ListNode head) { return reverse(null, head); } private ListNode reverse(ListNode prev, ListNode cur) { if (cur == null) { return prev; } ListNode temp = null; temp = cur.next;// 先保存下一个节点 cur.next = prev;// 反转 // 更新prev、cur位置 // prev = cur; // cur = temp; return reverse(cur, temp); }} &amp;Python1.Double Pointer 123456789101112131415161718# Definition for singly-linked list.# class ListNode:# def __init__(self, val=0, next=None):# self.val = val# self.next = nextclass Solution: def reverseList(self, head: ListNode) -&gt; ListNode: cur = head pre = None while(cur!=None): temp = cur.next # 保存一下 cur的下一个节点，因为接下来要改变cur-&gt;next cur.next = pre #反转 #更新pre、cur指针 pre = cur cur = temp return pre 2.Recursion 12345678910111213141516171819# Definition for singly-linked list.# class ListNode:# def __init__(self, val=0, next=None):# self.val = val# self.next = nextclass Solution: def reverseList(self, head: ListNode) -&gt; ListNode: def reverse(pre,cur): if not cur: return pre tmp = cur.next cur.next = pre return reverse(cur,tmp) return reverse(None,head)","link":"/2021/12/28/ReverseLinkedList/"},{"title":"Repeated Substring Pattern(1)","text":"TEST YOURSELFGiven a string s, check if it can be constructed by taking a substring of it and appending multiple copies of the substring together. 12345Example 1:Input: s = &quot;abab&quot;Output: trueExplanation: It is the substring &quot;ab&quot; twice. 1234Example 2:Input: s = &quot;aba&quot;Output: false HINTThe violent solution is a for loop to get the termination position of the substring, and then to determine whether the substring can be repeated to form a string, nested in another for loop, so it is O(n^2) time complexity. Here is an Better solution: Move Matching Move MatchingWhen a string s: abcabc is composed of repeated substrings within it, then the structure of the string must be such that: That is, it consists of the same substring before and after it. Then since the same substring precedes and follows the same substring, using s + s.In a string thus formed, the later substring is treated as the previous string and the earlier substring as the later string, and there must still be an s formed, as in the figure: So to determine whether the string s is composed of repeated substrings, as long as two s‘ s are spliced together and there is still an s in it, it means that s is composed of repeated substrings. Of course, when we determine whether an s appears in the s + s spliced string, we shave off the first and last characters of s + s, so as to avoid searching for the original s in s + s. What we want to search for is the s spliced out in the middle. &amp;C++12345678910class Solution {public: bool repeatedSubstringPattern(string s) { string t = s + s; t.erase(t.begin()); t.erase(t.end() - 1); // remove head and tail if (t.find(s) != std::string::npos) return true; // r return false; }};","link":"/2022/02/28/RepeatedSubstringPattern(1)/"},{"title":"Linked List Cycle","text":"Given the head of a linked list, return the node where the cycle begins. If there is no cycle, return null. There is a cycle in a linked list if there is some node in the list that can be reached again by continuously following the next pointer. Internally, pos is used to denote the index of the node that tail’s next pointer is connected to (0-indexed). It is -1 if there is no cycle. Note that pos is not passed as a parameter. Do not modify the linked list. 12345Example 1:Input: head = [3,2,0,-4], pos = 1Output: tail connects to node index 1Explanation: There is a cycle in the linked list, where tail connects to the second node. 12345Example 2:Input: head = [1,2], pos = 0Output: tail connects to node index 0Explanation: There is a cycle in the linked list, where tail connects to the first node. HINTThere are two main breakthrough points. Determining whether there is a ring in this linked list If there is a ring, how to find the entrance to the ring 1. Determining whether there is a ring in this linked list We can use the fast and slow pointer method to define fast and slow pointers respectively, starting from the head node, the fast pointer moves two nodes at a time, the slow pointer moves one node at a time, if the fast and slow pointers meet on the way, it means that the list has a ring. Why does the fast pointer move two nodes and the slow pointer move one node and if there is a ring, they must meet inside the ring, instead of always being staggered? First of all, the fast pointer must enter the ring first. If the fast pointer and the slow pointer meet, they must meet in the ring, there is no doubt about that. So let’s see, why must the fast pointer and the slow pointer meet? We can draw a ring and have the fast pointer start to catch up with the slow pointer at any one of the nodes. We will find that this is what happens in the end, as follows: fast and slow each take one more step, and fast and slow meet This is because fast takes two steps and slow takes one step. In fact, compared to slow, fast is approaching slow node by node, so fast must be able to overlap with slow. 2. If there is a ring, how to find the entrance to the ringAssume that the number of nodes from the head node to the ring entry node is x. The number of nodes from the ring entry node to the node where the fast pointer meets the slow pointer is y. The number of nodes from the meeting node to the ring entry node is z. As shown in the figure. Then when the fast pointer and slow pointer meet: the number of nodes walked by the slow pointer is: x + y, the number of nodes walked by the fast pointer is: x + y + n (y + z), n is the number of n turns in the ring before the fast pointer meets the slow pointer, and (y + z) is the number of nodes in a turn A. Since the fast pointer takes two nodes in one step and the slow pointer takes one node in one step, the number of nodes taken by the fast pointer = the number of nodes taken by the slow pointer * 2: (x + y) * 2 = x + y + n (y + z) eliminating one (x + y) on both sides: x + y = n (y + z) Since the entrance to the ring is to be found, then x is required because x represents the distance from the head node to the entrance node of the ring. So put x alone on the left: x = n (y + z) - y , Then from n(y + z), we can put a (y + z), and after sorting out the formula, we have the following formula: x = (n - 1) (y + z) + z. Note that n must be larger than or equal to 1 here, because the fast pointer must move at least one more turn to meet the slow pointer. What does this formula show? where n is 1. This means that the fast pointer encounters the slow pointer after one turn in the loop. When n is 1, the formula resolves to x = z This means that one pointer from the head node and one pointer from the meeting node, both of which go one node at a time, will be the node at the entrance to the ring when they meet. This means that at the meeting node, a pointer index1 is defined, and at the head node, a pointer index2 is defined. Let index1 and index2 move simultaneously, one node at a time, so that where they meet is the node at the entrance to the ring. where n is larger than 1, the fast pointer turns n times in the ring before encountering the slow pointer. In fact, the effect is the same as when n is 1. You can find the entrance node of the ring by this method, except that the index1 pointer takes an extra (n-1) turns in the ring and then meets index2, which is still the entrance node of the ring &amp;C++123456789101112131415161718192021222324252627282930/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */class Solution {public: ListNode *detectCycle(ListNode *head) { ListNode* fast = head; ListNode* slow = head; while(fast != NULL &amp;&amp; fast-&gt;next != NULL) { slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; // exsit a ring if (slow == fast) { ListNode* index1 = fast; ListNode* index2 = head; while (index1 != index2) { index1 = index1-&gt;next; index2 = index2-&gt;next; } return index2; // return the entrance of ring } } return NULL; }}; &amp;Java123456789101112131415161718192021public class Solution { public ListNode detectCycle(ListNode head) { ListNode slow = head; ListNode fast = head; while (fast != null &amp;&amp; fast.next != null) { slow = slow.next; fast = fast.next.next; if (slow == fast) {// exsit a ring ListNode index1 = fast; ListNode index2 = head; // the encounter nodeis the entrance of ring while (index1 != index2) { index1 = index1.next; index2 = index2.next; } return index1; } } return null; }}","link":"/2022/01/11/LinkedListCycle/"},{"title":"Reverse String","text":"TEST YOURSELFWrite a function that reverses a string. The input string is given as an array of characters s. You must do this by modifying the input array in-place with O(1) extra memory. 1234Example 1:Input: s = [&quot;h&quot;,&quot;e&quot;,&quot;l&quot;,&quot;l&quot;,&quot;o&quot;]Output: [&quot;o&quot;,&quot;l&quot;,&quot;l&quot;,&quot;e&quot;,&quot;h&quot;] 1234Example 2:Input: s = [&quot;H&quot;,&quot;a&quot;,&quot;n&quot;,&quot;n&quot;,&quot;a&quot;,&quot;h&quot;]Output: [&quot;h&quot;,&quot;a&quot;,&quot;n&quot;,&quot;n&quot;,&quot;a&quot;,&quot;H&quot;] HINTIn inverting a linked list, the double pointer method is used. Inverting strings is still done using the double pointer method, except that it is actually simpler to invert strings than list. Since strings are also arrays, the elements are distributed continuously in memory, which makes the difference between inverting a chain table and inverting a string. For strings, we define two pointers (or index subscripts), one from the front of the string and one from the back of the string, and both pointers move towards the middle at the same time and swap elements. Using the string hello as an example, the procedure is as follows: &amp;C++12345void reverseString(vector&lt;char&gt;&amp; s) { for (int i = 0, j = s.size() - 1; i &lt; s.size()/2; i++, j--) { swap(s[i],s[j]); }} &amp;Java12345678910111213class Solution { public void reverseString(char[] s) { int l = 0; int r = s.length - 1; while (l &lt; r) { s[l] ^= s[r]; s[r] ^= s[l]; s[l] ^= s[r]; l++;# r--; } }} &amp;Python1234567891011class Solution: def reverseString(self, s: List[str]) -&gt; None: &quot;&quot;&quot; Do not return anything, modify s in-place instead. &quot;&quot;&quot; left, right = 0, len(s) - 1 while left &lt; right: s[left], s[right] = s[right], s[left] left += 1 right -= 1","link":"/2022/01/30/ReverseString/"},{"title":"Recursive Traversal of Binary Tree","text":"Three elements of the recursive algorithm： Determine the parameters and return value of the recursive function: Determine which parameters need to be handled during the recursion process, then add this parameter to the recursive function, and also define what the return value of each recursion is and thus determine the return type of the recursive function. Determine the termination conditions: After writing a recursive algorithm, when running it, we will often encounter a stack overflow error, that is, we have not written the termination conditions or the termination conditions are not written correctly. The logic for determining a single level of recursion: Determine the information that needs to be processed at each level of recursion. Here too, the call itself is repeated to implement the recursion process. Example:The following is an example of a preorder traversal. Determine the parameters of the recursive function and return value: because to print out the value of the node of the preorder traversal, so the parameters need to be passed in the vector in the value of the node put, in addition to this point will not need to deal with what data is also not required to have a return value, so the recursive function return type is void: 1void traversal(TreeNode* cur, vector&lt;int&gt;&amp; vec) Determine the termination conditions: in the process of recursion, how is the recursion is considered to end it, of course, the current traversal of the node is empty, then this layer of recursion is going to end, so if the current traversal of this node is empty, it is a direct return: 1if (cur == NULL) return; Determine the logic of the single-level recursion: preorder traversal is the middle-left-right order, so the logic in the single-level recursion, is to first take the value of the node: 123vec.push_back(cur-&gt;val); // intraversal(cur-&gt;left, vec); //lefttraversal(cur-&gt;right, vec); // right Pre-Order: (Visit the parent node first, then the left and right nodes in turn) 1234567891011121314class Solution {public: void traversal(TreeNode* cur, vector&lt;int&gt;&amp; vec) { if (cur == NULL) return; vec.push_back(cur-&gt;val); // parent traversal(cur-&gt;left, vec); // left traversal(cur-&gt;right, vec); // right } vector&lt;int&gt; preorderTraversal(TreeNode* root) { vector&lt;int&gt; result; traversal(root, result); return result; }}; In-Order：(Visit the left node first, then the parent and right nodes in turn) 123456void traversal(TreeNode* cur, vector&lt;int&gt;&amp; vec) { if (cur == NULL) return; traversal(cur-&gt;left, vec); // left vec.push_back(cur-&gt;val); // parent traversal(cur-&gt;right, vec); // right} Post-Order：(Visit the left node first, then the right and parent nodes in turn) 123456void traversal(TreeNode* cur, vector&lt;int&gt;&amp; vec) { if (cur == NULL) return; traversal(cur-&gt;left, vec); // left traversal(cur-&gt;right, vec); // right vec.push_back(cur-&gt;val); // parent} &amp;Java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// Pre-Order-Recursionclass Solution { public List&lt;Integer&gt; preorderTraversal(TreeNode root) { List&lt;Integer&gt; result = new ArrayList&lt;Integer&gt;(); preorder(root, result); return result; } public void preorder(TreeNode root, List&lt;Integer&gt; result) { if (root == null) { return; } result.add(root.val); preorder(root.left, result); preorder(root.right, result); }}// In-Order-Recursionclass Solution { public List&lt;Integer&gt; inorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); inorder(root, res); return res; } void inorder(TreeNode root, List&lt;Integer&gt; list) { if (root == null) { return; } inorder(root.left, list); list.add(root.val); // inorder(root.right, list); }}// Post-Order-Recursionclass Solution { public List&lt;Integer&gt; postorderTraversal(TreeNode root) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); postorder(root, res); return res; } void postorder(TreeNode root, List&lt;Integer&gt; list) { if (root == null) { return; } postorder(root.left, list); postorder(root.right, list); list.add(root.val); // }} &amp;Python 12345678910111213141516171819202122232425262728293031323334353637383940414243444546 # Pre-Order-Recursionclass Solution: def preorderTraversal(self, root: TreeNode) -&gt; List[int]: # save result result = [] def traversal(root: TreeNode): if root == None: return result.append(root.val) # parent traversal(root.left) # left traversal(root.right) # right traversal(root) return result# In-Order-Recursionclass Solution: def inorderTraversal(self, root: TreeNode) -&gt; List[int]: result = [] def traversal(root: TreeNode): if root == None: return traversal(root.left) # left result.append(root.val) # parent traversal(root.right) # right traversal(root) return result# Post-Order-Recursionclass Solution: def postorderTraversal(self, root: TreeNode) -&gt; List[int]: result = [] def traversal(root: TreeNode): if root == None: return traversal(root.left) # left traversal(root.right) # right result.append(root.val) # parent traversal(root) return result","link":"/2022/04/12/RecursiveTraversalofBinaryTree/"},{"title":"Swap Nodes in Pairs","text":"Given a linked list, swap every two adjacent nodes and return its head. You must solve the problem without modifying the values in the list’s nodes (i.e., only nodes themselves may be changed.) 1234Example 1:Input: head = [1,2,3,4]Output: [2,1,4,3] 1234Example 2:Input: head = []Output: [] HINTWe can draw a diagram to avoid confusion Initially, cur points to the dummy head node, and then the following three steps are performed. After the operation, the linked list looks like this. It might be a little more intuitive to look at this: &amp;C++1234567891011121314151617181920class Solution {public: ListNode* swapPairs(ListNode* head) { ListNode* dummyHead = new ListNode(0); // set a dummy node dummyHead-&gt;next = head; // let dummy node point to head so t hat we can operate deletion ListNode* cur = dummyHead; while(cur-&gt;next != nullptr &amp;&amp; cur-&gt;next-&gt;next != nullptr) { ListNode* tmp = cur-&gt;next; // record temp node ListNode* tmp1 = cur-&gt;next-&gt;next-&gt;next; //record temp node cur-&gt;next = cur-&gt;next-&gt;next; // Step1 cur-&gt;next-&gt;next = tmp; // Step2 cur-&gt;next-&gt;next-&gt;next = tmp1; // Step3 cur = cur-&gt;next-&gt;next; // cur node moves 2 places, which prepare to next change operation } return dummyHead-&gt;next; }}; Time Complexity: O(n) Space Complexity: O(1) xxxxxxxxxx ​class Solution: def reverseString(self, s: List[str]) -&gt; None: “”” Do not return anything, modify s in-place instead. “”” left, right = 0, len(s) - 1 while left &lt; right: s[left], s[right] = s[right], s[left] left += 1 right -= 1cpp1234567891011121314151617// Resursionclass Solution { public ListNode swapPairs(ListNode head) { // base case if(head == null || head.next == null) return head; // get next node of this node ListNode next = head.next; // recursion ListNode newNode = swapPairs(next.next); // change operation next.next = head; head.next = newNode; return next; }} 12345678910111213141516171819// Dummy nodeclass Solution { public ListNode swapPairs(ListNode head) { ListNode dummyNode = new ListNode(0); dummyNode.next = head; ListNode prev = dummyNode; while (prev.next != null &amp;&amp; prev.next.next != null) { ListNode temp = head.next.next; // temp next node prev.next = head.next; // let next of prev change to next of head head.next.next = head; // let next of head.next(prev.next) points to head head.next = temp; // let temp of head =temp prev = head; // move forward 1 place head = head.next; // move forward 1 place } return dummyNode.next; }}","link":"/2021/12/28/SwapNodesinPairs/"},{"title":"Best Time to Buy and Sell StockII","text":"TEST YOURSELFGiven an array of integers prices, where prices[i] represents the price of a particular stock on day i. On each day, you can decide whether to buy and/or sell the stock. You can only hold a maximum of one share of stock at any one time. You can also buy and then sell on the same day. Returns the maximum profit you can make. Example1:12345Input: prices = [7,1,5,3,6,4]Output: 7Explanation: Buy on day 2 (stock price = 1) and sell on day 3 (stock price = 5), the profit from this transaction = 5 - 1 = 4. Subsequently, buy on day 4 (stock price = 3) and sell on day 5 (stock price = 6), the profit from this transaction = 6 - 3 = 3. The total profit is 4 + 3 = 7. Example2:1234Input: prices = [1,2,3,4,5]Output: 4Explanation: If you buy on day 1 (stock price = 1) and sell on day 5 (stock price = 5), you will make a profit on this transaction = 5 - 1 = 4. The total profit is 4 HintThe first thing to be clear about this question is two things: There is only one stock Currently there are only operations to buy or sell stocks To get profit at least two days for a trading unit. Greedy SolutionThis is a question where we might just think, pick a low to buy, then pick a high to sell, then pick a low to buy ….. The cycle repeats itself. If you think about the fact that the final profit can be broken down, then this question is very easy! How does it break down? If you buy on day 0 and sell on day 3, then the profit is: prices[3] - prices[0]. This is equivalent to (prices[3] - prices[2]) + (prices[2] - prices[1]) + (prices[1] - prices[0]). At this point, you are breaking down the profit into daily dimensions, rather than considering it as a whole from day 0 to day 3! Then according to prices you can get the sequence of profits per day: (prices[i] - prices[i - 1])….. (prices[1] - prices[0]). As shown in the figure: Some people how can there be no profit on the first day, and does the first day count or not. Of course there is no profit on the first day, at least until the second day, so the sequence of profits is one day less than the stock sequence! From the chart, we can find that in fact, we need to collect positive profits every day can be, collect positive profits of the interval, is the stock buying and selling interval, and we only need to focus on the final profit, do not need to record the interval. Then only collect positive profits is where greed is greedy! Local optimum: collect positive profits every day, global optimum: find the maximum profit. The local optimum can lead to the global optimum, can’t find a counterexample, try greedy! &amp;C++12345678910class Solution {public: int maxProfit(vector&lt;int&gt;&amp; prices) { int result = 0; for (int i = 1; i &lt; prices.size(); i++) { result += max(prices[i] - prices[i - 1], 0); } return result; }}; **Time complexity: ** O(n) Space complexity: O(1) &amp;Java12345678910class Solution { public int maxProfit(int[] prices) { int result = 0; for (int i = 1; i &lt; prices.length; i++) { result += Math.max(prices[i] - prices[i - 1], 0); } return result; }} &amp;Python123456class Solution: def maxProfit(self, prices: List[int]) -&gt; int: result = 0 for i in range(1, len(prices)): result += max(prices[i] - prices[i - 1], 0) return result","link":"/2022/11/30/BestTimetoBuyandSellStockII/"},{"title":"Jumping Game","text":"TEST YOURSELFGiven an array of non-negative integers nums, you are initially at the first subscript of the array. Each element of the array represents the maximum length you can jump from that position. Determine whether you can reach the last subscript. Example1:123Input: nums = [2,3,1,1,4]Output: trueExplanation: You can jump 1 step from subscript 0 to subscript 1, and then jump 3 steps from subscript 1 to the last subscript. Example2:123Input: nums = [3,2,1,0,4]Output: falseExplanation: No matter what, it will always reach the position with subscript 3. But the maximum jump length of the subscript is 0, so it is never possible to reach the last subscript. HintWhen I first saw this question, I thought: if the current position element is 3, should I jump one step, or two steps, or three steps, and how many steps is optimal? In fact, it does not matter how many steps to jump, the key lies in the coverage of the jump! You don’t have to specify how many steps you want to jump at a time, but take the maximum number of steps you can jump each time, and this is the range you can jump in. Within this range, it doesn’t matter how you jump, you can definitely jump over it anyway. The question then becomes whether or not the jumping range covers the end of the line! We take the maximum number of jumps per move (to get the maximum coverage), and update the maximum coverage every time we move a unit. Greedy algorithm local optimal solution: each time to take the maximum number of jump steps (to take the maximum coverage), the overall optimal solution: finally get the overall maximum coverage, to see whether it can reach the end. Local optimal launch global optimal, can not find a counterexample, try greedy! As shown in the figure: ![](Jumping Game/img1.png) i can only move within the range of cover each time it moves, and for each element it moves, cover gets the element’s value (the new range of the cover) added, allowing i to keep moving. And cover only takes max (the value of the element, the range of cover itself). If cover is greater than or equal to the end subscript, just return true! &amp;C++123456789101112class Solution {public: bool canJump(vector&lt;int&gt;&amp; nums) { int cover = 0; if (nums.size() == 1) return true; // There is only one element that can reach for (int i = 0; i &lt;= cover; i++) { // Note that this is less than or equal to cover cover = max(i + nums[i], cover); if (cover &gt;= nums.size() - 1) return true; // cover the end of the line. } return false; }}; **Time complexity: ** O(n) Space complexity: O(1) &amp;Java1234567891011121314151617class Solution { public boolean canJump(int[] nums) { if (nums.length == 1) { return true; } //Coverage, the initial coverage should be 0, because the following iteration starts from subscript 0 int coverRange = 0; //Updating the maximum coverage within the coverage area for (int i = 0; i &lt;= coverRange; i++) { coverRange = Math.max(coverRange, i + nums[i]); if (coverRange &gt;= nums.length - 1) { return true; } } return false; }} &amp;Python1234567891011class Solution: def canJump(self, nums: List[int]) -&gt; bool: cover = 0 if len(nums) == 1: return True i = 0 #Python does not support dynamic modification of variables in for loops, use while loops instead. while i &lt;= cover: cover = max(i + nums[i], cover) if cover &gt;= len(nums) - 1: return True i += 1 return False","link":"/2022/12/15/Jumping%20Game/"},{"title":"Jumping GameII","text":"TEST YOURSELFGiven a 0-indexed integer array nums of length n. The initial position is nums[0]. Each element nums[i], represents the maximum length of a jump forward from index i. In other words, if you are at nums[i], you can jump to any nums[i + j]. 0 &lt;= j &lt;= nums[i] i + j &lt; n Return the minimum number of jumps to reach nums[n - 1]. The generated test case reaches nums[n - 1]. Example1:12345Input: nums = [2,3,1,1,4]Output: 2Explanation: The minimum number of jumps to the last position is 2. To jump from a position labelled 0 to a position labelled 1, you jump 1 step, and then you jump 3 steps to the last position in the array. Example2:12Input: nums = [2,3,0,1,4]Output: 2 HintThis one is a lot harder than the original jumping game. But the idea is similar, and it still depends on the maximum coverage. This question is to calculate the minimum number of steps, then we need to figure out when the number of steps must be plus one? Greedy thinking, local optimal: the current moveable distance as much as possible, if not yet reached the end, the number of steps plus one. Overall optimisation: take as many steps as possible to reach the minimum number of steps. Although the idea is like this, but in the writing of the code can not really jump as far as possible, then we do not know where the next step can jump to the farthest. To start from the coverage, no matter how to jump, the coverage must be able to jump to the smallest number of steps to increase the coverage, coverage once the end of the coverage, you get the minimum number of steps! Here you need to count two coverage, the maximum coverage of the current step and the maximum coverage of the next step. If you move the index to reach the current step of the maximum coverage of the furthest distance, but not yet to the end of the words, then you must take another step to increase the coverage, until the coverage covers the end of the coverage. As shown in the figure: The significance of the coverage in the diagram is that as long as the area is in red, you can definitely get to it in up to two steps! &amp;C++As you can see from the diagram, it is when the moving subscript reaches the furthest distance subscript currently covered, the number of steps should be added by one to increase the distance covered. The final number of steps is the minimum number of steps. There is still a special case to consider, when the mobile subscript reaches the furthest distance subscript currently covered. If the current furthest distance marker is not the end of the set, the number of steps will be increased by one, and you still need to continue to walk. If the furthest distance marker is the end of the set, the number of steps need not be increased by one, because you can’t go back any further. 123456789101112131415161718class Solution {public: int jump(vector&lt;int&gt;&amp; nums) { if (nums.size() == 1) return 0; int curDistance = 0; // Currently covering the furthest distance index int ans = 0; // Record the maximum number of steps taken int nextDistance = 0; // The next step covers the furthest distance index for (int i = 0; i &lt; nums.size(); i++) { nextDistance = max(nums[i] + i, nextDistance); // Update the next furthest distance covered index if (i == curDistance) { // Encountering the furthest distance index currently covered ans++; // next step curDistance = nextDistance; // Update current furthest distance covered index if (nextDistance &gt;= nums.size() - 1) break; // cover the furthest distance to reach the end of the set, do not need to do ans++ operation, end } } return ans; }}; **Time complexity: ** O(n) Space complexity: O(1) &amp;Java12345678910111213141516171819202122232425262728class Solution { public int jump(int[] nums) { if (nums == null || nums.length == 0 || nums.length == 1) { return 0; } //Record the number of jumps int count=0; //Current maximum area of coverage int curDistance = 0; //Maximum coverage area int maxDistance = 0; for (int i = 0; i &lt; nums.length; i++) { //Update the largest coverage area within the available coverage area maxDistance = Math.max(maxDistance,i+nums[i]); // jumping one more step to reach the end if (maxDistance&gt;=nums.length-1){ count++; break; } //Update the next reachable maximum area when going to the maximum area currently covered if (i==curDistance){ curDistance = maxDistance; count++; } } return count; }} &amp;Python12345678910111213141516171819class Solution: def jump(self, nums): if len(nums) == 1: return 0 cur_distance = 0 # Currently covering the furthest distance index ans = 0 # Record the maximum number of steps taken next_distance = 0 # The next step covers the furthest distance index for i in range(len(nums)): next_distance = max(nums[i] + i, next_distance) # Update the next furthest distance covered index if i == cur_distance: #the furthest distance index currently covered ans += 1 # next step cur_distance = next_distance # Update current furthest distance covered index if next_distance &gt;= len(nums) - 1: # cover the furthest distance to reach the end of the set, do not need to do ans++ operation, end break return ans","link":"/2022/12/31/JumpingGameII/"},{"title":"Maiximum sum of subarray","text":"TEST YOURSELFGiven an array of integers nums, find a contiguous subarray with a maximal sum (the subarray contains at least one element) and return its maximal sum. A subarray is a contiguous part of an array. Example1:123Input: nums = [-2,1,-3,4,-1,2,1,-5,4]Output: 6Explanation:The sum of the consecutive subarrays [4,-1,2,1] is at most 6. Example2:12Input: nums = [5,4,-1,7,8]Output: 23 Greedy SolutionWhere is greed greedy? If -2 1 are together, when calculating the starting point, the calculation must start from 1, because the negative number will only pull down the sum, this is where greed is greedy! Local optimal: when the current “continuous sum” is negative, give up immediately and recalculate the “continuous sum” from the next element, because the negative number plus the next element “continuous sum” will only be get smaller and smaller. Global optimum: choose the largest “continuous sum”. In the case of a local optimum, and recording the largest “continuous sum”, the global optimum can be introduced. From the code point of view: traverse the nums, from the beginning with the count accumulation, if the count once plus nums[i] become negative, then it should be from nums[i+1] from 0 to accumulate the count, because the count has become negative, will only drag down the total sum. This is equivalent to a violent solution to constantly adjust the maximum sub-order and the starting position of the interval. The termination of the interval, in fact, if the count to the maximum value, recorded in time. For example, the following code: 1if (count &gt; result) result = count; This is equivalent to using result to record the maximum subsequence and interval sum (an adjustment to the termination position) The starting position in red is when Greedy starts an interval each time it takes a positive number of counts &amp;C++123456789101112131415class Solution {public: int maxSubArray(vector&lt;int&gt;&amp; nums) { int result = INT32_MIN; int count = 0; for (int i = 0; i &lt; nums.size(); i++) { count += nums[i]; if (count &gt; result) { // Take the cumulative maximum value of the interval (equivalent to continuously determining the maximum subsequence termination position) result = count; } if (count &lt;= 0) count = 0; // This is equivalent to resetting the starting position of the largest subsequence, since a negative number encountered must be pulling down the sum } return result; }}; **Time complexity: ** O(n) Space complexity: O(1) &amp;Java123456789101112131415161718class Solution { public int maxSubArray(int[] nums) { if (nums.length == 1){ return nums[0]; } int sum = Integer.MIN_VALUE; int count = 0; for (int i = 0; i &lt; nums.length; i++){ count += nums[i]; sum = Math.max(sum, count); // Take the cumulative maximum value of the interval (equivalent to continuously determining the maximum subsequence termination position) if (count &lt;= 0){ count = 0; // This is equivalent to resetting the starting position of the largest subsequence, since a negative number encountered must be pulling down the sum } } return sum; }} &amp;Python1234567891011class Solution: def maxSubArray(self, nums): result = float('-inf') # Initialisation results in negative infinity count = 0 for i in range(len(nums)): count += nums[i] if count &gt; result: # Take the cumulative maximum value of the interval (equivalent to continuously determining the maximum subsequence termination position) result = count if count &lt;= 0: # This is equivalent to resetting the starting position of the largest subsequence, since a negative number encountered must be pulling down the sum count = 0 return result","link":"/2022/11/15/Maiximumsumofsubarray/"},{"title":"N-Queens","text":"TEST YOURSELFAccording to the rules of chess, a queen can attack a piece in the same row or column or diagonal as her. The n-queen problem studies how to place n queens on an n × n board in such a way that the queens cannot attack each other. Given an integer n, return all the different solutions to the n-queen problem. Each solution contains a different piece placement scheme for the n-queen problem, where ‘Q’ and ‘.’ represent queens and empty squares, respectively. 123Input: n = 4Output:[[&quot;.Q..&quot;,&quot;...Q&quot;,&quot;Q...&quot;,&quot;..Q.&quot;],[&quot;..Q.&quot;,&quot;Q...&quot;,&quot;...Q&quot;,&quot;.Q..&quot;]]Explanation: As shown above, there are two different solutions to the 4 Queens problem HINTFirstly, let’s look at the queens’ constraints: Cannot be in the same line Cannot be in the same column Cannot be on the same diagonal In fact, the search for the position of the queens, can be abstracted as a tree. Below I use a 3 * 3 board, the search process will be abstracted as a tree, as shown in the figure: From the figure, we can see that the height of the 2D matrix is the height of the tree, and the width of the matrix is the width of each node in the tree structure. Then we use the constraints of the queens to backtrack and search the tree, as long as we search the leaf nodes of the tree, it means that we have found a reasonable location for the queens. Backtracking123456789101112void backtracking(parameters) { if (end condition) { results; return; } for (Selection: elements in the set at this level (the number of node children in the tree is the size of the set)) { deal with nodes; backtracking(path，choose list); // resursion Backtracking, reversal of results }} Parameters: Define the global variable result as a two-dimensional array to record the final result. The parameter n is the size of the board, and row is used to keep track of how many levels of the board have been traversed. 12vector&lt;vector&lt;string&gt;&gt; result;void backtracking(int n, int row, vector&lt;string&gt;&amp; chessboard) { Termination condition: 1234if (row == n) { result.push_back(chessboard); return;} Logic of single level search The recursive depth is the row that control the rows of the board, and the col of the for loop in each layer controls the columns of the board, determining where to place the queens. Each time, the search starts at the beginning of a new row, so it starts at 0. 1234567for (int col = 0; col &lt; n; col++) { if (isValid(row, col, chessboard, n)) { // validate chessboard[row][col] = 'Q'; // place queen backtracking(n, row + 1, chessboard); chessboard[row][col] = '.'; // backtrack,withdraw queen }} &amp;C++1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Solution {private:vector&lt;vector&lt;string&gt;&gt; result;// n the size of the board// row current recurssion rowvoid backtracking(int n, int row, vector&lt;string&gt;&amp; chessboard) { if (row == n) { result.push_back(chessboard); return; } for (int col = 0; col &lt; n; col++) { if (isValid(row, col, chessboard, n)) { // validate chessboard[row][col] = 'Q'; // place queen backtracking(n, row + 1, chessboard); chessboard[row][col] = '.'; // backtrack,withdraw queen } }}bool isValid(int row, int col, vector&lt;string&gt;&amp; chessboard, int n) { // check col for (int i = 0; i &lt; row; i++) { // prune if (chessboard[i][col] == 'Q') { return false; } } // Check for queens at 45 degrees for (int i = row - 1, j = col - 1; i &gt;=0 &amp;&amp; j &gt;= 0; i--, j--) { if (chessboard[i][j] == 'Q') { return false; } } // Check for queens at 135 degrees for(int i = row - 1, j = col + 1; i &gt;= 0 &amp;&amp; j &lt; n; i--, j++) { if (chessboard[i][j] == 'Q') { return false; } } return true;}public: vector&lt;vector&lt;string&gt;&gt; solveNQueens(int n) { result.clear(); std::vector&lt;std::string&gt; chessboard(n, std::string(n, '.')); backtracking(n, 0, chessboard); return result; }}; **Time complexity: ** O(n!) Space complexity: O(n) &amp;JAVA12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class Solution { List&lt;List&lt;String&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;String&gt;&gt; solveNQueens(int n) { char[][] chessboard = new char[n][n]; for (char[] c : chessboard) { Arrays.fill(c, '.'); } backTrack(n, 0, chessboard); return res; } public void backTrack(int n, int row, char[][] chessboard) { if (row == n) { res.add(Array2List(chessboard)); return; } for (int col = 0;col &lt; n; ++col) { if (isValid (row, col, n, chessboard)) { chessboard[row][col] = 'Q'; backTrack(n, row+1, chessboard); chessboard[row][col] = '.'; } } } public List Array2List(char[][] chessboard) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (char[] c : chessboard) { list.add(String.copyValueOf(c)); } return list; } public boolean isValid(int row, int col, int n, char[][] chessboard) { // check col for (int i=0; i&lt;row; ++i) { // prune if (chessboard[i][col] == 'Q') { return false; } } //Check for queens at 45 degrees for (int i=row-1, j=col-1; i&gt;=0 &amp;&amp; j&gt;=0; i--, j--) { if (chessboard[i][j] == 'Q') { return false; } } // Check for queens at 135 degrees for (int i=row-1, j=col+1; i&gt;=0 &amp;&amp; j&lt;=n-1; i--, j++) { if (chessboard[i][j] == 'Q') { return false; } } return true; }} &amp;Python123456789101112131415161718192021222324252627282930313233343536373839404142class Solution: def solveNQueens(self, n: int) -&gt; List[List[str]]: result = [] chessboard = ['.' * n for _ in range(n)] # initialise the board self.backtracking(n, 0, chessboard, result) # backtrack return [[''.join(row) for row in solution] for solution in result] def backtracking(self, n: int, row: int, chessboard: List[str], result: List[List[str]]) -&gt; None: if row == n: result.append(chessboard[:]) # return for col in range(n): if self.isValid(row, col, chessboard): chessboard[row] = chessboard[row][:col] + 'Q' + chessboard[row][col+1:] # place queen self.backtracking(n, row + 1, chessboard, result) # recursion to the next row chessboard[row] = chessboard[row][:col] + '.' + chessboard[row][col+1:] # backtarck def isValid(self, row: int, col: int, chessboard: List[str]) -&gt; bool: # check col for i in range(row): if chessboard[i][col] == 'Q': return False # illegal place # check for queen at 45 degrees i, j = row - 1, col - 1 while i &gt;= 0 and j &gt;= 0: if chessboard[i][j] == 'Q': return False i -= 1 j -= 1 # check for queen at 135 degrees i, j = row - 1, col + 1 while i &gt;= 0 and j &lt; len(chessboard): if chessboard[i][j] == 'Q': return False i -= 1 j += 1 return True # legal place","link":"/2022/10/15/N-Queens/"},{"title":"Basics of Neural NetworksI","text":"What Is A Perceptron and How Does It Make Up Neural Networks?A perceptron is the building block of neural networks, similar to the biological building blocks of nucleic acids, lipids, carbohydrates, and proteins. 4 parts make up these as well: Input values Weights and bias Summation function Activation function The input values are affected by the predetermined weight values and then added up together in the summation function. After the summation function, the value is squashed between -1 and 1 or 0 and 1 in the activation function. This will determine if that perceptron will be activated. The more negative the value is the closer it will be to the minimum (-1 or 0) and then more positive the value the closer it is to the maximum (1). If it is activated then the output is sent along its way. Single PerceptronIf the perceptron is in a single form, then the first and only output will be in a yes or no format. Single perceptrons are great for linearly separability datasets, groups that can be separated using a single line with a constant slope. Neural NetworkHowever, if more than one perceptron is present and joined in a layered fashion we have produced a neural network. The output from the first layer becomes the input for the second layer and so on until the output layer sums up the total activation. The total activation is the confidence level of the network for its final decision. What Are The Neural Network Layers?Neural networks have 3 layers: Input Layer Hidden Layer Output Layer The input layer is the starting point of the network. This is the layer where the values that will be used for the prediction are brought in. The hidden layer is where the network works its “magic”. The activation amounts of the input values are calculated throughout this area. The hidden layer can be as small as 1 or as numerous as needed for the project. Finally, the output layer is where the activation of each of the final nodes is used to select a solution. As you can see, each unit is connected to each unit of the subsequent layer. This allows for nearby perceptrons to communicate with one another and create weights that are optimal for its use. Just remember, the first layer is the input and the last layer is the output. Anything in between is the hidden layer. How Does The Computer “See” Neural Networks?Matrix MultiplicationLet’s break down a neural network with 2 input values, 1 hidden layer containing 3 nodes, and we end with 2 output layer nodes. We will keep this easy to follow by using letters unique to each element so that you can reference it as we go. Weights x Input LayerMatrix multiplication is done by taking the rows of the first matrix and multiplying each element with the respective element in the second matrix. The main rule for this operation is that the number of columns in the first matrix must match the number of rows in the second matrix. Let us take a look at the first set of matrixes we will be using for this network. As we can see, if the matrix of the input values is first we will have an undefined result due to the rule above not being honored. However, if we flip the matrixes we will be able to perform multiplication. With the resulting/input matrix as the second, or right, matrix and the weight matrix as the first, or left, matrix. Let’s perform the first multiplication in this network. Our resulting matrix contains 3 rows which correlate to each of the 3 nodes in the hidden layer: Weights x Hidden LayerLet’s do the final multiplication. The output matrix has the final result and it contains the 2 rows which correlate to the 2 nodes of the output layer: The result from the equations above would be the activation level for each of the neurons. If O 0 was higher than O 1 then the prediction associated with O 0 would be given as the solution.","link":"/2023/07/15/BasicsofNeuralNetworksI/"},{"title":"Basics of Neural NetworksII","text":"Let’s introduce some popular Neural Network Layers Fully-Connected Layer (Linear Layer) The output neuron is connected to all input neurons ​ Shape of Tensors: What is the problem with the fully connected layer? The problem is that the shape of the data is “ignored“. For example, when the input data is an image, the image is usually a 3-dimensional shape in the height, length, and channel directions. However, when inputting to a fully-connected layer, the 3-dimensional data needs to be flattened to 1-dimensional data. The fully connected layer ignores the shape and processes all the input data as the same neurons (neurons of the same dimension), so it cannot utilise the information related to the shape. A convolutional layer, on the other hand, can keep the shape. When the input data is an image, the convolutional layer receives the input data as 3-dimensional data and outputs the same as 3-dimensional data to the next layer. Thus in CNN, it is possible (probable) to correctly understand data with shape such as images. In CNN, the input and output data of the convolutional layer is sometimes referred to as a feature map. In this case, the input data of the convolutional layer is called input feature map and the output data is called output feature map. The processing performed by the convolutional layer is the convolution operation. The convolution operation is equivalent to the “filter operation” in image processing. Convolution Layer-1D The output neuron is connected to input neurons in the receptive field ​ Shape of Tensors: Convolution Layer-2D The output neuron is connected to input neurons in the receptive field ​ Shape of Tensors: - PaddingPadding is used primarily to resize the output. For example, when applying a filter of (2, 2) to input data of size (4, 4), the output size becomes (3, 3), which corresponds to the output size being reduced by 1 element compared to the input size. If the space is shrunk every time the convolution operation is performed, then at some point the output size may change to 1, making it impossible to apply the convolution operation anymore. In order to avoid such a situation, one has to use the padding Zero Padding pads the input boundaries with zero. (Default in PyTorch) Other Paddings: Reflection Padding, Replication Padding, Constant Padding - Receptive Field•In convolution, each output element depends on kh × kw receptive field in the input. • Each successive convolution adds k-1 to the receptive field size • With L layers, the receptive field size is L ⋅ (k − 1) + 1 Pooling Layer Downsample the feature map to a smaller size • The output neuron pools the features in the receptive field, similar to convolution. ​ • Usually, the stride is the same as the kernel size: s = k • Pooling operates over each channel independently. ​ • No learnable parameter Normalization Layer Normalizing the features makes optimization faster • Normalization layer normalizes the features as follows,$$\\mathop{x_i}\\limits^- = 1/σ(x_i-u_i)$$• u_i is the mean, and σ_i is the standard deviation (std) over the set of pixels . • Then learns a per-channel linear transform (trainable scale γ and shift β ) to compensate for the possible lost of representational ability.$$y= γ_i \\mathop{x_i}\\limits^- +β_i$$ Activation Function On the one hand, if there is no activation function limit, the data is passed layer by layer, the final number is getting bigger and bigger, similar to the explosion, so the activation function is needed to limit the results to a certain range each time On the other hand, the activation function plays a similar role as neuron, responsible for the control of the signal (or data) to decide whether the neuron is activated or not, so as to make the model learning effect is better.","link":"/2023/07/30/BasicsofNeuralNetworksII/"},{"title":"Introduction to Video Compression","text":"Background of video compressionVideo is made up of a combination of pictures in a certain order and number, and each picture inside is called a frame. The consecutive frames form the animation and video we see. But this is video in its most primitive format, directly sampling the video without compression there is actually no way to actually transmit it over the internet. This is because video information is very redundant and has a lot of data. Take a 1920×1080 picture as an example, to transmit this picture needs 1920×1080×8×3 bit , that is, 47Mb. If you want to transmit a video of 30 frames per second, each frame is a picture of this resolution, then 1s need to transmit 3047Mb=1.4Gb* of data, the bandwidth you need is 1.4Gb. But home gigabit bandwidth is still in popularity. in popularity, so it’s almost impossible to transfer such a large video on a daily basis. And since much of the information is redundant, it’s not cost-effective to transmit the raw video signal. So the video must be compressed before it can be transmitted properly. This compression process is called video encoding. Then the video coding to do is: the video data with encoder to remove redundancy, the original pixel value encoded into 0, 1 bit stream, which includes redundancy such as temporal redundancy, spatial redundancy etc., in the receiving end of the decoding, we hope that the reconstruction of the video quality as good as possible. Deep tools for traditional schemeNow the video coding combined with deep learning is mainly divided into two categories: one is the combination of deep learning and traditional video coding methods, and the other is the end-to-end coding methods, we first introduce the former, there are many modules in traditional video coding, and we pick the three modules of inter-frame prediction, intra-frame prediction, and entropy coding as an example to understand how the deep learning is applied to this. Deep Inter predictionLet’s start with a brief introduction of what inter-frame prediction is: Inter-frame prediction, is a tool to do prediction between video frames to remove temporal redundancy. As shown in the figure, the Kth frame and the K+1th frame only the position of the circle has been shifted, then we only need to record the Kth frame and the displacement information of the circle to get the K+1th frame. This greatly compresses the raw data. It can be said that inter-frame prediction is at the heart of video coding. This is because the time dimension is unique to video relative to images. In traditional video coding schemes, inter-frame prediction is mainly achieved by motion estimation (ME) and motion compensation (MC). The general process is: Firstly, the image is divided into macroblocks in a predefined way, and then motion estimation is carried out. Motion estimation: finding a best matching block for each pixel block of the current image in the previously coded image, and the commonly used matching criteria are MSE (Mean square error), MAD (mean absolutie difference) and SAD ( Matching-Pixel Count) and so on. Then the motion vector is calculated. Motion vector is the relative position between the current position of the pixel block in the current frame and the total position of the reference block in the reference frame, which represents the trajectory of the object in the pixel block between the two frames. This relative position is represented as a vector of two coordinate values (MV_x, MV_y), called Motion Vector (MV). For further compression, there is Motion Vector Prediction (MVP), there also exists a prediction value for the motion vector, the MV of the current block is predicted from the MV of the neighbouring blocks, with MVP it is possible to determine the starting point and direction of the motion estimation (MC). Finally, Motion compensation: according to the method of motion vectors and inter-frame prediction, the estimated value of the current frame is obtained. However, the inter-frame prediction will have some bad predictions due to the object’s motion deformation, lighting changes, and lossy reference frames. Various techniques have been proposed to improve block-level motion estimation and motion compensation, such as using multiple reference frames, bi-directional inter-frame prediction, and so on. Deep learning methods also address these problems. Deep Intra predictionThe basic idea of intra-frame prediction is to use the correlation of neighbouring pixels in a frame to eliminate spatial redundancy by predicting the pixels in the current block by the neighbouring pixels in the null domain. For example, if a picture is divided into four blocks, the lower right block can be predicted from the three blocks in the upper left corner of the picture, so there is no need to encode the lower right block separately, thus saving space. Traditional intra-frame coding methods are broadly divided into two parts: block division and pattern selection. Deep Entropy EncodingFirst of all, what is entropy coding? Entropy coding is coding in which no information is lost during the coding process according to the entropy principle. Information entropy is the average amount of information (a measure of uncertainty) of the source, which can also be said to be the number of bits needed to encode the average of all symbols. Common types of entropy coding are: Shannon, Huffman and arithmetic coding, stroke coding (RLE), context-based adaptive variable length coding (CAVLC), context-based adaptive binary arithmetic coding (CABAC). In video coding, entropy coding transforms a set of elemental symbols used to represent a video sequence into a compressed stream for transmission or storage. The input symbols may include quantised transform coefficients, motion vectors, header information (macroblock header, image header, sequence header, etc.), and additional information (marker bits that are important for correct decoding). As shown in the figure is a flow diagram of CABAC, it mainly includes three steps: binarizer, context modeler and binary arithmetic coder, where binarizer and context modeler are used to predict the probability of grammatical elements. Then many deep learning methods are optimised for the probability estimation step.","link":"/2023/06/15/IntroductiontoVideoCompression/"},{"title":"01 Backpack","text":"01 BackpackThere are n items and a backpack that can carry at most w weights. The weight of the ith item is weight[i] and the value obtained is value[i] . Each item can be used only once, so solve for the items in the backpack that have the greatest sum of values. There are really only two states for each item, take or don’t take, so you can use backtracking to search out all the cases, then the time complexity is o(2^n), where n denotes the number of items. So the violent solution is exponential level time complexity. It is only then that the dynamic programming solution is needed for optimisation! In the following explanation, I give an example: The maximum weight of the backpack is 4. The items are: Ask what is the maximum value of an item that a backpack can carry? The numbers that appear in the following explanations and illustrations are based on this example. 2D dp[] 01 backpacks1. Determine the dp[]] and the meaning of indexdp[i] [j] : denotes that there are dp[i] [j] distinct paths from (0 , 0) to (i, j). For the backack problem, one way to write it is to use a two-dimensional array, i.e., dp[i] [j] represents the maximum sum of the values of any of the items with index [0-i] that can be taken and put into a backpack with capacity j. The following is the definition of dp[i] [j]: 2. Determine the recursive formulaRecall again the meaning of dp[i] [j]: what is the maximum sum of values that can be taken arbitrarily from the items with subscripts [0-i] and put into a backpack with capacity j. Then there can be two directions to introduce dp[i] [j] that: Do not put item i: introduced by dp[i - 1] [j], that is, the capacity of the backpack is j, the maximum value of not putting item i inside, at this time dp[i] [j] is dp[i - 1] [j]. (In fact, it means that when the weight of item i is greater than the weight of backpack j, item i cannot be put into the backpack, so the value in the backpack remains the same as before.) Putting item i: introduced by dp[i - 1] [j - weight[i]], dp[i - 1] [j - weight[i]] is the maximum value of not putting item i when the capacity of the backpack is j - weight[i], then dp[i - 1] [j - weight[i]] + value[i] (the value of item i) is the value of the backpack to put item i to get the the maximum value of item i in the backpack So the recursive formula: dp[i] [j] = max(dp[i - 1] [j], dp[i - 1] [j - weight[i]] + value[i]) 3. How the dp[] is initialisedRegarding the initialisation, it must match the definition of the dp[] , otherwise it will get messier and messier when it comes to the recursive formula. First from the definition of dp[i] [j], if the backpack capacity j is 0, that is, dp[i] [0], no matter which items are selected, the sum of the backpack value must be 0. As shown in the figure: Look at the other cases again: The state transfer equation dp[i] [j] = max(dp[i - 1] [j], dp[i - 1] [j - weight[i]] + value[i]); It can be seen that i is derived from i-1, and then must be initialised if i is 0. dp[0] [j], i.e.: the maximum value that can be stored in a backpack of each capacity when i is 0 and the item numbered 0 is stored. Then it is clear that when j &lt; weight[0], dp[0] [j] should be 0, because the capacity of the backpack is smaller than the weight of the numbered 0 item. When j &gt;= weight[0], dp[0] [j] should be value[0], because the backpack capacity is enough for item number 0. The code is initialised as follows: 1234567for (int j = 0 ; j &lt; weight[0]; j++) { // Of course this step can be omitted if the dp[][] is pre-initialised to 0 dp[0][j] = 0;}// positive-order traversalfor (int j = weight[0]; j &lt;= bagweight; j++) { dp[0][j] = value[0];} At this point the dp[] [] is initialised as shown: Both dp[0] [j] and dp[i] [0] are initialised, so how much should the other index be initialised? In fact, from the recursive formula: dp[i] [j] = max(dp[i - 1] [j], dp[i - 1] [j - weight[i]] + value[i]); you can see that dp[i] [j] is derived from the upper-left value, then the other index initialised to whatever value is OK, because they will all be overwritten. Initially -1, initially -2, initially 100, all are fine! But it’s just that it’s easier to uniformly initialise the dp[] [] to 0 uniformly at the start. As shown: Finally, the complete code is: 123456// initialise dp[][]vector&lt;vector&lt;int&gt;&gt; dp(weight.size(), vector&lt;int&gt;(bagweight + 1, 0));for (int j = weight[0]; j &lt;= bagweight; j++) { dp[0][j] = value[0];} 4. Determining the traversal orderIn the following figure, it can be seen that there are two traversal dimensions: item and pack weight Iterate over items first, then over pack weights 12345678// The size of the weight array is the number of items.for(int i = 1; i &lt; weight.size(); i++) { // traversal item for(int j = 0; j &lt;= bagweight; j++) { // traversal backpack capacity if (j &lt; weight[i]) dp[i][j] = dp[i - 1][j]; else dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]); }} Iterate over weights first, then over item 1234567for(int j = 0; j &lt;= bagweight; j++) { // traversal backpack capacity for(int i = 1; i &lt; weight.size(); i++) { // traversal item if (j &lt; weight[i]) dp[i][j] = dp[i - 1][j]; else dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]); }} 5. Derivation of dp[] []by example The end result is dp[2] [4] &amp;C++12345678910111213141516171819202122232425262728void test_2_wei_bag_problem1() { vector&lt;int&gt; weight = {1, 3, 4}; vector&lt;int&gt; value = {15, 20, 30}; int bagweight = 4; // 2d-array vector&lt;vector&lt;int&gt;&gt; dp(weight.size(), vector&lt;int&gt;(bagweight + 1, 0)); // initialise for (int j = weight[0]; j &lt;= bagweight; j++) { dp[0][j] = value[0]; } // size of array = number of items for(int i = 1; i &lt; weight.size(); i++) { // travesal item for(int j = 0; j &lt;= bagweight; j++) { // traversal backpack capacity if (j &lt; weight[i]) dp[i][j] = dp[i - 1][j]; else dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]); } } cout &lt;&lt; dp[weight.size() - 1][bagweight] &lt;&lt; endl;}int main() { test_2_wei_bag_problem1();} &amp;Java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class BagProblem { public static void main(String[] args) { int[] weight = {1,3,4}; int[] value = {15,20,30}; int bagSize = 4; testWeightBagProblem(weight,value,bagSize); } public static void testWeightBagProblem(int[] weight, int[] value, int bagSize){ // dp[] int goods = weight.length; // number of items int[][] dp = new int[goods][bagSize + 1]; // Initialise the dp array // After the array is created, the default value is 0. for (int j = weight[0]; j &lt;= bagSize; j++) { dp[0][j] = value[0]; } // Fill the dp[][] for (int i = 1; i &lt; weight.length; i++) { for (int j = 1; j &lt;= bagSize; j++) { if (j &lt; weight[i]) { /** * If the current backpack is not as big as the current item i, then item i is not put down. * Then the maximum value of the first i-1 items that can be put down is the maximum value of the current situation. */ dp[i][j] = dp[i-1][j]; } else { /** * The current capacity of the backpack can hold the item i * Then at this point there are two scenarios: * 1, do not put item i * 2. Put item i * Compare these two cases, which backpack has the largest value of items. */ dp[i][j] = Math.max(dp[i-1][j] , dp[i-1][j-weight[i]] + value[i]); } } } // print dp[][[]] for (int i = 0; i &lt; goods; i++) { for (int j = 0; j &lt;= bagSize; j++) { System.out.print(dp[i][j] + &quot;\\t&quot;); } System.out.println(&quot;\\n&quot;); } }} &amp;Python123456789101112131415161718192021222324#no parameterdef test_2_wei_bag_problem1(): weight = [1, 3, 4] value = [15, 20, 30] bagweight = 4 # 2d array dp = [[0] * (bagweight + 1) for _ in range(len(weight))] # initialise for j in range(weight[0], bagweight + 1): dp[0][j] = value[0] # size of array = number of items for i in range(1, len(weight)): # travesal item for j in range(bagweight + 1): # traversal backpack capacity if j &lt; weight[i]: dp[i][j] = dp[i - 1][j] else: dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]) print(dp[len(weight) - 1][bagweight])test_2_wei_bag_problem1() 12345678910111213141516171819202122232425262728#with parameterdef test_2_wei_bag_problem1(weight, value, bagweight): # 2d array dp = [[0] * (bagweight + 1) for _ in range(len(weight))] # initialise for j in range(weight[0], bagweight + 1): dp[0][j] = value[0] # size of array = number of items for i in range(1, len(weight)): # travesal item for j in range(bagweight + 1): # traversal backpack capacity if j &lt; weight[i]: dp[i][j] = dp[i - 1][j] else: dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]) return dp[len(weight) - 1][bagweight]if __name__ == &quot;__main__&quot;: weight = [1, 3, 4] value = [15, 20, 30] bagweight = 4 result = test_2_wei_bag_problem1(weight, value, bagweight) print(result)","link":"/2023/02/28/01BackpackTheoryBasics/"},{"title":"Min Cost Climbing Stairs","text":"TEST YOURSELFYou are given an integer array cost where cost[i] is the cost of ith step on a staircase. Once you pay the cost, you can either climb one or two steps. You can either start from the step with index 0, or the step with index 1. Return the minimum cost to reach the top of the floor. 1234567Example 1:Input: cost = [10,15,20]Output: 15Explanation: You will start at index 1.- Pay 15 and climb two steps to reach the top.The total cost is 15. 123456789101112Example 2:Input: cost = [1,100,1,1,1,100,1,1,100,1]Output: 6Explanation: You will start at index 0.- Pay 1 and climb two steps to reach index 2.- Pay 1 and climb two steps to reach index 4.- Pay 1 and climb two steps to reach index 6.- Pay 1 and climb one step to reach index 7.- Pay 1 and climb two steps to reach index 9.- Pay 1 and climb one step to reach the top.The total cost is 6. HINT1. Determine the dp[]] and the meaning of subscriptsTo use dynamic programming, we have to have an array to keep track of the state, and this problem requires only a one-dimensional array dp[i]. ​ dp[i]: the minimum body force spent to reach step i is dp[i]. 2. Determine the recursive formulaThere can be two ways to get dp[i], one is dp[i-1] one is dp[i-2]. The jump from dp[i - 1] to dp[i] costs dp[i - 1] + cost[i - 1] dp[i - 2] jumping to dp[i] costs dp[i - 2] + cost[i - 2] So should we choose to jump from dp[i - 1] or from dp[i - 2]? The smallest must be chosen, so dp[i] = min(dp[i - 1] + cost[i - 1], dp[i - 2] + cost[i - 2]); 3. How the dp[] is initialisedLook at the recursive formula, dp[i] is pushed out by dp[i - 1], dp[i - 2], and since initialising all of dp[i] is impossible, it’s enough to initialise only dp[0] and dp[1], and everything else ends up being pushed out by dp[0], dp[1]. So what should dp[0] be? According to the definition of the dp array, to reach the 0th step of the smallest body spent dp[0], then dp[0] should be cost[0], for example, cost = [1, 100, 1, 1, 1, 1, 100, 1, 1, 100, 1, 1, 1, 1, 100, 1] words, dp[0] is cost[0] should be 1. So initialise dp[0] = 0 and dp[1] = 0. 4. Determining the traversal orderBecause it is a simulation of the steps, and dp[i] by dp[i-1], dp[i-2] introduced, so it is from front to back traversal cost[]. 5. Derivation of dp[] by examplexxxxxxxxxx class Solution {​ public int minSubArrayLen(int s, int[] nums) { int left = 0; int sum = 0; int result = Integer.MAX_VALUE; for (int right = 0; right &lt; nums.length; right++) { sum += nums[right]; while (sum &gt;= s) { result = Math.min(result, right - left + 1); sum -= nums[left++]; } } return result == Integer.MAX_VALUE ? 0 : result; }}cpp &amp;C++123456789101112class Solution {public: int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) { vector&lt;int&gt; dp(cost.size() + 1); dp[0] = 0; dp[1] = 0; for (int i = 2; i &lt;= cost.size(); i++) { dp[i] = min(dp[i - 1] + cost[i - 1], dp[i - 2] + cost[i - 2]); } return dp[cost.size()]; }}; Time Complexity: O(n) Space Complexity: O(n) 1234567891011121314class Solution {public: int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) { int dp0 = 0; int dp1 = 0; for (int i = 2; i &lt;= cost.size(); i++) { int dpi = min(dp1 + cost[i - 1], dp0 + cost[i - 2]); dp0 = dp1; dp1 = dpi; } return dp1; }}; Time Complexity: O(n) Space Complexity: O(1) &amp;Java12345678910111213141516171819// Mode 1: no payment for the first stepclass Solution { public int minCostClimbingStairs(int[] cost) { int len = cost.length; int[] dp = new int[len + 1]; // Starts with a step labelled 0 or 1 and therefore pays 0. dp[0] = 0; dp[1] = 0; // Calculate the minimum cost to reach each level of steps for (int i = 2; i &lt;= len; i++) { dp[i] = Math.min(dp[i - 1] + cost[i - 1], dp[i - 2] + cost[i - 2]); } return dp[len]; }} 12345678910111213// Mode II: pay first stepclass Solution { public int minCostClimbingStairs(int[] cost) { int[] dp = new int[cost.length]; dp[0] = cost[0]; dp[1] = cost[1]; for (int i = 2; i &lt; cost.length; i++) { dp[i] = Math.min(dp[i - 1], dp[i - 2]) + cost[i]; } //Final step, if it is climbed from the penultimate step, the stamina spent on the final step can be disregarded return Math.min(dp[cost.length - 1], dp[cost.length - 2]); }} &amp;Python123456789101112class Solution: def minCostClimbingStairs(self, cost: List[int]) -&gt; int: dp = [0] * (len(cost) + 1) dp[0] = 0 # Initial value, indicating that no stamina is spent from the starting point dp[1] = 0 # Initial value, indicating that no stamina is spent from the fisrt step for i in range(2, len(cost) + 1): # At step i, you can choose to spend energy from the previous step (i-1) to the current step, or from the previous two steps (i-2) to the current step. # Choose the path that costs the lesser amount of stamina, add the cost of the current step, and update the dp[] dp[i] = min(dp[i - 1] + cost[i - 1], dp[i - 2] + cost[i - 2]) return dp[len(cost)] # Minimum cost to the roof 1234567891011121314class Solution: def minCostClimbingStairs(self, cost: List[int]) -&gt; int: dp0 = 0 # Initial value, indicating that no stamina is spent from the starting point dp1 = 0 # Initial value, indicating that no stamina is spent from the fisrt step for i in range(2, len(cost) + 1): # # At step i, you can choose to spend energy from the previous step (i-1) to the current step, or from the previous two steps (i-2) to the current step. # Select the path with the smaller cost, add the cost of the current step, and get the minimum cost of the current step. dpi = min(dp1 + cost[i - 1], dp0 + cost[i - 2]) dp0 = dp1 # Update dp0 to the value from the previous step, i.e. dp1 from the previous loop dp1 = dpi # Update dp1 to the minimum spend for the current step return dp1 # Minimum cost to the roof","link":"/2023/01/30/MinCostClimbingStairs/"},{"title":"Unique Paths","text":"TEST YOURSELFYou are given an m x n integer array grid. There is a robot initially located at the top-left corner (i.e., grid[0][0]). The robot tries to move to the bottom-right corner (i.e., grid[m - 1][n - 1]). The robot can only move either down or right at any point in time. An obstacle and space are marked as 1 or 0 respectively in grid. A path that the robot takes cannot include any square that is an obstacle. Return the number of possible unique paths that the robot can take to reach the bottom-right corner. The testcases are generated so that the answer will be less than or equal to 2 * 109. 12345678Example 1:Input: obstacleGrid = [[0,0,0],[0,1,0],[0,0,0]]Output: 2Explanation: There is one obstacle in the middle of the 3x3 grid above.There are two ways to reach the bottom-right corner:1. Right -&gt; Right -&gt; Down -&gt; Down2. Down -&gt; Down -&gt; Right -&gt; Right 1234Example 2:Input: obstacleGrid = [[0,1],[0,0]]Output: 1 HINTThe most intuitive idea for this question, at first glance, is to use a deep search in graph theory to enumerate how many paths there are. Note that the question says that the robot can only move down or to the right one step at a time, so in fact, the path that the robot has travelled can be abstracted as a binary tree, and the leaf nodes are the end point! An example is shown in the figure: At this point the problem can be transformed into finding the number of leaf nodes of a binary tree with the following code: 123456789101112class Solution {private: int dfs(int i, int j, int m, int n) { if (i &gt; m || j &gt; n) return 0; if (i == m &amp;&amp; j == n) return 1; // Finding a method = finding leaf nodes return dfs(i + 1, j, m, n) + dfs(i, j + 1, m, n); }public: int uniquePaths(int m, int n) { return dfs(1, 1, m, n); }}; You will notice the timeout if you submit the code! To analyse the time complexity, the algorithm for this deep search is actually to traverse the entire binary tree. The depth of this tree is actually m+n-1 (depth is calculated by starting at 1). The number of nodes in the binary tree is 2^(m + n - 1) - 1. It can be understood that the deep search algorithm is traversing the entire full binary tree (not actually traversing the entire full binary tree, it’s just an approximation). So the time complexity of the above deep search code is O(2^(m + n - 1) - 1), which is exponentially large, as you can see! Introduce Dynamic Programming1. Determine the dp[]] and the meaning of indexdp[i] [j] : denotes that there are dp[i] [j] distinct paths from (0 , 0) to (i, j). 2. Determine the recursive formuladp[i] [j] = dp[i - 1] [j] + dp[i] [j - 1] But one thing to note here is that because of the obstacle, (i, j) should stay in the initial state if it is the obstacle (initial state is 0). So the code is: 123if (obstacleGrid[i][j] == 0) { // When (i, j) is unobstructed, then derive dp[i][j] dp[i][j] = dp[i - 1][j] + dp[i][j - 1];} 3. How the dp[] is initialisedFirstly, dp[i] [0] must all be 1 because there is only one path from the position (0, 0) to (i, 0), then the same applies to dp[0] [j] 123vector&lt;vector&lt;int&gt;&gt; dp(m, vector&lt;int&gt;(n, 0)); // intial 0for (int i = 0; i &lt; m; i++) dp[i][0] = 1;for (int j = 0; j &lt; n; j++) dp[0][j] = 1; But if (i, 0) this edge has an obstacle after it, all positions after (and including) the obstacle are out of reach, so dp[i] [0] after the obstacle should still have the initial value 0. The same is true for the initialisation of the index (0, j). so the complete code is: 123vector&lt;vector&lt;int&gt;&gt; dp(m, vector&lt;int&gt;(n, 0));for (int i = 0; i &lt; m &amp;&amp; obstacleGrid[i][0] == 0; i++) dp[i][0] = 1;for (int j = 0; j &lt; n &amp;&amp; obstacleGrid[0][j] == 0; j++) dp[0][j] = 1; 4. Determining the traversal orderHere it is important to look at the recursive formula dp[i] [j] = dp[i - 1] [j] + dp[i] [j - 1], where dp[i] [j] are derived from above and to the left of them, so traversing them one layer at a time, left to right, is sufficient. This ensures that when deriving dp[i] [j], dp[i - 1] [j] and dp[i] [j - 1] must have values 123456for (int i = 1; i &lt; m; i++) { for (int j = 1; j &lt; n; j++) { if (obstacleGrid[i][j] == 1) continue; dp[i][j] = dp[i - 1][j] + dp[i][j - 1]; }} 5. Derivation of dp[] by exampleTake Example 1: The according dp[] as shown: &amp;C++12345678910111213141516171819class Solution {public: int uniquePathsWithObstacles(vector&lt;vector&lt;int&gt;&gt;&amp; obstacleGrid) { int m = obstacleGrid.size(); int n = obstacleGrid[0].size(); if (obstacleGrid[m - 1][n - 1] == 1 || obstacleGrid[0][0] == 1) //If there is an obstacle at the start or end point, return directly to 0 return 0; vector&lt;vector&lt;int&gt;&gt; dp(m, vector&lt;int&gt;(n, 0)); for (int i = 0; i &lt; m &amp;&amp; obstacleGrid[i][0] == 0; i++) dp[i][0] = 1; for (int j = 0; j &lt; n &amp;&amp; obstacleGrid[0][j] == 0; j++) dp[0][j] = 1; for (int i = 1; i &lt; m; i++) { for (int j = 1; j &lt; n; j++) { if (obstacleGrid[i][j] == 1) continue; dp[i][j] = dp[i - 1][j] + dp[i][j - 1]; } } return dp[m - 1][n - 1]; }}; Time Complexity:O(n × m) n and m are the length and width of the obstacleGrid, respectively. Space Complexity:O(n × m) There is also a space-optimised version here: 123456789101112131415161718192021222324class Solution {public: int uniquePathsWithObstacles(vector&lt;vector&lt;int&gt;&gt;&amp; obstacleGrid) { if (obstacleGrid[0][0] == 1) return 0; vector&lt;int&gt; dp(obstacleGrid[0].size()); for (int j = 0; j &lt; dp.size(); ++j) if (obstacleGrid[0][j] == 1) dp[j] = 0; else if (j == 0) dp[j] = 1; else dp[j] = dp[j-1]; for (int i = 1; i &lt; obstacleGrid.size(); ++i) for (int j = 0; j &lt; dp.size(); ++j){ if (obstacleGrid[i][j] == 1) dp[j] = 0; else if (j != 0) dp[j] = dp[j] + dp[j-1]; } return dp.back(); }}; Time Complexity: O(n × m) Space Complexity: O(m) &amp;Java1234567891011121314151617181920212223242526class Solution { public int uniquePathsWithObstacles(int[][] obstacleGrid) { int m = obstacleGrid.length; int n = obstacleGrid[0].length; int[][] dp = new int[m][n]; //If there is an obstacle at the start or end point, return 0 directly. if (obstacleGrid[m - 1][n - 1] == 1 || obstacleGrid[0][0] == 1) { return 0; } for (int i = 0; i &lt; m &amp;&amp; obstacleGrid[i][0] == 0; i++) { dp[i][0] = 1; } for (int j = 0; j &lt; n &amp;&amp; obstacleGrid[0][j] == 0; j++) { dp[0][j] = 1; } for (int i = 1; i &lt; m; i++) { for (int j = 1; j &lt; n; j++) { dp[i][j] = (obstacleGrid[i][j] == 0) ? dp[i - 1][j] + dp[i][j - 1] : 0; } } return dp[m - 1][n - 1]; }} 1234567891011121314151617181920212223// Space-optimised versionclass Solution { public int uniquePathsWithObstacles(int[][] obstacleGrid) { int m = obstacleGrid.length; int n = obstacleGrid[0].length; int[] dp = new int[n]; for (int j = 0; j &lt; n &amp;&amp; obstacleGrid[0][j] == 0; j++) { dp[j] = 1; } for (int i = 1; i &lt; m; i++) { for (int j = 0; j &lt; n; j++) { if (obstacleGrid[i][j] == 1) { dp[j] = 0; } else if (j != 0) { dp[j] += dp[j - 1]; } } } return dp[n - 1]; }} &amp;Python123456789101112131415161718192021222324#version1class Solution: def uniquePathsWithObstacles(self, obstacleGrid): m = len(obstacleGrid) n = len(obstacleGrid[0]) if obstacleGrid[m - 1][n - 1] == 1 or obstacleGrid[0][0] == 1: return 0 dp = [[0] * n for _ in range(m)] for i in range(m): if obstacleGrid[i][0] == 0: # When encountering an obstacle, exit the loop directly, followed by the default of all 0 dp[i][0] = 1 else: break for j in range(n): if obstacleGrid[0][j] == 0: dp[0][j] = 1 else: break for i in range(1, m): for j in range(1, n): if obstacleGrid[i][j] == 1: continue dp[i][j] = dp[i - 1][j] + dp[i][j - 1] return dp[m - 1][n - 1] 1234567891011121314151617181920212223242526#version2class Solution: def uniquePathsWithObstacles(self, obstacleGrid): if obstacleGrid[0][0] == 1: return 0 dp = [0] * len(obstacleGrid[0]) # Create a one-dimensional list to store the number of paths # Initialise the number of paths in the first row for j in range(len(dp)): if obstacleGrid[0][j] == 1: dp[j] = 0 elif j == 0: dp[j] = 1 else: dp[j] = dp[j - 1] # Calculate the number of paths in other rows for i in range(1, len(obstacleGrid)): for j in range(len(dp)): if obstacleGrid[i][j] == 1: dp[j] = 0 elif j != 0: dp[j] = dp[j] + dp[j - 1] return dp[-1] # Returns the last element, i.e. the number of paths to the end point","link":"/2023/02/15/UniquePaths/"},{"title":"Introduction to Dynamic Programming","text":"What is Dynamic Programming?Dynamic programming, or DP for short, is most efficiently used if a problem has many overlapping subproblems. So each state in dynamic programming must be derived from the previous state, which distinguishes it from greedy, which does not have state derivation, but selects the optimal directly from the local… For example, there are N items and a backpack that can carry at most W items. The weight of the i th item is weight[i] and the value obtained is value[i] . Each item can be used only once, solve for which items will be loaded into the backpack to maximize the sum of item values. In dynamic programming dp[j] is derived from dp[j - weight[i]] and then max(dp[j], dp[j - weight[i]] + value[i]) is taken. But what if it’s greedy, each time you take an item pick the largest or smallest and you’re done, no relation to the previous state. So greed can’t solve the problem of dynamic programming. For a dynamic programming problem, I will break it down into the following five steps: Determine the dp[] and the meaning of the subscripts. Determine the recursive formula How the dp[] is initialized Determine the traversal order Derive of the dp[] with examples Some of you may wonder why you need to determine the recursion formula first and then think about initialization. Because in some cases the recursive formula determines how the dp array is initialized! Let’s start from a very classic math problem. Test yourselfThe Fibonacci numbers, commonly denoted F(n) form a sequence, called the Fibonacci sequence, such that each number is the sum of the two preceding ones, starting from 0 and 1. That is, 123F(0) = 0, F(1) = 1F(n) = F(n - 1) + F(n - 2), for n &gt; 1.Given n, calculate F(n). 1234Example 1:Input: n = 2Output: 1Explanation: F(2) = F(1) + F(0) = 1 + 0 = 1. 1234Example 2:Input: n = 3Output: 2Explanation: F(3) = F(2) + F(1) = 1 + 1 = 2. HINTDynamic programming in five steps:Here we are going to use a one-dimensional dp array to hold the results of the recursion Determining the dp[] and what the subscripts meandp[i] is defined as: the Fibonacci value of the i th number is dp[i] Determine the recursive formulaWhy is this a very simple introductory question? Because the question has given us the recursive formula straight away: the state transfer equation dp[i] = dp[i - 1] + dp[i - 2]; How to initialise the dp[]The question gives us how to initialise it directly as well, as follows: Determining the order of traversalFrom the recursive formula dp[i] = dp[i - 1] + dp[i - 2], it can be seen that dp[i] is dependent on dp[i - 1] and dp[i - 2], then the traversal order must be from front to back traversal Example derivation of the dp[] Following this recursive formula dp[i] = dp[i - 1] + dp[i - 2], let’s derive that when N is 10, the dp array should be the following array: 10 1 1 2 3 5 8 13 21 34 55 *If you find that the result is not correct, print out the dp[] to see if it is consistent with the column we derived. &amp;C++12345678910111213class Solution {public: int fib(int N) { if (N &lt;= 1) return N; vector&lt;int&gt; dp(N + 1); dp[0] = 0; dp[1] = 1; for (int i = 2; i &lt;= N; i++) { dp[i] = dp[i - 1] + dp[i - 2]; } return dp[N]; }}; Time Complexity: O(n) Space Complexity: O(n) It is certainly noticeable that we only need to maintain two values, not record the entire sequence. 123456789101112131415class Solution {public: int fib(int N) { if (N &lt;= 1) return N; int dp[2]; dp[0] = 0; dp[1] = 1; for (int i = 2; i &lt;= N; i++) { int sum = dp[0] + dp[1]; dp[0] = dp[1]; dp[1] = sum; } return dp[1]; }}; Time Complexity: O(n) Space Complexity: O(1) 12345678// Resursionclass Solution {public: int fib(int N) { if (N &lt; 2) return N; return fib(N - 1) + fib(N - 2); }}; Time Complexity: O(n^2) Space Complexity: O(n) &amp;Java12345678910111213class Solution { public int fib(int n) { if (n &lt; 2) return n; int a = 0, b = 1, c = 0; for (int i = 1; i &lt; n; i++) { c = a + b; a = b; b = c; } return c; }} 12345678910111213class Solution { public int fib(int n) { if (n &lt;= 1) return n; int[] dp = new int[n + 1]; dp[0] = 0; dp[1] = 1; for (int index = 2; index &lt;= n; index++){ dp[index] = dp[index - 1] + dp[index - 2]; } return dp[n]; }} &amp;Python12345678910111213141516#Dynamic Programming Iclass Solution: def fib(self, n: int) -&gt; int: if n == 0: return 0 dp = [0] * (n + 1) dp[0] = 0 dp[1] = 1 for i in range(2, n + 1): dp[i] = dp[i - 1] + dp[i - 2] return dp[n] 1234567891011121314#Dynamic Programming IIclass Solution: def fib(self, n: int) -&gt; int: if n &lt;= 1: return n dp = [0, 1] for i in range(2, n + 1): total = dp[0] + dp[1] dp[0] = dp[1] dp[1] = total return dp[1]","link":"/2023/01/15/DynamicProgrammingBasic/"},{"title":"Intro of Machine Learning","text":"01 BackpackThere are n items and a backpack that can carry at most w weights. The weight of the ith item is weight[i] and the value obtained is value[i] . Each item can be used only once, so solve for the items in the backpack that have the greatest sum of values. There are really only two states for each item, take or don’t take, so you can use backtracking to search out all the cases, then the time complexity is o(2^n), where n denotes the number of items. So the violent solution is exponential level time complexity. It is only then that the dynamic programming solution is needed for optimisation! In the following explanation, I give an example: The maximum weight of the backpack is 4. The items are: Ask what is the maximum value of an item that a backpack can carry? The numbers that appear in the following explanations and illustrations are based on this example. 2D dp[] 01 backpacks1. Determine the dp[]] and the meaning of indexdp[i] [j] : denotes that there are dp[i] [j] distinct paths from (0 , 0) to (i, j). For the backack problem, one way to write it is to use a two-dimensional array, i.e., dp[i] [j] represents the maximum sum of the values of any of the items with index [0-i] that can be taken and put into a backpack with capacity j. The following is the definition of dp[i] [j]: 2. Determine the recursive formulaRecall again the meaning of dp[i] [j]: what is the maximum sum of values that can be taken arbitrarily from the items with subscripts [0-i] and put into a backpack with capacity j. Then there can be two directions to introduce dp[i] [j] that: Do not put item i: introduced by dp[i - 1] [j], that is, the capacity of the backpack is j, the maximum value of not putting item i inside, at this time dp[i] [j] is dp[i - 1] [j]. (In fact, it means that when the weight of item i is greater than the weight of backpack j, item i cannot be put into the backpack, so the value in the backpack remains the same as before.) Putting item i: introduced by dp[i - 1] [j - weight[i]], dp[i - 1] [j - weight[i]] is the maximum value of not putting item i when the capacity of the backpack is j - weight[i], then dp[i - 1] [j - weight[i]] + value[i] (the value of item i) is the value of the backpack to put item i to get the the maximum value of item i in the backpack So the recursive formula: dp[i] [j] = max(dp[i - 1] [j], dp[i - 1] [j - weight[i]] + value[i]) 3. How the dp[] is initialisedRegarding the initialisation, it must match the definition of the dp[] , otherwise it will get messier and messier when it comes to the recursive formula. First from the definition of dp[i] [j], if the backpack capacity j is 0, that is, dp[i] [0], no matter which items are selected, the sum of the backpack value must be 0. As shown in the figure: Look at the other cases again: The state transfer equation dp[i] [j] = max(dp[i - 1] [j], dp[i - 1] [j - weight[i]] + value[i]); It can be seen that i is derived from i-1, and then must be initialised if i is 0. dp[0] [j], i.e.: the maximum value that can be stored in a backpack of each capacity when i is 0 and the item numbered 0 is stored. Then it is clear that when j &lt; weight[0], dp[0] [j] should be 0, because the capacity of the backpack is smaller than the weight of the numbered 0 item. When j &gt;= weight[0], dp[0] [j] should be value[0], because the backpack capacity is enough for item number 0. The code is initialised as follows: 1234567for (int j = 0 ; j &lt; weight[0]; j++) { // Of course this step can be omitted if the dp[][] is pre-initialised to 0 dp[0][j] = 0;}// positive-order traversalfor (int j = weight[0]; j &lt;= bagweight; j++) { dp[0][j] = value[0];} At this point the dp[] [] is initialised as shown: Both dp[0] [j] and dp[i] [0] are initialised, so how much should the other index be initialised? In fact, from the recursive formula: dp[i] [j] = max(dp[i - 1] [j], dp[i - 1] [j - weight[i]] + value[i]); you can see that dp[i] [j] is derived from the upper-left value, then the other index initialised to whatever value is OK, because they will all be overwritten. Initially -1, initially -2, initially 100, all are fine! But it’s just that it’s easier to uniformly initialise the dp[] [] to 0 uniformly at the start. As shown: Finally, the complete code is: 123456// initialise dp[][]vector&lt;vector&lt;int&gt;&gt; dp(weight.size(), vector&lt;int&gt;(bagweight + 1, 0));for (int j = weight[0]; j &lt;= bagweight; j++) { dp[0][j] = value[0];} 4. Determining the traversal orderIn the following figure, it can be seen that there are two traversal dimensions: item and pack weight Iterate over items first, then over pack weights 12345678// The size of the weight array is the number of items.for(int i = 1; i &lt; weight.size(); i++) { // traversal item for(int j = 0; j &lt;= bagweight; j++) { // traversal backpack capacity if (j &lt; weight[i]) dp[i][j] = dp[i - 1][j]; else dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]); }} Iterate over weights first, then over item 1234567for(int j = 0; j &lt;= bagweight; j++) { // traversal backpack capacity for(int i = 1; i &lt; weight.size(); i++) { // traversal item if (j &lt; weight[i]) dp[i][j] = dp[i - 1][j]; else dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]); }} 5. Derivation of dp[] []by example The end result is dp[2] [4] &amp;C++12345678910111213141516171819202122232425262728void test_2_wei_bag_problem1() { vector&lt;int&gt; weight = {1, 3, 4}; vector&lt;int&gt; value = {15, 20, 30}; int bagweight = 4; // 2d-array vector&lt;vector&lt;int&gt;&gt; dp(weight.size(), vector&lt;int&gt;(bagweight + 1, 0)); // initialise for (int j = weight[0]; j &lt;= bagweight; j++) { dp[0][j] = value[0]; } // size of array = number of items for(int i = 1; i &lt; weight.size(); i++) { // travesal item for(int j = 0; j &lt;= bagweight; j++) { // traversal backpack capacity if (j &lt; weight[i]) dp[i][j] = dp[i - 1][j]; else dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]); } } cout &lt;&lt; dp[weight.size() - 1][bagweight] &lt;&lt; endl;}int main() { test_2_wei_bag_problem1();} &amp;Java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class BagProblem { public static void main(String[] args) { int[] weight = {1,3,4}; int[] value = {15,20,30}; int bagSize = 4; testWeightBagProblem(weight,value,bagSize); } public static void testWeightBagProblem(int[] weight, int[] value, int bagSize){ // dp[] int goods = weight.length; // number of items int[][] dp = new int[goods][bagSize + 1]; // Initialise the dp array // After the array is created, the default value is 0. for (int j = weight[0]; j &lt;= bagSize; j++) { dp[0][j] = value[0]; } // Fill the dp[][] for (int i = 1; i &lt; weight.length; i++) { for (int j = 1; j &lt;= bagSize; j++) { if (j &lt; weight[i]) { /** * If the current backpack is not as big as the current item i, then item i is not put down. * Then the maximum value of the first i-1 items that can be put down is the maximum value of the current situation. */ dp[i][j] = dp[i-1][j]; } else { /** * The current capacity of the backpack can hold the item i * Then at this point there are two scenarios: * 1, do not put item i * 2. Put item i * Compare these two cases, which backpack has the largest value of items. */ dp[i][j] = Math.max(dp[i-1][j] , dp[i-1][j-weight[i]] + value[i]); } } } // print dp[][[]] for (int i = 0; i &lt; goods; i++) { for (int j = 0; j &lt;= bagSize; j++) { System.out.print(dp[i][j] + &quot;\\t&quot;); } System.out.println(&quot;\\n&quot;); } }} &amp;Python123456789101112131415161718192021222324#no parameterdef test_2_wei_bag_problem1(): weight = [1, 3, 4] value = [15, 20, 30] bagweight = 4 # 2d array dp = [[0] * (bagweight + 1) for _ in range(len(weight))] # initialise for j in range(weight[0], bagweight + 1): dp[0][j] = value[0] # size of array = number of items for i in range(1, len(weight)): # travesal item for j in range(bagweight + 1): # traversal backpack capacity if j &lt; weight[i]: dp[i][j] = dp[i - 1][j] else: dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]) print(dp[len(weight) - 1][bagweight])test_2_wei_bag_problem1() 12345678910111213141516171819202122232425262728#with parameterdef test_2_wei_bag_problem1(weight, value, bagweight): # 2d array dp = [[0] * (bagweight + 1) for _ in range(len(weight))] # initialise for j in range(weight[0], bagweight + 1): dp[0][j] = value[0] # size of array = number of items for i in range(1, len(weight)): # travesal item for j in range(bagweight + 1): # traversal backpack capacity if j &lt; weight[i]: dp[i][j] = dp[i - 1][j] else: dp[i][j] = max(dp[i - 1][j], dp[i - 1][j - weight[i]] + value[i]) return dp[len(weight) - 1][bagweight]if __name__ == &quot;__main__&quot;: weight = [1, 3, 4] value = [15, 20, 30] bagweight = 4 result = test_2_wei_bag_problem1(weight, value, bagweight) print(result)","link":"/2023/03/15/MachineLearning01/"},{"title":"Machine Learning - ClassificationI","text":"1.1 sklearn Converters and Estimators1.1.1 Transformer - parent class of feature engineeringThink about the steps of feature engineering done before? 1 Instantiate (instantiated a transformer class (Transformer). 2 Call fit_transform (for documents to build the classification word frequency matrix, can not be called at the same time) We call the interface* of feature engineering as a *transformer, where the transformer call there are so many forms of: fit transform fit fit transform What is the difference between these methods? Let’s take a look at the following code to get a clear picture: ​ 1.1.2 Estimator (implementation of sklearn machine learning algorithm)​ An important role in sklearn is played by estimators1 Estimators for classification: k-nearest neighbour: sklearn.neighbors Bayesian: sklearn.naive_bayes Logistic Regression: sklearn.linear_model.LogisticRegression Decision Trees and Random Forest: sklearn.linear_model. sklearn.tree 2 Estimators for regression Linear regression: sklearn.linear_model.LinearRegression Ridge Regression: sklearn.linear_model.Ridge 3 Estimators for Unsupervised Learning clustering: sklearn.linear_model. sklearn.cluster.KMeans Workflow of Estimator: 1.2 K-Nearest Neighbour Algorithm​ 1.2.1 What is K-Nearest Neighbour Algorithm sklearn.neighbours.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’) KNN core idea: Your “neighbours” infer your category. 1 How to determine who is a neighbour? Calculate the distance: Distance formula Euclidean distance Manhattan distance Absolute distance Minkowski distance ​ 2 Analysis of film genres n_neighbors: value of k ​ k = 1 Romance​ k = 2 Romance​ ……​ k = 6 Undetermined​ k = 7 Action film What if the number of recent films taken is different? What would be the result?If k is too small, it is vulnerable to outliers.If k is too large, the sample will be unbalanced.Combine with the previous data of dating objects, analyse what kind of processing needs to be done in: Handling of dimensionless quantities Standardisation 1.2.2 Case 1: Iris species predictionAcquisition of data Data set division Feature engineering - Standardisation KNN predictor process Model evaluation 12345678910111213141516171819202122232425262728293031from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifierdef knn_iris(): #Acquisition of data iris = load_iris() #Dataset division x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=22) #Feature engineering - Standardisation transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) #KNN predictor process estimator = KNeighborsClassifier(n_neighbors=3) estimator.fit(x_train,y_train) #model evaluation #Method 1: Direct comparison of true and predicted values y_predict = estimator.predict(x_test) print(&quot;y_predict:\\n&quot;,y_predict) print(&quot;Direct comparison of true and predicted values:\\n&quot;,y_test == y_predict) #Method 2: Calculation of accuracy score = estimator.score(x_test,y_test) print(&quot;accuracy:\\n&quot;,score) return Noneif __name__ == '__main__': knn_iris() 1.2.3 K-Nearest Neighbour Summary​ Pros: simple, easy to understand, easy to implement, no training required​ Disadvantages: ​ 1) K value must be specified, improper choice of K value then classification accuracy cannot be guaranteed ​ 2) Lazy algorithm, large amount of computation and memory overhead when classifying test samples Usage scenarios: small data scenarios, thousands to tens of thousands of samples, specific scenarios and specific business to test 1.3 Model Selection and Tuning1.3.1 What is cross validation ?Cross-validation: Take the training data and divide it into training and validation sets. Take the following figure as an example: The data is divided into 4 parts, one of which is used as the validation set. Then after 4 times (groups) of tests, each time to replace a different validation set. That is to say, the results of the 4 sets of models are obtained, and the average value is taken as the final result. Also known as 4 fold cross validation. We knew before that the data is divided into training set and test set, but in order to get more accurate model results from training. Do the following: Training set: training set + validation set. Test set: test set 1.3.2 Hyperparametric Search - Grid SearchUsually, there are many parameters that need to be specified manually (e.g., the value of K in the k-nearest neighbour algorithm), which are called hyperparameters. However, the manual process is tedious, so several combinations of hyperparameters need to be preset for the model. Each set of hyperparameters is evaluated using cross-validation. Finally the optimal parameter combination is selected to build the model sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None) Perform an exhaustive search for the specified parameter values of the estimator estimator: estimator object param_grid: estimator parameter (dict) {“n_neighbors”:[1,3,5]} cv: specify several folds of cross-validation fit(): input training data score(): accuracy Result Analysis: 1234Best parameters: best_params_Best result: best_score_Best estimator: best_estimator_Cross-validation results: cv_results_ Selective Tuning of Iris Instances : 12345678910111213141516171819202122232425262728293031323334353637383940414243from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.model_selection import GridSearchCVdef knn_iris_gscv(): #=add grid search and cross validation #get data iris = load_iris() #split the dataset x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=22) #feature engneering - Standard transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.fit_transform(x_test) #KNN estimator estimator = KNeighborsClassifier() #don't need extra K #grid search and CV #prepare data param_data = {&quot;n_neighbors&quot;:[1,3,5,7,9,11]} estimator = GridSearchCV(estimator,param_grid=param_data,cv=10) estimator.fit(x_train,y_train) #model evaluation #Method 1: Direct comparison of true and predicted values y_predict = estimator.predict(x_test) print(&quot;y_predict:\\n&quot;,y_predict) print(&quot;Direct comparison of true and predicted values:\\n&quot;,y_test == y_predict) #Method 2: calculate the accuracy score = estimator.score(x_test,y_test) print(&quot;accuracy:\\n&quot;,score) #optimal parameter print(&quot;optimal parameter:\\n&quot;,estimator.best_params_) #optimal result print(&quot;optimal result:\\n&quot;,estimator.best_score_) #optimal estimator print(&quot;optimal estimator:\\n&quot;,estimator.best_estimator_) #result of CV print(&quot;result of CV:\\n&quot;,estimator.cv_results_) return Noneif __name__ == '__main__': knn_iris_gscv()","link":"/2023/04/15/MachineLearning03/"},{"title":"Intro of Machine Learning II","text":"1.4 Feature Preprocessing: Understand numerical data, categorical data characteristics Apply MinMaxScaler and StandardScaler to implement Normalisation of feature data Feature preprocessing: the process of converting feature data into feature data more suitable for the algorithmic model by means of some transformation functions. Why do we need to normalise/standardise?Large differences in the units or sizes of features, or a feature whose variance is orders of magnitude larger than other features, can easily influence (dominate) the target result, preventing some algorithms from learning other features. 1.41 Normalisation: Maps the data by transforming the original data between (default [0,1]) sklearn.preprocessing.MinMaxScaler (feature_range=(0,1)…) MinMaxScalar.fit_transform(X) ​ X: numpy array format data[n_samples,n_features] return value: transformed array of the same shape 123456789101112import pandas as pdfrom sklearn.preprocessing import MinMaxScalerdef minmax_demo(): data = pd.read_csv(&quot;dating.txt&quot;) data = data.iloc[:,:3] #只取数据前三列 print(&quot;data=\\n&quot;,data) transfer = MinMaxScaler() #默认0-1 data_new = transfer.fit_transform(data) print(&quot;data_new=\\n&quot;,data_new)if __name__ == '__main__': minmax_demo() Note that the maximum and minimum values are changing, in addition, the maximum and minimum values are very susceptible to anomalies so this method is less robust and only suitable for traditional accurate small data scenarios 1.4.2 Standardisation:Transform the data to a mean of 0 and standard deviation of 1 by transforming the original data.For normalisation: If outliers occur that affects the maximum and minimum values, then the result will obviously changeFor standarlisation:If outliers occur, due to the fact that there is a With a fixed amount of data, a small number of anomalies do not have a significant effect on the mean, and thus the variance changes less. sklearn.preprocessing.StandardScaler()After processing, for each column, all data are clustered around a mean of 0 and a standard deviation of 1 StandardScaler.fit transform(X)X: numpyarray format data In amples,n_features] return value:transformed shape of the same array 1.5 Feature Reduction1.5.1 Dimensionality reductionDimensionality reduction is the process of reducing the number of random variables (features) to obtain a set of “uncorrelated“ principal variables, subject to certain constraints. Reduce the number of random variables Correlated Feature eg: Correlation between relative humidity and rainfall It is because we are using features for learning when we are doing training. If there is a problem with the features themselves or if there is a strong correlation between the features, it will have a greater impact on the algorithm’s ability to learn the predictions 1.5.2 Two approaches to dimensionality reduction Feature selection Principal Component Analysis (can be understood as a form of feature extraction) 1.5.3 What is feature selection Definition: Data containing redundant or correlated variables (or features, attributes, metrics, etc.) designed to identify the main features from the original ones. Method: Filter: focuses on exploring the characteristics of the feature itself, the association between the feature and the feature and the target value. Variance Selection: low variance feature filtering. Variance selection method: low variance feature filtering. Correlation Coefficient Embedded: algorithm automatically selects features (correlation between features and target value). Decision Tree:Information Entropy, Information Gain9 Regularisation:L1, L2 Deep learning: convolution, etc. 3. Filter 3.1 Low variance feature filteringRemove some features with low variance, the significance of variance was covered earlier. Combined with the size of the variance to consider the perspective of this approach: feature variance is small: the value of most samples of a feature is relatively similar feature variance is large: the value of many samples of a feature are different 3.1.1sklearn.feature_selection.VarianceThreshold(threshold = 0.0) Remove all low variance features Variance.fit_transform(X) X:numpy array format data [n_samplesn_features] Return value:Features with training set variance below threshold are removed. The default value is to keep all non-zero variance features, i.e., remove all features with the same value in all samples.4.1.2 Data computationWe perform a filter between the indicator features of certain stocks, the data is in the file “factor_regression_data/factor_returns.csv” excluding the indexdate’return columns (these types do not match and are not required indicators).These are the characteristics in total 1234567891011121314151617181920212223242526272829def variance_demo(): &quot;&quot;&quot; Low variance feature filtering :return: &quot;&quot;&quot; # 1、load data​ data = pd.read_csv(&quot;factor_returns.csv&quot;)​ data = data.iloc[:, 1:-2]​ print(&quot;data:\\n&quot;, data)# 2、initialise transer()transfer = VarianceThreshold(threshold=10)# 3、use fit_transformdata_new = transfer.fit_transform(data)print(&quot;data_new:\\n&quot;, data_new, data_new.shape)r1 = pearsonr(data[&quot;pe_ratio&quot;], data[&quot;pb_ratio&quot;])print(&quot;correlation coefficients：\\n&quot;, r1)r2 = pearsonr(data['revenue'], data['total_expense'])print(&quot;the correlation between revenue and total_expense：\\n&quot;, r2)return None 3.2 Correlation coefficients 3.21 CharacteristicsThe value of correlation coefficient lies between -1 and +1 i.e. -1&lt;=r&lt;=1. The properties are as follows: When r&gt;0, it means that the two variables are positively planted off, r&lt;0, the two variables are negatively correlated When |r|=1, it means that the two variables are perfectly correlated, when r=0, it means that there is no correlation between the two variables When 0&lt;|r|&lt;1, it means that there is a certain degree of correlation between the two variables. The closer r is to 1, the closer the linear relationship between the two variables; the closer r is to 0, the weaker the linear correlation between the two variables. Generally can be divided into three levels: r &lt; 0.4 for low degree of correlation; 0.4&lt;|rl &lt; 0.7 for significant correlation; 0.7&lt;=|r| &lt; 1 for high degree of linear correlation 1234from scipy.stats import pearsonrx :(N,)array_likey :(N,) array_like Returns: (Pearson's correlation coefficient, p-value) 1.6 PCA1.6.1 What is Principal Component Analysis (PCA)?Definition:The process of transforming high-dimensional data into low-dimensional data, in which the original data may be discarded and new variables may be created.Role: is the dimensionality of the data compression, as far as possible to reduce the dimensionality of the original data (complexity), the loss of a small amount of information.Application: regression analysis or cluster analysis. ​ Given: Now place the data in a two-dimensional spatial Cartesian coordinate system Now let’s figure out how to reduce the two-dimensional data to one dimension (a straight line) We can see that although it is reduced to one dimension, there is a loss of data from the original five points to three, and so: sklearn.decomposition.PCA(n_components=None Decompose data into lower dimensional spaces n_components ​ Fractional: Indicates what percent of the information is retained ​ Integer: Reduces to how many features. **Pca.fit_transform(X)**X:numpy array format data [n_samples,n_features] Return value: array of the specified dimension after transformation For example: 12345678910from sklearn.decomposition import PCAdef pca_demo(): data = [[2,8,4,5],[6,3,0,8],[5,4,9,1]] transfer = PCA(n_components=2) #reduce to 2 features data_new = transfer.fit_transform(data) print(&quot;data_new:\\n&quot;,data_new) return Noneif __name__ == '__main__': pca_demo()","link":"/2023/03/31/MachineLearning02/"},{"title":"Machine Learning - ClassificationII","text":"1.4 Navie Bayes1.4.1 What is a Navie Bayesian?Naive Bayes is a probabilistic machine learning algorithm based on the Bayes Theorem, used in a wide variety of classification tasks. 1.4.2 What is Conditional Probability? Coin Toss and Fair Dice Example When you flip a fair coin, there is an equal chance of getting either heads or tails. So you can say the probability of getting heads is 50%. Similarly what would be the probability of getting a 1 when you roll a dice with 6 faces? Assuming the dice is fair, the probability of 1/6 = 0.166. Playing Cards Example If you pick a card from the deck, can you guess the probability of getting a queen given the card is a spade? Well, I have already set a condition that the card is a spade. So, the denominator (eligible population) is 13 and not 52. And since there is only one queen in spades, the probability it is a queen given the card is a spade is 1/13 = 0.077 This is a classic example of conditional probability. So, when you say the conditional probability of A given B, it denotes the probability of A occurring given that B has already occurred. Mathematically, Conditional probability of A given B can be computed as: P(A|B) = P(A AND B) / P(B) 1.4.3 The Bayes Rule​ ​ 1.4.4 The Naive BayesThe Bayes Rule provides the formula for the probability of Y given X. But, in real-world problems, you typically have multiple X variables. When the features are independent, we can extend the Bayes Rule to what is called Naive Bayes. It is called ‘Naive’ because of the naive assumption that the X’s are independent of each other. 1.4.5 Naive Bayes Example by HandSay you have 1000 fruits which could be either ‘banana’, ‘orange’ or ‘other’. These are the 3 possible classes of the Y variable. We have data for the following X variables, all of which are binary (1 or 0). Long Sweet Yellow The first few rows of the training dataset look like this: For the sake of computing the probabilities, let’s aggregate the training data to form a counts table like this. So the objective of the classifier is to predict if a given fruit is a ‘Banana’ or ‘Orange’ or ‘Other’ when only the 3 features (long, sweet and yellow) are known. Let’s say you are given a fruit that is: Long, Sweet and Yellow, can you predict what fruit it is? Step 1: Compute the ‘Prior’ probabilities for each of the class of fruits. That is, the proportion of each fruit class out of all the fruits from the population. You can provide the ‘Priors’ from prior information about the population. Otherwise, it can be computed from the training data. For this case, let’s compute from the training data. Out of 1000 records in training data, you have 500 Bananas, 300 Oranges and 200 Others. So the respective priors are 0.5, 0.3 and 0.2. P(Y=Banana) = 500 / 1000 = 0.50 P(Y=Orange) = 300 / 1000 = 0.30 P(Y=Other) = 200 / 1000 = 0.20 Step 2: Compute the probability of evidence that goes in the denominator. This is nothing but the product of P of Xs for all X. This is an optional step because the denominator is the same for all the classes and so will not affect the probabilities. P(x1=Long) = 500 / 1000 = 0.50 P(x2=Sweet) = 650 / 1000 = 0.65 P(x3=Yellow) = 800 / 1000 = 0.80 Step 3: Compute the probability of likelihood of evidences that goes in the numerator. It is the product of conditional probabilities of the 3 features. If you refer back to the formula, it says P(X1 |Y=k). Here X1 is ‘Long’ and k is ‘Banana’. That means the probability the fruit is ‘Long’ given that it is a Banana. In the above table, you have 500 Bananas. Out of that 400 is long. So, P(Long | Banana) = 400/500 = 0.8. Here, I have done it for Banana alone. Probability of Likelihood for Banana P(x1=Long | Y=Banana) = 400 / 500 = 0.80 P(x2=Sweet | Y=Banana) = 350 / 500 = 0.70 P(x3=Yellow | Y=Banana) = 450 / 500 = 0.90. So, the overall probability of Likelihood of evidence for Banana = 0.8 * 0.7 * 0.9 = 0.504 Step 4: Substitute all the 3 equations into the Naive Bayes formula, to get the probability that it is a banana. Similarly, you can compute the probabilities for ‘Orange’ and ‘Other fruit’. The denominator is the same for all 3 cases, so it’s optional to compute. Clearly, Banana gets the highest probability, so that will be our predicted class. 1.4.6 What is Laplace Correction?The value of P(Orange | Long, Sweet and Yellow) was zero in the above example, because, P(Long | Orange) was zero. That is, there were no ‘Long’ oranges in the training data. It makes sense, but when you have a model with many features, the entire probability will become zero because one of the feature’s value was zero. To avoid this, we increase the count of the variable with zero to a small value (usually 1) in the numerator, so that the overall probability doesn’t become zero. This approach is called ‘Laplace Correction’. Most Naive Bayes model implementations accept this or an equivalent form of correction as a parameter. 1.4.7 Case: Classifying News12345678910111213141516171819202122232425262728293031323334353637from sklearn.datasets import fetch_20newsgroupsfrom sklearn.model_selection import train_test_splitfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNBdef nb_news(): &quot;&quot;&quot; :return: &quot;&quot;&quot;# 1）Acquisition of data news = fetch_20newsgroups(subset=&quot;all&quot;)# 2）Dataset divisionx_train, x_test, y_train, y_test = train_test_split(news.data, news.target)# 3）Feature engineering-tfidftransfer = TfidfVectorizer()x_train = transfer.fit_transform(x_train)x_test = transfer.transform(x_test)# 4）Navie Bayes predictor processestimator = MultinomialNB()estimator.fit(x_train, y_train)# 5）model evaluation#Method 1: Direct comparison of true and predicted valuesy_predict = estimator.predict(x_test)print(&quot;y_predict:\\n&quot;, y_predict)print(&quot;Direct comparison of true and predicted values:\\n&quot;, y_test == y_predict)# Method 2: Calculation of accuracyscore = estimator.score(x_test, y_test)print(&quot;accuracy：\\n&quot;, score)return Noneif __name__ == '__main__': nb_news() 1.5 Decision Tree1.5.1 What are Decision Tree Classifiers?Decision tree classifiers are supervised machine learning models. This means that they use prelabelled data in order to train an algorithm that can be used to make a prediction. Decision tree classifiers work like flowcharts. Each node of a decision tree represents a decision point that splits into two leaf nodes. Each of these nodes represents the outcome of the decision and each of the decisions can also turn into decision nodes. Eventually, the different decisions will lead to a final classification. The diagram below demonstrates how decision trees work to make decisions. The top node is called the root node. Each of the decision points are called decision nodes. The final decision point is referred to as a leaf node. 1.5.2 How do Decision Tree Classifiers Work?Decision trees work by splitting data into a series of binary decisions. These decisions allow you to traverse down the tree based on these decisions. You continue moving through the decisions until you end at a leaf node, which will return the predicted classification. The image below shows a decision tree being used to make a classification decision: How does a decision tree algorithm know which decisions to make? The definition of entropy: H is called information entropy in bits. Information gain The information gain g(D,A) of feature A on the training dataset D, is defined as the difference between the information entropy H(D) of the set D and the information conditional entropy H(D|A) of D under the given conditions of feature A 1.5.3 Example: Bank Loan We use A1, A2, A3, and A4 to represent age, having a job, owning a house, and loan status. The final calculation is g(D,A1) = 0.313, g(D,A2) = 0.324, g(D,A3) = 0.420, g(D,A4) = 0.363. so we choose A3 as the first feature for the division. This way we can slowly build up one tree at a time 1.5.4 Titanic Survival Prediction123456789101112131415161718192021222324252627282930313233343536373839import pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.feature_extraction import DictVectorizerfrom sklearn.tree import DecisionTreeClassifier, export_graphvizdef titanic():# 1 Acquisition of data path = &quot;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt&quot; titanic = pd.read_csv(path) # Filtering eigenvalues and target values x = titanic[[&quot;pclass&quot;, &quot;age&quot;, &quot;sex&quot;]] y = titanic[&quot;survived&quot;] # 2、data process # 1）handle missed value x[&quot;age&quot;].fillna(x[&quot;age&quot;].mean(), inplace=True) #fill average value # 2) Convert to Dictionary x = x.to_dict(orient=&quot;records&quot;) # 3、dataset division x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=22) # 4、Dictionary Features Extraction transfer = DictVectorizer() x_train = transfer.fit_transform(x_train) x_test = transfer.transform(x_test) # 3）Decision Tree Predictor estimator = DecisionTreeClassifier(criterion=&quot;entropy&quot;, max_depth=8) estimator.fit(x_train, y_train)# 4）model evaluation# Method 1: Direct comparison of true and predicted valuesy_predict = estimator.predict(x_test)print(&quot;y_predict:\\n&quot;, y_predict)print(&quot; Direct comparison of true and predicted values:\\n&quot;, y_test == y_predict)# Method 2: Calculating accuracyscore = estimator.score(x_test, y_test)print(&quot;accuracy：\\n&quot;, score)# Visualise Decision Treeexport_graphviz(estimator, out_file=&quot;titanic_tree.dot&quot;, feature_names=transfer.get_feature_names())if __name__ == '__main__': titanic()","link":"/2023/04/30/MachineLearning04/"},{"title":"Machine Learning - RegressionI","text":"1.1 Linear Regression1.1.1 Linear Regression Application Scenarios House price prediction Sales forecasting Finance: Loan amount prediction, using linear regression and coefficient analysis factors ​ 1.1.2 What is Linear Regression? Definition Linear regression is a type of analysis that uses regression equations (functions) to model the relationship between one or more independent variables (eigenvalues) and the dependent variable (target value). The case of only one independent variable is called univariate regression, and the case of more than one independent variable is called multivariate regression ​ So how do you understand it? Let’s look at a few examples Final grade: 0.7x exam grade + 0.3x usual grade House price = 0.02x distance from centre +0.04x urban nitrous oxide concentration +(-0.12x average house price for owner-occupied housing) +0.254x urban crime rate In the two examples above, we see that a relationship has been established between the eigenvalues and the target values, and this relationship can be understood as a linear model. Analysis of the relationship between the characteristics and the target in linear regressionThere are two kinds of linear models among linear regression, one is a linear relationship and the other is a non-linear relationship. Here we can only draw a plane to better understand, so all use a single feature or two features as an example. 1.1.3 Principles of loss and optimisation in linear regressionAssuming the house example just given, this relationship exists between the real data 1Real relationship: real house price = 0.02x distance from centre + 0。04x urban nitrous oxide concentration + (-0.12x average house price for owner-occupied housing) +0.254x urban crime rate So now, let’s randomly specify a relationship (guess) 1Randomly specify the relationship: predicted house price = 0.25x distance from the centre + 0.14x urban nitric oxide concentration + 0.42x average house price for owner-occupied housing) +0.36x urban crime rate Isn’t there some error between the real result and our prediction? The goal is to find a line that minimises the sum of the distances from all points to the line, i.e. minimises the error 1. Loss functionDefine the total loss: ​ - y_i is the true value of the ith training sample-*** h(x_i)*** is the eigenvalue combination prediction function for the ith training sample -Also known as least squares How do we go about reducing this loss so that we can predict a little more accurately? Since this loss exists, we have always said that machine learning has the function of automatic learning, which can be reflected in linear regression. Here you can optimise the total loss of the regression by using some optimisation methods (actually a derivative function in mathematics)!! 2. Optimisation Normal Equation Understanding: X is the matrix of eigenvalues and y is the matrix of target values. Directly find the best resultDisadvantage: When the features are too many and complex, the solution is too slow and no results are obtained. Grandient Descent Understanding: a is the learning rate, need to be specified manually (hyperparameters), a next to the overall representation of the direction of the function along the direction of the decline to find, and finally you can find the lowest point of the valley, and then update the value of the W Use: To find better results for tasks with very large training data sizes. 1.1.4 Linear Regression Application sklearn.linear_model.LinearRegression(fit_intercept=True) Optimisation by regular equations fit_intercept: whether to compute bias LinearRegression.coef: regression coefficients LinearRegression.intercept: bias sklearn.linear_model.SGDRegressor(loss=”squared_loss”, fit_intercept=Truelearning_rate =’invscaling’, eta0=0.01) SGDRegressor class implements stochastic gradient descent learning, which supports different loss functions and regularization penalty terms to fit linear regression models. loss:loss type loss=”squared loss”normal least squares fit_intercept: whether to compute bias learning_rate : string, optional Learning rate padding‘constant’: eta = eta0‘optimal’: eta = 1.0 / (alpha * (t + t0)) ‘invscaling’: eta = eta0 / pow(t, power_t) power_t=0.25: present in parent class For a constant learning rate, you can use learning_rate=’constant and use eta0 to specify the learning rateSGDRegressor.coef: regression coefficient 0SGDRegressorintercept_: bias 1.1.5 Boston Home Price Forecast123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LinearRegression, SGDRegressorfrom sklearn.metrics import mean_squared_errordef linear1(): &quot;&quot;&quot; A normal equations optimisation approach to forecasting house prices in Boston :return: &quot;&quot;&quot; # 1）Acquisition of data boston = load_boston() # 2）dataset division x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22) # 3）Normalisation transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.transform(x_test) # 4）estimator estimator = LinearRegression() estimator.fit(x_train, y_train) # 5）result print(&quot;The normal equation-weights are：\\n&quot;, estimator.coef_) print(&quot;The normal equation-bias are：\\n&quot;, estimator.intercept_) # 6）model evaluation y_predict = estimator.predict(x_test) print(&quot;predicted house price：\\n&quot;, y_predict) error = mean_squared_error(y_test, y_predict) print(&quot;The normal equation-mean square error is：\\n&quot;, error) return Nonedef linear2(): &quot;&quot;&quot; Gradient descent optimisation approach to forecasting house prices in Boston :return: &quot;&quot;&quot; # 1）Acquisition of data boston = load_boston() print(&quot;number of features：\\n&quot;, boston.data.shape) # 2）dataset division x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22) # 3）Normalisation transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.transform(x_test) # 4）estimator estimator = SGDRegressor(learning_rate=&quot;constant&quot;, eta0=0.01, max_iter=10000, penalty=&quot;l1&quot;) estimator.fit(x_train, y_train) # 5）result print(&quot;Grandient Descent-weights：\\n&quot;, estimator.coef_) print(&quot;Grandient Descent-bias：\\n&quot;, estimator.intercept_) # 6）model evaluation y_predict = estimator.predict(x_test) print(&quot;predicted house price：\\n&quot;, y_predict) error = mean_squared_error(y_test, y_predict) print(&quot;Grandient Descent-mean square error is：\\n&quot;, error) return Noneif __name__ == '__main__': linear1() linear2() 1.1.6 About the optimisation methods GD, SGD, SAG1GD- Gradient Descentthe original gradient descent method needs to calculate the value of all samples to be able to derive the gradient, the amount of computation is large, so there will be a series of improvements later. 2 SGD- Stochastic gradient descentIt considers only one training sample in an iteration.Advantages:-Efficient-Easy to implementDisadvantages:-SGD requires many hyperparameters: e.g., regular term parameters, number of iterations.-SGD is sensitive to feature normalisation. 3 SAG-Stochastic Average Gradientdue to the slow rate of convergence, gradient descent based algorithms such as SAG have been proposed.Scikit-learn: ridge regression, logistic regression, etc. will have SAG optimisation","link":"/2023/05/15/MachineLearning05/"},{"title":"Machine Learning - RegressionII","text":"1.2 Overfitting&amp;Underfitting1.2.1 Definition Overfitting: A hypothesis that is able to obtain a better fit than other hypotheses on the training data, but does not fit the data well on the test dataset, is then considered to have overfitted the hypothesis. (Over modelling) Underfitting: A hypothesis that does not get a better fit on the training data and does not fit the data well on the test dataset is considered to be underfitting. (Model oversimplified) 1.2.2 Reason and SolutionsUnderfitting. Reason: Learning too few features to the data. Solution: Increase the number of features in the data Overfitting. Reason: Too many original features, some noisy features are present, the model is too complex because the model tries to take into account each test data point.. Solution: Regularisation 1.2.3Regularisation category L2 regularisation Function: It can make some of the W’s are very small, all close to 0, weaken the influence of a certain feature. Advantage:The smaller the parameter, the simpler the model, and the simpler the model, the less likely to produce overfitting phenomenon.Ridge regressionLoss function after adding L2 regularisation: Note: m is the number of samples and n is the number of features. L1 regularisationFunction: It can make some of these w values directly 0, removing the effect of this featureLASSO regression hw(xi) is the predicted value, yi is the true value, L1 regularisation is changing wj^2 to |wj|. 1.3 The improvement-Ridge RegressionRidge regression is, in fact, a type of linear regression. Only when the algorithm builds the regression equation, it adds the restriction of regularisation, so as to achieve the effect of resolving overfitting sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver=”auto”normalize=False) Linear regression with L2 regularisation alpha:strength of regularisation, also called λ（01 110) solver: will automatically choose the optimisation method based on the data sag :If the dataset, features are larger, choose this stochastic gradient descent optimisation normalize:whether the data is normalised or not normalize=False:can call preprocessing.StandardScaler to standardise data before fit Ridge.coef: regression weights Ridge.intercept_:regression bias The stronger the regularisation, the smaller the weighting factor will be The smaller the regularisation, the larger the weighting factor will be 123456789101112131415161718192021222324252627282930313233343536373839404142from sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LinearRegression, SGDRegressorfrom sklearn.metrics import mean_squared_errordef linear3(): &quot;&quot;&quot; Ridge Regression :return: &quot;&quot;&quot; # 1）Acquisition of data boston = load_boston() print(&quot;number of feature：\\n&quot;, boston.data.shape) # 2）dataset division x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=22) # 3）Standard transfer = StandardScaler() x_train = transfer.fit_transform(x_train) x_test = transfer.transform(x_test) #4）estimator estimator = Ridge(alpha=0.5, max_iter=10000) estimator.fit(x_train, y_train) # 5）result print(&quot;Ridige Regression-weights：\\n&quot;, estimator.coef_) print(&quot;Ridige Regression-bias：\\n&quot;, estimator.intercept_) # 6）model evaluation y_predict = estimator.predict(x_test) print(&quot;predicted house price：\\n&quot;, y_predict) error = mean_squared_error(y_test, y_predict) print(&quot;Ridige Regression-mean square error：\\n&quot;, error) return Noneif __name__ == '__main__': linear3()","link":"/2023/05/31/MachineLearning06/"},{"title":"Model Compression - Pruning","text":"Why is the Neural Network Pruning (Why?)Much of the success of convolutional neural networks stems from larger and larger network architectures and more and more neurons. While this allows neural networks to perform better in many tasks, it also puts more demands on computer hardware: more arithmetic power, more storage space. However, this makes the cost of using neural networks skyrocket, a problem that cannot be ignored in edge devices. Model compression looks to reduce model size while minimising the loss of accuracy. Neural network pruning achieves model compression by removing unnecessary weights (e.g., weights below a certain threshold) and neurons (ReLU, Dropout, and other nonlinear functions that bring about zero-setting operations, etc.) What is the Neural Network Pruning (What?)Pruning is the process of removing redundant channels, filters, neurons, and layers from a network in order to obtain a lighter weight network without compromising performance. Pruning classification Depending on the type of network elements, it can be classified as neuron pruning and connection pruning; Depending on whether the network structure is changed before and after pruning, it can be classified as structured pruning and unstructured pruning; Depending on whether there is pruning in the inference phase, it can be classified as static pruning and dynamic pruning; Structured pruning can include neuron pruning and connection pruning, but is changing the structure of the network before pruning. Unstructured pruning does not change the structure of the network, but rather changes the branch weights to 0, such that a branch with a weight of 0 has no effect on the whole network, and thus is equivalent to subtracting that branch. If it is observed that a branch with a weight of 0 at some iterations has a large effect on the current iteration (affecting performance), the zero weight caused by unstructured pruning can be dynamically restored to provide network performance. 1. Structured pruning neuron pruning Neurons are the basic nodes of a neural network. Neuron pruning is to cut out the network nodes and their related connections together that do not have much effect on the inputs and outputs, so this pruning method will change the structure of the network belongs to structural pruning. Neuron pruning based on neuron pruning more coarse-grained pruning, such as channel, filter, and layer pruning, also change the structure of the network will change the structure of the network. - channel-wise pruningSuppose we want to try to reduce the computation of a particular individual conv layer. The following information is known about it: Shape of Tensor:Input feature map X: N x c x IH X IW, where N is the batch size, c is the number of input feature maps, and IH and IW are the length and width of the input feature map respectivelyConvolutional filters W: n x c x Kh x Kw, where n and c are the number of input and output feature map channels for this conv layer, and Kh and Kw are the length and width of the conv kernel Output feature map Y: N x n x OH X OW, where N is the batch size, n is the number of output feature maps, and OH and OW are the length and width of the output feature map respectively. The problem we want to solve is how to reduce some channel information of X while ensuring that the overall information of Y is still not lost too much. The following equation can represent this optimisation problem. Step-by-step implementation of the channel pruningRelaxation of the original optimisation problemThe L0 optimisation problem discussed above is essentially an NP-hard problem. We give it some more constraints and thus turn it into an L1 norm optimisation problem as shown below: Firstly, the normalisation factor of the parameter β is increased so that it looks for the right set of parameters in the direction we care about (i.e., so that β has as many zeros as possible, thus pruning out a certain amount of information about the input channels).In addition, we add a strong constraint that the norm of W is 1 so that the W solution we get is not too simple The sub-optimisation problem for finding the βFirst, we can fix W constant and seek a combination of parameters β, i.e., decide which input channels of the input feature map can be discarded. This is clearly an NP-hard combinatorial optimisation problem, and we can use the classical heuristic LASSO approach to iteratively find the optimal β value, as shown in the following equation: The sub-optimisation problem of finding the WWe then fix the constant β obtained above and solve for the optimal W parameter values, essentially solving the following MSE problem. Overall solution scheme In practice, there are two ways to follow. One is to optimise problem one and two alternately, i.e., at the very beginning, initialise the W values with the trainee model weights and set λ to 0, i.e., without any normalization penality factor, and ||β|| to c, i.e., to retain all the input channels; then gradually increase the value of λ as the iteration proceeds; and every time we change the value of λ, we will keep solving β and W alternatively until the final ||β|| is stable. By the time the final value of ||β|| satisfies the value of less than or equal to c’, we will be able to solve for β and W alternately is less than or equal to c’, then we stop and get the final value of W as {βiWi}. The other one is to perform two optimisation problems separately. That is, first continuously optimise to get the value of β so that it satisfies ||β|| &lt;= c’, and then fix β and optimise once to get the W value. Observations show that the results of these two methods are similar, while the second one is clearly less computationally intensive and more practical Pruning for the whole modelThe method described above is for pruning a single conv layer. When applying this method to the whole CNN model, the method is similar, and it is only necessary to apply it to each layer sequentiallyBelow is the optimised pruning for each layer when applied to the whole model. In contrast to equation (1), it replaces Y with Y’, where Y’ refers to the output feature map on the original model, so we’ll take into account the effect of accumulated error when tuning the parameters for the whole model weigth.","link":"/2023/08/30/ModelCompressionPruning/"},{"title":"Activation Function","text":"I. What is an activation function?In contact with the deep learning, especially in the neural network, we will find that in each layer of the neural network output will use a function (such as sigmoid, tanh, Relu, etc.) on the results of the operation, this function is the activation function . So why do we need to add an activation function? And what are the problems if you don’t add it? Firstly, we know that neural networks mimic the workings of human neurons, and an activation function is a function added to an artificial neural network to help it learn complex patterns in the data. In a neuron, the input is weighted and summed in a series of weighted sums and then applied to another function, which is the activation function in this case. Similar to the neuron-based model of the human brain, the activation function ultimately determines whether or not a signal is delivered and what is to be fired to the next neuron. In an artificial neural network, the activation function of a node defines the output of that node for a given input or set of inputs. Activation functions can be classified as linear activation functions (linear equations controlling the mapping of inputs to outputs, e.g., f(x) = x, etc.) and nonlinear activation functions (nonlinear equations controlling the mapping of inputs to outputs, e.g., Sigmoid, Tanh, ReLU, LReLU, PReLU, Swish, etc.) why do we need to use the activation function?Because the input and output of each layer of a neural network is a linear summation process, the output of the next layer is just a linear transformation of the input function of the previous layer, so if there is no activation function, then no matter how complex the neural network you constructed, how many layers, the final output is a linear combination of inputs, and the purely linear combination is not capable of solving more complex problems. After introducing the activation function, we will find that the common activation function is non-linear, so it also introduces a non-linear element to the neuron, which allows the neural network to approximate any other non-linear function, which can make the neural network applied to more non-linear models. In general, the activation function is an important part in neurons, and in order to enhance the representation and learning ability of the network, the activation functions of neural networks are nonlinear and usually have the following properties: Continuous and differentiable (allowing for non-derivable at a few points), a differentiable activation function can be used directly to learn the network parameters using numerical optimisation methods; The activation function and its derivatives should be as simple as possible; too much complexity is not conducive to improving the network computation rate; The value domain of the derivative of the activation function should be in a suitable interval, not too big or too small, otherwise it will affect the efficiency and stability of training II. Common activation functions SigmoidSigmoid function, also called Logistic function, is used for the output of the hidden layer neuron, the value range is (0,1), it can map a real number to the interval (0,1), can be used to do binary classification, the effect is better when the feature difference is more complex or the difference is not particularly large. sigmoid is a very common activation function, the function expression is as follows: ​ The image resembles an S-curve The condition to use the Sigmoid activation function: The Sigmoid function has an output range of 0 to 1. Since the output value is limited to 0 to 1, it normalises the output of each neuron; A model used to take the predicted probability as an output. The Sigmoid function is well suited since the probabilities take values from 0 to 1; Gradient smoothing to avoid ‘jumpy’ output values; The function is differentiable. This means that the slope of the sigmoid curve can be found for any two points; Clear prediction, i.e. very close to 1 or 0. Shortcomings of the Sigmoid activation function: The gradient vanishes: note that the rate of change of the sigmoid function flattens out as it approaches 0 and 1, i.e. the gradient of the sigmoid tends to 0. When a neural network is backpropagated using the sigmoid activation function, neurons with outputs close to 0 or 1 have their gradient approaching 0. These neurons are called saturated neurons. Therefore, the weights of these neurons are not updated. In addition, the weights of the neurons connected to such neurons are also updated very slowly. The problem is called gradient vanishing. Thus, imagine a large neural network containing Sigmoid neurons, many of which are saturated, then the network cannot perform backpropagation. Non-zero centred: Sigmoid outputs are not zero centred, the output is constantly greater than 0. Non-zero centred outputs cause the inputs of the neurons in the subsequent layer to undergo a Bias Shift, which further slows down the convergence of the gradient descent. Computationally expensive: The exp() function is computationally expensive and slower to run on a computer than other nonlinear activation functions. Tanh The Tanh activation function is also called hyperbolic tangent activation function. Similar to the Sigmoid function, the Tanh function uses the truth value, but the Tanh function compresses it into the interval -1 to 1. Unlike the Sigmoid, the output of the Tanh function is centred on zero, since the interval is between -1 and 1. Function expressions: We can find that the Tanh function can be viewed as a zoomed and shifted Logistic function with a value range of (-1, 1). tanh is related to the sigmoid as follows: The image of the tanh activation function is also S-shaped. As a hyperbolic tangent function, the tanh function has a relatively similar curve to the sigmoid function. But it has some advantages over the sigmoid function. In practice, the Tanh function is used in preference to the Sigmoid function. Negative inputs are treated as negative values, zero input values are mapped close to zero, and positive inputs are treated as positive values: When the input is large or small, the output is almost smooth and has a small gradient, which is not favourable for weight updating. The difference between the two is in the output spacing, tanh has an output spacing of 1 and the whole function is centred on 0, which is better than a sigmoid function; In tanh graph, negative inputs will be strongly mapped as negative and zero inputs are mapped as close to zero. Shortcomings of tanh: Similar to the sigmoid, the tanh function suffers from vanishing gradients, and thus ‘kills’ gradients at saturation (when x is large or small). Note: In general binary classification problems, the tanh function is used in the hidden layer and the sigmoid function is used in the output layer, but this is not fixed and needs to be adapted to the particular problem. ReLU ReLU function, also known as Rectified Linear Unit (RLU), is a segmented linear function, which makes up for the gradient vanishing problem of the sigmoid function as well as the tanh function, and is widely used in current deep neural networks.The ReLU function is essentially a ramp function, and the formula and the function image are as follows: ReLU function is a popular activation function in deep learning, compared with sigmoid function and tanh function, it has the following advantages: When the input is positive, the derivative is 1, which improves the gradient vanishing problem to some extent and accelerates the convergence of gradient descent; It is much faster to compute. there are only linear relationships in the ReLU function, so it is faster to compute than sigmoid and tanh. Considered biologically plausible, e.g., unilateral inhibition, wide excitability bounds (i.e., excitability can be very high). Shortcomings of the ReLU function: Dead ReLU problem. When the input is negative, ReLU fails completely, which is not a problem during forward propagation. Some regions are sensitive and some are not. But during back propagation, if the input is negative, the gradient will be completely zero; Not zero-centred: Similar to the Sigmoid activation function, the output of the ReLU function is not zero-centred, the output of the ReLU function is zero or positive, introducing a bias offset to the neural network in the later layers can affect the efficiency of gradient descent. Leaky ReLUTo solve the problem of vanishing gradient in the ReLU activation function when x &lt; 0, we use Leaky ReLU - a function that tries to fix the dead ReLU problem. Let’s take a closer look at Leaky ReLU. The function expression and image are shown below: Why would using Leaky ReLU work better than ReLU? Leaky ReLU adjusts for the problem of negative zero gradients by giving a negative input (0.01x) to a very small linear component of x, which gives a positive gradient of 0.1 when x &lt; 0. This function alleviates the dead ReLU problem to some extent. leak helps to extend the range of the ReLU function, usually with a value of 0.01 or so; The function range of Leaky ReLU is (negative infinity to positive infinity). Although Leaky ReLU has all the characteristics of ReLU activation function (such as computationally efficient, fast convergence, not saturated in the positive region), it does not completely prove that Leaky ReLU is always better than ReLU in practice. SoftmaxSoftmax is an activation function for multi-class classification problems where class membership is required for more than two class labels. For any real vector of length K, Softmax can compress it into a real vector of length K with values in the range (0, 1) and the sum of the elements in the vector is 1. The expression of the function is as follows: Softmax is different from the normal max function: the max function only outputs the maximum value, but Softmax ensures that smaller values have a smaller probability and are not discarded outright. We can think of it as a probabilistic or “soft” version of the argmax function. The denominator of the Softmax function combines all the factors of the original output value, which means that the various probabilities obtained by the Softmax function are related to each other. Shortcomings of the Softmax activation function: It is not differentiable at zero; Negative inputs have a gradient of zero, which means that for activations in that region, the weights are not updated during backpropagation, thus creating dead neurons that never activate","link":"/2023/08/15/Activation/"},{"title":"Knowledge DistillationI","text":"1 BackgroundAlthough in general we don’t try to distinguish between the models used for training and deployment, there is a certain inconsistency between training and deployment: During training, we need to use complex models with large computational resources in order to extract information from very large, highly redundant datasets. In experiments, the models that work best tend to be very large, or even obtained by integrating multiple models. Whereas large models are not easy to deploy into services, common limitations are as follows. Slow inference speed High deployment resource requirements (memory, graphics memory, etc.) During deployment, we have strict limits on latency as well as computational resources.Therefore, model compression (reducing the number of model parameters while maintaining performance) becomes an important issue. Model distillation is one of the methods of model compression. 2 Rationale for knowledge distillation2.1 Teacher Model and Student ModelKnowledge distillation uses the Teacher-Student model, in which the teacher is the output of “knowledge” and the student is the recipient of “knowledge”. The process of knowledge distillation is divided into 2 stages. Original model training: Training the “Teacher model“, referred to as Net-T, which is characterised by a relatively complex model, and can also be integrated from multiple separately trained models. We do not impose any restrictions on the Teacher model in terms of model architecture, number of parameters, or whether it is integrated or not, the only requirement is that, for input X, it can output Y, where Y is mapped by softmax, and the output value corresponds to the probability value of the corresponding category. Streamlined model training: Train the “Student model“, abbreviated as Net-S, which is a single model with a small number of parameters and a relatively simple model structure. Similarly, for input X, it is possible to output Y, which is softmax mapped to the probability value of the corresponding category. 2.2 Key points in knowledge distillationIf we go back to the most basic theory of machine learning, we can clearly realise one thing (which is often overlooked after we delve into machine learning): the most fundamental aim of machine learning is to train models that generalise well to a given problem. Generalisation: the relationship between inputs and outputs is well represented on all data for a problem, whether it’s training data, test data, or any unknown data belonging to the problem. In reality, since it is impossible to collect all the data for a problem as training data, and new data are always being generated, we have to settle for the second best, and the training goal becomes modelling the relationship between inputs and outputs on the existing training dataset. Since the training dataset is a sampling of the real data distribution, the optimal solution on the training dataset tends to deviate more or less from the real optimal solution And when it comes to knowledge distillation, since we already have a Net-T with strong generalisation capability, we can directly let Net-S learn the generalisation capability of Net-T when we use Net-T to distill the training Net-S. A straightforward and efficient way to migrate the generalisation ability is to use the probabilities of the categories output from the softmax layer as the “soft target”. [Comparison between the KD training process and the traditional training process] Traditional training process (hard targets): find the great likelihood of ground truth. KD’s training process (soft targets): use class probabilities of large model as soft targets. Why is the KD training process more efficient?The output of the softmax layer, in addition to the positive examples, the negative labels also carry a lot of information, for example, some negative labels correspond to probabilities much larger than other negative labels. Whereas in the traditional training process (hard target), all negative labels are treated uniformly. In other words, the KD training approach makes each sample bring more information to Net-S than the traditional training approach. 2.3 The softmax functionLet’s review the original softmax function. However, if we use the output of the softmax layer as the soft target, there is another problem: when the entropy of the softmax output is relatively small, the negative labels are close to 0, and their contribution to the loss function is very small, so small that it can be ignored. Therefore, the variable “temperature“ comes in handy. The following formula shows the softmax function after adding the temperature variable. Here T is the temperature.The original softmax function is a special case of T = 1. The higher T is, the smoother the output probability distribution of softmax tends to be, the higher the entropy of its distribution is, the information carried by negative labels will be relatively amplified, and the model training will pay more attention to negative labels. 3 Specific methods of knowledge distillation3.1. Generic Approach to Knowledge Distillation The first step is to train Net-T; the second step is to distil the knowledge from Net-T to Net-S at high temperature T The process of high temperature distillation: The objective function of the distillation process is weighted by the distill loss (corresponding to the soft target) and the student loss (corresponding to the hard target). The diagram is shown above. Net-T and Net-S are input into the transfer set at the same time (here we can directly reuse the training set used to train Net-T), and the softmax distribution (with high temperature) generated by Net-T is used as the soft target, and the cross entropy of the softmax output of Net-S and the soft target under the same temperature T is the first part of the Loss function: The softmax output of Net-S for T = 1 and the cross entropy of the ground truth is the second part of the Loss function The necessity of the second part of the Loss is actually quite easy to understand: Net-T also has a certain error rate, and the use of ground truth can effectively reduce the possibility of the error being propagated to the Net-S. For example, although a teacher is far more knowledgeable than a student, he may still make mistakes, and if the student can refer to the standard answer in addition to the teacher’s teaching, it can effectively reduce the possibility of being “led astray” by the teacher’s occasional mistakes. 4 Discussion on “temperature”We all know that “distillation” needs to be carried out at high temperatures, so what does the temperature of this “distillation” represent and how is the appropriate temperature chosen? 4.1. Characteristics of temperatureBefore answering this question, it is useful to discuss the characteristics of the temperature T The original softmax function is a special case of T=1 When T&lt;1** the probability distribution is “steeper” than the original, and When **T&gt;1 the probability distribution is “flatter” than the original. The higher the temperature, the more evenly distributed the softmax values are (think about the extreme case: (i) The softmax values are uniformly distributed when T = ∞ (ii) The softmax values are equal to argmax when T-&gt;0 i.e., the value at the maximum probability tends to 1, while the other values tend to 0) Regardless of the value of the temperature T, the soft target has a tendency to ignore relatively small information that it carries 4.2. what does temperature represent and how to pick the right one?What the temperature changes is how much attention is paid to the negative labels during Net-S training: when the temperature is low, less attention is paid to the negative labels, especially those that are significantly lower than the average; while when the temperature is high, the values associated with the negative labels will be relatively larger, and Net-S will pay relatively more attention to the negative labels. In fact, negative labels contain certain information, especially those with values significantly higher than the mean. However, due to the training process of Net-T, it is decided that the negative label part is relatively noisy, and the lower the value of the negative label, the less reliable its information is. Therefore the temperature selection is more empirical, essentially a trade-off between the following two things. Learning from negative labels that are partially informative –&gt; Temperature to be higher Protect from noise in negative labels –&gt; keep temperature low Overall, the choice of T is related to the size of the Net-S. When the number of Net-S parameters is small, a relatively low temperature is sufficient (because models with a small number of parameters can’t capture all the knowledge, so they can appropriately ignore some of the negative labels).","link":"/2023/09/15/KD01/"},{"title":"Model compression-QuantizationI","text":"I. What is Quantization?Quantization is the process of constraining an input from a continuous or otherwise large set of values to a discrete set why do we need to use the activation function? Today’s AI is too BIG! Memory is Expensive Low Bit-Width Operations are Cheap II. Numeric Data Types Integer Floating-Point Number III Quantization3.1 K-Means based Weight Quantization Step 1 Weight sharing by scalar quantization First quantize the weights into k bins, then we only need to store a small index. Step 2 centroids fine-tuning During update, the gradients of same bin summed together, multiplied by the learning rate and subtracted from the shared centroids from last iteration 3.2 Linear QuantizationLinear Quantization is an affine mapping of integers to real numbers r=S(q-Z) 3.2.1 Linear Quantized Fully-Connected Layer ReLU ReLU function, also known as Rectified Linear Unit (RLU), is a segmented linear function, which makes up for the gradient vanishing problem of the sigmoid function as well as the tanh function, and is widely used in current deep neural networks.The ReLU function is essentially a ramp function, and the formula and the function image are as follows: ReLU function is a popular activation function in deep learning, compared with sigmoid function and tanh function, it has the following advantages: When the input is positive, the derivative is 1, which improves the gradient vanishing problem to some extent and accelerates the convergence of gradient descent; It is much faster to compute. there are only linear relationships in the ReLU function, so it is faster to compute than sigmoid and tanh. Considered biologically plausible, e.g., unilateral inhibition, wide excitability bounds (i.e., excitability can be very high). Shortcomings of the ReLU function: Dead ReLU problem. When the input is negative, ReLU fails completely, which is not a problem during forward propagation. Some regions are sensitive and some are not. But during back propagation, if the input is negative, the gradient will be completely zero; Not zero-centred: Similar to the Sigmoid activation function, the output of the ReLU function is not zero-centred, the output of the ReLU function is zero or positive, introducing a bias offset to the neural network in the later layers can affect the efficiency of gradient descent. Leaky ReLUTo solve the problem of vanishing gradient in the ReLU activation function when x &lt; 0, we use Leaky ReLU - a function that tries to fix the dead ReLU problem. Let’s take a closer look at Leaky ReLU. The function expression and image are shown below: Why would using Leaky ReLU work better than ReLU? Leaky ReLU adjusts for the problem of negative zero gradients by giving a negative input (0.01x) to a very small linear component of x, which gives a positive gradient of 0.1 when x &lt; 0. This function alleviates the dead ReLU problem to some extent. leak helps to extend the range of the ReLU function, usually with a value of 0.01 or so; The function range of Leaky ReLU is (negative infinity to positive infinity). Although Leaky ReLU has all the characteristics of ReLU activation function (such as computationally efficient, fast convergence, not saturated in the positive region), it does not completely prove that Leaky ReLU is always better than ReLU in practice. SoftmaxSoftmax is an activation function for multi-class classification problems where class membership is required for more than two class labels. For any real vector of length K, Softmax can compress it into a real vector of length K with values in the range (0, 1) and the sum of the elements in the vector is 1. The expression of the function is as follows: Softmax is different from the normal max function: the max function only outputs the maximum value, but Softmax ensures that smaller values have a smaller probability and are not discarded outright. We can think of it as a probabilistic or “soft” version of the argmax function. The denominator of the Softmax function combines all the factors of the original output value, which means that the various probabilities obtained by the Softmax function are related to each other. Shortcomings of the Softmax activation function: It is not differentiable at zero; Negative inputs have a gradient of zero, which means that for activations in that region, the weights are not updated during backpropagation, thus creating dead neurons that never activate 5.1 Layer and Block Block 1.它的任何子类都必须定义一个将其输入转换为输出的前向传播函数， 并且必须存储任何必需的参数。 有些块不需要任何参数。 最 为了计算梯度，块必须具有反向传播函数。 卷积（convolution）: 直观上可以想象在靠近输入的底层，一些通道专门识别边缘，而一些通道专门识别纹理。 输出大小等于输入大小�ℎ×��减去卷积核大小�ℎ×��，即： class MLP(nn.Module): # 用模型参数声明层。这里，我们声明两个全连接的层 def __init__(self): # 调用MLP的父类Module的构造函数来执行必要的初始化。 # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍） super().__init__() self.hidden = nn.Linear(20, 256) # 隐藏层 self.out = nn.Linear(256, 10) # 输出层 # 定义模型的前向传播，即如何根据输入X返回所需的模型输出 def forward(self, X): # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。 return self.out(F.relu(self.hidden(X))) # 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核 conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False) # 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度）， # 其中批量大小和通道数都为1 X = X.reshape((1, 1, 6, 8)) Y = Y.reshape((1, 1, 6, 7)) lr = 3e-2 # 学习率 for i in range(10): Y_hat = conv2d(X) l = (Y_hat - Y) ** 2 conv2d.zero_grad() l.sum().backward() # 迭代卷积核 conv2d.weight.data[:] -= lr * conv2d.weight.grad if (i + 1) % 2 == 0: print(f'epoch {i+1}, loss {l.sum():.3f}') Common Activation function 1.Gumberl softmax 通俗易懂地理解Gumbel Softmax - 知乎 (zhihu.com)","link":"/2023/06/30/Quantization01/"},{"title":"Knowledge DistillationII","text":"1 BackgroundAlthough in general we don’t try to distinguish between the models used for training and deployment, there is a certain inconsistency between training and deployment: During training, we need to use complex models with large computational resources in order to extract information from very large, highly redundant datasets. In experiments, the models that work best tend to be very large, or even obtained by integrating multiple models. Whereas large models are not easy to deploy into services, common limitations are as follows. Slow inference speed High deployment resource requirements (memory, graphics memory, etc.) During deployment, we have strict limits on latency as well as computational resources.Therefore, model compression (reducing the number of model parameters while maintaining performance) becomes an important issue. Model distillation is one of the methods of model compression. 2 Rationale for knowledge distillation2.1 Teacher Model and Student ModelKnowledge distillation uses the Teacher-Student model, in which the teacher is the output of “knowledge” and the student is the recipient of “knowledge”. The process of knowledge distillation is divided into 2 stages. Original model training: Training the “Teacher model“, referred to as Net-T, which is characterised by a relatively complex model, and can also be integrated from multiple separately trained models. We do not impose any restrictions on the Teacher model in terms of model architecture, number of parameters, or whether it is integrated or not, the only requirement is that, for input X, it can output Y, where Y is mapped by softmax, and the output value corresponds to the probability value of the corresponding category. Streamlined model training: Train the “Student model“, abbreviated as Net-S, which is a single model with a small number of parameters and a relatively simple model structure. Similarly, for input X, it is possible to output Y, which is softmax mapped to the probability value of the corresponding category. 2.2 Key points in knowledge distillationIf we go back to the most basic theory of machine learning, we can clearly realise one thing (which is often overlooked after we delve into machine learning): the most fundamental aim of machine learning is to train models that generalise well to a given problem. Generalisation: the relationship between inputs and outputs is well represented on all data for a problem, whether it’s training data, test data, or any unknown data belonging to the problem. In reality, since it is impossible to collect all the data for a problem as training data, and new data are always being generated, we have to settle for the second best, and the training goal becomes modelling the relationship between inputs and outputs on the existing training dataset. Since the training dataset is a sampling of the real data distribution, the optimal solution on the training dataset tends to deviate more or less from the real optimal solution And when it comes to knowledge distillation, since we already have a Net-T with strong generalisation capability, we can directly let Net-S learn the generalisation capability of Net-T when we use Net-T to distill the training Net-S. A straightforward and efficient way to migrate the generalisation ability is to use the probabilities of the categories output from the softmax layer as the “soft target”. [Comparison between the KD training process and the traditional training process] Traditional training process (hard targets): find the great likelihood of ground truth. KD’s training process (soft targets): use class probabilities of large model as soft targets. Why is the KD training process more efficient?The output of the softmax layer, in addition to the positive examples, the negative labels also carry a lot of information, for example, some negative labels correspond to probabilities much larger than other negative labels. Whereas in the traditional training process (hard target), all negative labels are treated uniformly. In other words, the KD training approach makes each sample bring more information to Net-S than the traditional training approach. 2.3 The softmax functionLet’s review the original softmax function. However, if we use the output of the softmax layer as the soft target, there is another problem: when the entropy of the softmax output is relatively small, the negative labels are close to 0, and their contribution to the loss function is very small, so small that it can be ignored. Therefore, the variable “temperature“ comes in handy. The following formula shows the softmax function after adding the temperature variable. Here T is the temperature.The original softmax function is a special case of T = 1. The higher T is, the smoother the output probability distribution of softmax tends to be, the higher the entropy of its distribution is, the information carried by negative labels will be relatively amplified, and the model training will pay more attention to negative labels. 3 Specific methods of knowledge distillation3.1. Generic Approach to Knowledge Distillation The first step is to train Net-T; the second step is to distil the knowledge from Net-T to Net-S at high temperature T The process of high temperature distillation: The objective function of the distillation process is weighted by the distill loss (corresponding to the soft target) and the student loss (corresponding to the hard target). The diagram is shown above. Net-T and Net-S are input into the transfer set at the same time (here we can directly reuse the training set used to train Net-T), and the softmax distribution (with high temperature) generated by Net-T is used as the soft target, and the cross entropy of the softmax output of Net-S and the soft target under the same temperature T is the first part of the Loss function: The softmax output of Net-S for T = 1 and the cross entropy of the ground truth is the second part of the Loss function The necessity of the second part of the Loss is actually quite easy to understand: Net-T also has a certain error rate, and the use of ground truth can effectively reduce the possibility of the error being propagated to the Net-S. For example, although a teacher is far more knowledgeable than a student, he may still make mistakes, and if the student can refer to the standard answer in addition to the teacher’s teaching, it can effectively reduce the possibility of being “led astray” by the teacher’s occasional mistakes. 4 Discussion on “temperature”We all know that “distillation” needs to be carried out at high temperatures, so what does the temperature of this “distillation” represent and how is the appropriate temperature chosen? 4.1. Characteristics of temperatureBefore answering this question, it is useful to discuss the characteristics of the temperature T The original softmax function is a special case of T=1 When T&lt;1** the probability distribution is “steeper” than the original, and When **T&gt;1 the probability distribution is “flatter” than the original. The higher the temperature, the more evenly distributed the softmax values are (think about the extreme case: (i) The softmax values are uniformly distributed when T = ∞ (ii) The softmax values are equal to argmax when T-&gt;0 i.e., the value at the maximum probability tends to 1, while the other values tend to 0) Regardless of the value of the temperature T, the soft target has a tendency to ignore relatively small information that it carries 4.2. what does temperature represent and how to pick the right one?What the temperature changes is how much attention is paid to the negative labels during Net-S training: when the temperature is low, less attention is paid to the negative labels, especially those that are significantly lower than the average; while when the temperature is high, the values associated with the negative labels will be relatively larger, and Net-S will pay relatively more attention to the negative labels. In fact, negative labels contain certain information, especially those with values significantly higher than the mean. However, due to the training process of Net-T, it is decided that the negative label part is relatively noisy, and the lower the value of the negative label, the less reliable its information is. Therefore the temperature selection is more empirical, essentially a trade-off between the following two things. Learning from negative labels that are partially informative –&gt; Temperature to be higher Protect from noise in negative labels –&gt; keep temperature low Overall, the choice of T is related to the size of the Net-S. When the number of Net-S parameters is small, a relatively low temperature is sufficient (because models with a small number of parameters can’t capture all the knowledge, so they can appropriately ignore some of the negative labels).","link":"/2023/09/30/KD02/"}],"tags":[{"name":"BackTracking","slug":"BackTracking","link":"/tags/BackTracking/"},{"name":"Combination","slug":"Combination","link":"/tags/Combination/"},{"name":"Resursion","slug":"Resursion","link":"/tags/Resursion/"},{"name":"C&#x2F;C++","slug":"C-C","link":"/tags/C-C/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"LinkedList","slug":"LinkedList","link":"/tags/LinkedList/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"BinaryTree","slug":"BinaryTree","link":"/tags/BinaryTree/"},{"name":"Array","slug":"Array","link":"/tags/Array/"},{"name":"loop invariant","slug":"loop-invariant","link":"/tags/loop-invariant/"},{"name":"BinarySearchTree","slug":"BinarySearchTree","link":"/tags/BinarySearchTree/"},{"name":"Iteration","slug":"Iteration","link":"/tags/Iteration/"},{"name":"Pruning","slug":"Pruning","link":"/tags/Pruning/"},{"name":"TraversalOfBinaryTree","slug":"TraversalOfBinaryTree","link":"/tags/TraversalOfBinaryTree/"},{"name":"Linkedlist","slug":"Linkedlist","link":"/tags/Linkedlist/"},{"name":"String","slug":"String","link":"/tags/String/"},{"name":"DoublePointers","slug":"DoublePointers","link":"/tags/DoublePointers/"},{"name":"GreedyAlgorithm","slug":"GreedyAlgorithm","link":"/tags/GreedyAlgorithm/"},{"name":"VideoCompression","slug":"VideoCompression","link":"/tags/VideoCompression/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"modelcompression","slug":"modelcompression","link":"/tags/modelcompression/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"DynamicProgramming","slug":"DynamicProgramming","link":"/tags/DynamicProgramming/"},{"name":"machinelearning","slug":"machinelearning","link":"/tags/machinelearning/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"NaiveBayes","slug":"NaiveBayes","link":"/tags/NaiveBayes/"},{"name":"FeatureExtraction","slug":"FeatureExtraction","link":"/tags/FeatureExtraction/"},{"name":"FeaturePreprocessing","slug":"FeaturePreprocessing","link":"/tags/FeaturePreprocessing/"},{"name":"KNN","slug":"KNN","link":"/tags/KNN/"},{"name":"CrossValidation","slug":"CrossValidation","link":"/tags/CrossValidation/"},{"name":"DecisionTree","slug":"DecisionTree","link":"/tags/DecisionTree/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"LinearRegression","slug":"LinearRegression","link":"/tags/LinearRegression/"},{"name":"RidgeRegression","slug":"RidgeRegression","link":"/tags/RidgeRegression/"},{"name":"deeplearning","slug":"deeplearning","link":"/tags/deeplearning/"},{"name":"KnowledgeDistillation","slug":"KnowledgeDistillation","link":"/tags/KnowledgeDistillation/"},{"name":"quantization","slug":"quantization","link":"/tags/quantization/"},{"name":"model compression","slug":"model-compression","link":"/tags/model-compression/"}],"categories":[{"name":"Algorithm","slug":"Algorithm","link":"/categories/Algorithm/"},{"name":"Video Compression","slug":"Video-Compression","link":"/categories/Video-Compression/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"Machine learning","slug":"Machine-learning","link":"/categories/Machine-learning/"}],"pages":[{"title":"Welcome to Jasmine&#39;blog","text":"This is my personal website and I will share relevant notes of Computer programming and Math.","link":"/About/index.html"}]}