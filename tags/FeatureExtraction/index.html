<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: FeatureExtraction - Jasmine</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jasmine"><meta name="msapplication-TileImage" content="/img/avatar.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jasmine"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Jasmine"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Jasmine"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="John Doe"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Jasmine","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"Jasmine","logo":{"@type":"ImageObject","url":{"text":"Tianhao'Site"}}},"description":""}</script><link rel="icon" href="/img/avatar.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="jasmine"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="jasmine"></script><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Tianhao&#039;Site</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">FeatureExtraction</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-30T23:00:00.000Z" title="31/03/2023, 00:00:00">2023-03-31</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-learning/">Machine learning</a></span><span class="level-item">8 minutes read (About 1165 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/31/MachineLearning02/">Intro of Machine Learning II</a></h1><div class="content"><h3 id="1-4-Feature-Preprocessing"><a href="#1-4-Feature-Preprocessing" class="headerlink" title="1.4 Feature Preprocessing"></a>1.4 Feature Preprocessing</h3><p>:  Understand numerical data, categorical data characteristics Apply <strong>MinMaxScaler</strong> and <strong>StandardScaler</strong> to implement <strong>Normalisation</strong> of feature data</p>
<p>Feature preprocessing:  the process of <strong>converting feature data into feature data</strong> more suitable for the algorithmic model by means of some transformation functions.</p>
<p><img src="/2023/03/31/MachineLearning02/img1.png"></p>
<h4 id="Why-do-we-need-to-normalise-x2F-standardise"><a href="#Why-do-we-need-to-normalise-x2F-standardise" class="headerlink" title="Why do we need to normalise&#x2F;standardise?"></a>Why do we need to normalise&#x2F;standardise?</h4><p>Large differences in the units or sizes of features, or a feature whose variance is orders of magnitude larger than other features, can easily <strong>influence (dominate) the target result</strong>, preventing some algorithms from learning other features.</p>
<h4 id="1-41-Normalisation-Maps-the-data-by-transforming-the-original-data-between-default-0-1"><a href="#1-41-Normalisation-Maps-the-data-by-transforming-the-original-data-between-default-0-1" class="headerlink" title="1.41 Normalisation: Maps the data by transforming the original data between (default [0,1])"></a>1.41 Normalisation: Maps the data by transforming the original data between (default [0,1])</h4><p><img src="/2023/03/31/MachineLearning02/img2.png"></p>
<p><strong>sklearn.preprocessing.MinMaxScaler (feature_range&#x3D;(0,1)…)</strong></p>
<ul>
<li><p><strong>MinMaxScalar.fit_transform(X)</strong></p>
<p>​      X: numpy array format data[n_samples,n_features]</p>
</li>
<li><p><strong>return value</strong>: transformed array of the same shape</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">minmax_demo</span>():</span><br><span class="line">    data = pd.read_csv(<span class="string">&quot;dating.txt&quot;</span>)</span><br><span class="line">    data = data.iloc[:,:<span class="number">3</span>]  <span class="comment">#只取数据前三列</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data=\n&quot;</span>,data)</span><br><span class="line">    transfer = MinMaxScaler() <span class="comment">#默认0-1</span></span><br><span class="line">    data_new = transfer.fit_transform(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_new=\n&quot;</span>,data_new)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    minmax_demo()</span><br></pre></td></tr></table></figure>



<p><img src="/2023/03/31/MachineLearning02/img3.png"></p>
<p>Note that the maximum and minimum values are changing, in addition, the maximum and minimum values are very <strong>susceptible</strong> to anomalies so this method is <strong>less robust</strong> and only suitable for traditional accurate small data scenarios</p>
<h4 id="1-4-2-Standardisation-Transform-the-data-to-a-mean-of-0-and-standard-deviation-of-1-by-transforming-the-original-data"><a href="#1-4-2-Standardisation-Transform-the-data-to-a-mean-of-0-and-standard-deviation-of-1-by-transforming-the-original-data" class="headerlink" title="1.4.2 Standardisation:Transform the data to a mean of 0 and standard deviation of 1 by transforming the original data."></a>1.4.2 Standardisation:Transform the data to a mean of 0 and standard deviation of 1 by transforming the original data.</h4><p>For <strong>normalisation</strong>: If outliers occur that affects the maximum and minimum values, then the result will obviously change<br>For standarlisation:If outliers occur, due to the fact that there is a With a fixed amount of data, a small number of anomalies <strong>do not have a significant effect on the mean</strong>, and thus the variance changes less.</p>
<p><img src="/2023/03/31/MachineLearning02/img4.png"></p>
<ul>
<li><strong>sklearn.preprocessing.StandardScaler()</strong><br>After processing, for each column, all data are clustered around a mean of 0 and a standard deviation of 1</li>
<li><strong>StandardScaler.fit transform(X)</strong><br>X: numpyarray format data In amples,n_features]</li>
<li><strong>return value</strong>:transformed shape of the same array</li>
</ul>
<h3 id="1-5-Feature-Reduction"><a href="#1-5-Feature-Reduction" class="headerlink" title="1.5 Feature Reduction"></a>1.5 Feature Reduction</h3><h4 id="1-5-1-Dimensionality-reduction"><a href="#1-5-1-Dimensionality-reduction" class="headerlink" title="1.5.1 Dimensionality reduction"></a>1.5.1 Dimensionality reduction</h4><p><strong>Dimensionality reduction</strong> is the process of reducing the number of random variables (features) to obtain a set of “<strong>uncorrelated</strong>“ principal variables, subject to certain constraints.</p>
<ul>
<li><strong>Reduce the number of random variables</strong></li>
</ul>
<p><img src="/2023/03/31/MachineLearning02/img5.png"></p>
<ul>
<li><strong>Correlated Feature</strong></li>
</ul>
<p>eg: Correlation between relative humidity and rainfall</p>
<p><u><em>It is because we are using features for learning when we are doing training. If there is a problem with the features themselves or if there is a strong correlation between the features, it will have a greater impact on the algorithm’s ability to learn the predictions</em></u></p>
<h4 id="1-5-2-Two-approaches-to-dimensionality-reduction"><a href="#1-5-2-Two-approaches-to-dimensionality-reduction" class="headerlink" title="1.5.2  Two approaches to dimensionality reduction"></a>1.5.2  Two approaches to dimensionality reduction</h4><ul>
<li><strong>Feature selection</strong></li>
<li><strong>Principal Component Analysis</strong> (can be understood as a form of feature extraction)</li>
</ul>
<h4 id="1-5-3-What-is-feature-selection"><a href="#1-5-3-What-is-feature-selection" class="headerlink" title="1.5.3 What is feature selection"></a>1.5.3 What is feature selection</h4><ol>
<li><strong>Definition</strong>: Data containing redundant or correlated variables (or features, attributes, metrics, etc.) designed to <strong>identify the main features</strong> from the original ones.</li>
<li><strong>Method:</strong></li>
</ol>
<ul>
<li><p><strong>Filter</strong>:  focuses on exploring the characteristics of the feature itself, the association between the feature and the feature and the target value.</p>
<ul>
<li><strong>Variance Selection</strong>: low variance feature filtering. Variance selection method: low variance feature filtering.</li>
<li><strong>Correlation Coefficient</strong></li>
</ul>
</li>
<li><p><strong>Embedded</strong>: algorithm automatically selects features (correlation between features and target value).</p>
<ul>
<li><p><strong>Decision Tree</strong>:Information Entropy, Information Gain9</p>
</li>
<li><p><strong>Regularisation</strong>:L1, L2</p>
</li>
<li><p><strong>Deep learning</strong>: convolution, etc.</p>
</li>
</ul>
</li>
</ul>
<p><strong>3. Filter</strong></p>
<p><strong>3.1</strong> <strong>Low variance feature filtering</strong><br>Remove some features with <strong>low variance</strong>, the significance of variance was covered earlier. Combined with the size of the variance to consider the perspective of this approach:</p>
<ul>
<li><strong>feature variance is small</strong>: the value of most samples of a feature is relatively similar </li>
<li><strong>feature variance is large</strong>: the value of many samples of a feature are different</li>
</ul>
<p><strong>3.1.1</strong><br><strong>sklearn.feature_selection.VarianceThreshold(threshold &#x3D; 0.0)</strong></p>
<ul>
<li>Remove all low variance features</li>
<li><strong>Variance.fit_transform(X)</strong><ul>
<li>X:numpy array format data [n_samplesn_features]</li>
<li>Return value:Features with training set variance below threshold are removed. The default value is to keep all non-zero variance features, i.e., remove all features with the same value in all samples.<br>4.1.2 Data computation<br>We perform a filter between the indicator features of certain stocks, the data is in the file “factor_regression_data&#x2F;factor_returns.csv” excluding the indexdate’return columns (these types do not match and are not required indicators).<br>These are the characteristics in total</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">variance_demo</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Low variance feature filtering</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1、load data</span></span><br><span class="line"></span><br><span class="line">​    data = pd.read_csv(<span class="string">&quot;factor_returns.csv&quot;</span>)</span><br><span class="line">​    data = data.iloc[:, <span class="number">1</span>:-<span class="number">2</span>]</span><br><span class="line">​    <span class="built_in">print</span>(<span class="string">&quot;data:\n&quot;</span>, data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、initialise transer()</span></span><br><span class="line"></span><br><span class="line">transfer = VarianceThreshold(threshold=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、use fit_transform</span></span><br><span class="line"></span><br><span class="line">data_new = transfer.fit_transform(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;data_new:\n&quot;</span>, data_new, data_new.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">r1 = pearsonr(data[<span class="string">&quot;pe_ratio&quot;</span>], data[<span class="string">&quot;pb_ratio&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;correlation coefficients：\n&quot;</span>, r1)</span><br><span class="line">r2 = pearsonr(data[<span class="string">&#x27;revenue&#x27;</span>], data[<span class="string">&#x27;total_expense&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;the correlation between revenue and total_expense：\n&quot;</span>, r2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p><strong>3.2</strong> <strong>Correlation coefficients</strong></p>
<p><img src="/2023/03/31/MachineLearning02/img6.png"></p>
<p><strong>3.21 Characteristics</strong><br>The value of correlation coefficient lies between -1 and +1 i.e. <strong>-1&lt;&#x3D;r&lt;&#x3D;1</strong>. The properties are as follows:</p>
<ul>
<li><p>When <strong>r&gt;0</strong>, it means that the two variables are positively planted off, <strong>r&lt;0</strong>, the two variables are negatively correlated </p>
</li>
<li><p>When <strong>|r|&#x3D;1</strong>, it means that the two variables are perfectly correlated, when <strong>r&#x3D;0</strong>, it means that there is no correlation between the two variables </p>
</li>
<li><p>When <strong>0&lt;|r|&lt;1</strong>, it means that there is a certain degree of correlation between the two variables. The closer r is to 1, the closer the linear relationship between the two variables; the closer r is to 0, the weaker the linear correlation between the two variables.</p>
</li>
<li><p>Generally can be divided into <strong>three levels</strong>: <strong>r &lt; 0.4</strong> for low degree of correlation; <strong>0.4&lt;|rl &lt; 0.7</strong> for significant correlation; <strong>0.7&lt;&#x3D;|r| &lt; 1</strong> for high degree of linear correlation</p>
<figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from scipy.stats import pearsonr</span><br><span class="line">x <span class="symbol">:</span>(<span class="built_in">N</span>,)array_like</span><br><span class="line">y <span class="symbol">:</span>(<span class="built_in">N</span>,) array_like Retur<span class="symbol">ns:</span> (<span class="built_in">Pearson</span>&#x27;s correlation coefficient, p-<span class="built_in">value</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/31/MachineLearning02/img7.png"></p>
</li>
</ul>
<h3 id="1-6-PCA"><a href="#1-6-PCA" class="headerlink" title="1.6 PCA"></a>1.6 PCA</h3><h4 id="1-6-1-What-is-Principal-Component-Analysis-PCA"><a href="#1-6-1-What-is-Principal-Component-Analysis-PCA" class="headerlink" title="1.6.1 What is Principal Component Analysis (PCA)?"></a>1.6.1 What is Principal Component Analysis (PCA)?</h4><p><strong>Definition</strong>:The process of transforming high-dimensional data into low-dimensional data, in which the original data may be discarded and new variables may be created.<br><strong>Role</strong>: is the dimensionality of the data compression, as far as possible to reduce the dimensionality of the original data (complexity), the loss of a small amount of information.<br><strong>Application</strong>: regression analysis or cluster analysis.</p>
<p>​                        Given:</p>
<img src="/2023/03/31/MachineLearning02/img8.png" style="zoom:67%;">

<p><strong>Now place the data in a two-dimensional spatial Cartesian coordinate system</strong></p>
<img src="/2023/03/31/MachineLearning02/img9.png" style="zoom:67%;">

<p><strong>Now let’s figure out how to reduce the two-dimensional data to one dimension (a straight line)</strong></p>
<img src="/2023/03/31/MachineLearning02/img10.png" style="zoom:67%;">

<p> <strong>We can see that although it is reduced to one dimension, there is a loss of data from the original five points to three, and so:</strong></p>
<img src="/2023/03/31/MachineLearning02/img11.png" style="zoom:67%;">

<p><strong>sklearn.decomposition.PCA(n_components&#x3D;None</strong></p>
<ul>
<li><strong>Decompose data into lower dimensional spaces</strong></li>
<li><strong>n_components</strong></li>
</ul>
<p>​       Fractional: Indicates what percent of the information is retained</p>
<p>​     Integer: Reduces to how many features. </p>
<ul>
<li>**Pca.fit_transform(X)**X:numpy array format data [n_samples,n_features]</li>
<li><strong>Return value</strong>: array of the specified dimension after transformation</li>
</ul>
<p><strong>For example:</strong></p>
<img src="/2023/03/31/MachineLearning02/img12.png" style="zoom:67%;">

<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition  import PCA</span><br><span class="line"></span><br><span class="line">def pca_demo():</span><br><span class="line">    data = <span class="string">[[2,8,4,5],[6,3,0,8],[5,4,9,1]]</span></span><br><span class="line">    transfer = PCA(n_components=<span class="number">2</span>) #reduce to <span class="number">2</span> features</span><br><span class="line">    data_new = transfer.fit_transform(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_new:\n&quot;</span>,data_new)</span><br><span class="line">    <span class="keyword">return</span> None</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    pca_demo()</span><br></pre></td></tr></table></figure>

<p><img src="/2023/03/31/MachineLearning02/img13.png"></p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">32</span></span></a></li><li><a class="level is-mobile" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Machine-learning/"><span class="level-start"><span class="level-item">Machine learning</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/Video-Compression/"><span class="level-start"><span class="level-item">Video Compression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-29T23:00:00.000Z">2023-09-30</time></p><p class="title"><a href="/2023/09/30/KD02/">Knowledge DistillationII</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-14T23:00:00.000Z">2023-09-15</time></p><p class="title"><a href="/2023/09/15/KD01/">Knowledge DistillationI</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-29T23:00:00.000Z">2023-08-30</time></p><p class="title"><a href="/2023/08/30/ModelCompressionPruning/">Model Compression - Pruning</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-14T23:00:00.000Z">2023-08-15</time></p><p class="title"><a href="/2023/08/15/Activation/">Activation Function</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-29T23:00:00.000Z">2023-07-30</time></p><p class="title"><a href="/2023/07/30/BasicsofNeuralNetworksII/">Basics of Neural NetworksII</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Array/"><span class="tag">Array</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BackTracking/"><span class="tag">BackTracking</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BinarySearchTree/"><span class="tag">BinarySearchTree</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BinaryTree/"><span class="tag">BinaryTree</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C-C/"><span class="tag">C/C++</span><span class="tag">32</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Combination/"><span class="tag">Combination</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CrossValidation/"><span class="tag">CrossValidation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DecisionTree/"><span class="tag">DecisionTree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepLearning/"><span class="tag">DeepLearning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DoublePointers/"><span class="tag">DoublePointers</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DynamicProgramming/"><span class="tag">DynamicProgramming</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeatureExtraction/"><span class="tag">FeatureExtraction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeaturePreprocessing/"><span class="tag">FeaturePreprocessing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GreedyAlgorithm/"><span class="tag">GreedyAlgorithm</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Iteration/"><span class="tag">Iteration</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KNN/"><span class="tag">KNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KnowledgeDistillation/"><span class="tag">KnowledgeDistillation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinearRegression/"><span class="tag">LinearRegression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinkedList/"><span class="tag">LinkedList</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linkedlist/"><span class="tag">Linkedlist</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NaiveBayes/"><span class="tag">NaiveBayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pruning/"><span class="tag">Pruning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Resursion/"><span class="tag">Resursion</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RidgeRegression/"><span class="tag">RidgeRegression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/String/"><span class="tag">String</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TraversalOfBinaryTree/"><span class="tag">TraversalOfBinaryTree</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VideoCompression/"><span class="tag">VideoCompression</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deeplearning/"><span class="tag">deeplearning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/loop-invariant/"><span class="tag">loop invariant</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machinelearning/"><span class="tag">machinelearning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/model-compression/"><span class="tag">model compression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/modelcompression/"><span class="tag">modelcompression</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quantization/"><span class="tag">quantization</span><span class="tag">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="Tianhao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Tianhao</p><p class="is-size-6 is-block">a MATH and CS student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Bristol, United Kingdom</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">46</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/jasminepp/jasminepp.github.io" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/jasminepp/jasminepp.github.io"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">September 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Tianhao&#039;Site</a><p class="is-size-7"><span>&copy; 2023 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="jasmine"></script><script src="jasmine"></script><script src="jasmine" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="jasmine" defer></script><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>