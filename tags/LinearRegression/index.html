<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: LinearRegression - JasminePP</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="JasminePP"><meta name="msapplication-TileImage" content="/img/avatar.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="JasminePP"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="JasminePP"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="JasminePP"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Tianhao Peng"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"JasminePP","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Tianhao Peng"},"publisher":{"@type":"Organization","name":"JasminePP","logo":{"@type":"ImageObject","url":{"text":"Jasmine'Site"}}},"description":""}</script><link rel="icon" href="/img/avatar.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="jasmine"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="jasmine"></script><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Jasmine&#039;Site</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">LinearRegression</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-05-14T23:00:00.000Z" title="15/05/2023, 00:00:00">2023-05-15</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-learning/">Machine learning</a></span><span class="level-item">7 minutes read (About 1121 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/05/15/MachineLearning05/">Machine Learning - RegressionI</a></h1><div class="content"><h3 id="1-1-Linear-Regression"><a href="#1-1-Linear-Regression" class="headerlink" title="1.1 Linear Regression"></a>1.1 Linear Regression</h3><h4 id="1-1-1-Linear-Regression-Application-Scenarios"><a href="#1-1-1-Linear-Regression-Application-Scenarios" class="headerlink" title="1.1.1  Linear Regression Application Scenarios"></a>1.1.1  Linear Regression Application Scenarios</h4><ul>
<li>House price prediction</li>
<li>Sales forecasting</li>
<li>Finance: Loan amount prediction, using linear regression and coefficient analysis factors</li>
</ul>
<p>​       <img src="/2023/05/15/MachineLearning05/img1.png"></p>
<h4 id="1-1-2-What-is-Linear-Regression"><a href="#1-1-2-What-is-Linear-Regression" class="headerlink" title="1.1.2 What is Linear Regression?"></a>1.1.2 What is Linear Regression?</h4><ol>
<li><h5 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h5></li>
</ol>
<p><strong>Linear regression</strong> is a type of analysis that uses <strong>regression equations</strong> (functions) to model the relationship between one or more independent variables (<strong>eigenvalues</strong>) and the dependent variable (<strong>target value</strong>).</p>
<ul>
<li>The case of only one independent variable is called <strong>univariate regression</strong>, and the case of more than one independent variable is called <strong>multivariate regression</strong></li>
</ul>
<p>​      <img src="/2023/05/15/MachineLearning05/img2.png"></p>
<p>So how do you understand it? Let’s look at a few <strong>examples</strong></p>
<ol>
<li><p>Final grade: 0.7x exam grade + 0.3x usual grade</p>
</li>
<li><p>House price &#x3D; 0.02x distance from centre +0.04x urban nitrous oxide concentration +(-0.12x average house price for owner-occupied housing) +0.254x urban crime rate</p>
<p>In the two examples above, we see that a relationship has been established between the eigenvalues and the target values, and this relationship can be understood as a linear model.</p>
</li>
<li><p><strong>Analysis of the relationship between the characteristics and the target in linear regression</strong><br>There are two kinds of linear models among linear regression, one is a <strong>linear relationship</strong> and the other is a <strong>non-linear relationship</strong>. Here we can only draw a plane to better understand, so all use a single feature or two features as an example.</p>
</li>
</ol>
<p><img src="/2023/05/15/MachineLearning05/img3.png"></p>
<h4 id="1-1-3-Principles-of-loss-and-optimisation-in-linear-regression"><a href="#1-1-3-Principles-of-loss-and-optimisation-in-linear-regression" class="headerlink" title="1.1.3 Principles of loss and optimisation in linear regression"></a>1.1.3 Principles of loss and optimisation in linear regression</h4><p>Assuming the house example just given, this relationship exists between the real data</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Real</span> relationship: real house price = <span class="number">0</span>.<span class="number">02</span>x distance from centre + <span class="number">0</span>。<span class="number">04</span>x urban nitrous oxide concentration + (-<span class="number">0</span>.<span class="number">12</span>x average house price for owner-occupied housing) +<span class="number">0</span>.<span class="number">254</span>x urban crime rate</span><br></pre></td></tr></table></figure>

<p>So now, let’s randomly specify a relationship (guess)</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Randomly</span> specify the relationship: predicted house price = <span class="number">0</span>.<span class="number">25</span>x distance from the centre + <span class="number">0</span>.<span class="number">14</span>x urban nitric oxide concentration + <span class="number">0</span>.<span class="number">42</span>x average house price for owner-occupied housing) +<span class="number">0</span>.<span class="number">36</span>x urban crime rate</span><br></pre></td></tr></table></figure>

<p> Isn’t there some <strong>error</strong> between the real result and our prediction? </p>
<p><u><em><strong>The goal is to find a line that minimises the sum of the distances from all points to the line, i.e. minimises the error</strong></em></u></p>
<h4 id="1-Loss-function"><a href="#1-Loss-function" class="headerlink" title="1. Loss function"></a><strong>1. Loss function</strong></h4><p>Define the <strong>total loss</strong>:</p>
<p><img src="/2023/05/15/MachineLearning05/img4.png"><br>​    - <em><strong>y_i</strong></em> is the true value of the ith training sample<br>-*** h(x_i)*** is the eigenvalue combination prediction function for the ith training sample</p>
<p>-Also known as <strong>least squares</strong></p>
<p>How do we go about reducing this loss so that we can predict a little more accurately? Since this loss exists, we have always said that machine learning has the function of <strong>automatic learning</strong>, which can be reflected in linear regression. Here you can optimise the total loss of the regression by using some optimisation methods (actually a <strong>derivative function</strong> in mathematics)!!</p>
<h4 id="2-Optimisation"><a href="#2-Optimisation" class="headerlink" title="2. Optimisation"></a>2. Optimisation</h4><ul>
<li><strong>Normal Equation</strong></li>
</ul>
<p><img src="/2023/05/15/MachineLearning05/img5.png"></p>
<p><strong>Understanding</strong>: X is the matrix of eigenvalues and y is the matrix of target values. Directly find the best result<br><strong>Disadvantage</strong>: When the features are too many and complex, the solution is too slow and no results are obtained.</p>
<ul>
<li><strong>Grandient Descent</strong></li>
</ul>
<p><img src="/2023/05/15/MachineLearning05/img6.png"></p>
<p><img src="/2023/05/15/MachineLearning05/img7.jpg"></p>
<p><img src="/2023/05/15/MachineLearning05/img8.png"></p>
<p><strong>Understanding</strong>: <em><strong>a</strong></em> is the learning rate, need to be specified manually (hyperparameters), a next to the overall representation of the direction of the function along the direction of the decline to find, and finally you can find the <strong>lowest point</strong> of the valley, and then update the value of the W </p>
<p><strong>Use</strong>: To find better results for tasks with very large training data sizes.</p>
<h4 id="1-1-4-Linear-Regression-Application"><a href="#1-1-4-Linear-Regression-Application" class="headerlink" title="1.1.4 Linear Regression Application"></a>1.1.4 Linear Regression Application</h4><ul>
<li><strong>sklearn.linear_model.LinearRegression(fit_intercept&#x3D;True)</strong></li>
</ul>
<p>Optimisation by regular equations</p>
<p><strong>fit_intercept</strong>: whether to compute bias</p>
<p><strong>LinearRegression.coef</strong>: regression coefficients</p>
<p> <strong>LinearRegression.intercept</strong>: bias</p>
<ul>
<li><p><strong>sklearn.linear_model.SGDRegressor(loss&#x3D;”squared_loss”, fit_intercept&#x3D;Truelearning_rate &#x3D;’invscaling’, eta0&#x3D;0.01)</strong></p>
</li>
<li><p><strong>SGDRegressor class</strong> implements <em>stochastic gradient descent learning</em>, which supports different loss functions and regularization penalty terms to fit linear regression models.</p>
</li>
<li><p><strong>loss</strong>:loss type </p>
</li>
<li><p>loss&#x3D;”squared loss”normal least squares <strong>fit_intercept</strong>: whether to compute bias</p>
</li>
<li><p><strong>learning_rate</strong> : string, optional</p>
</li>
</ul>
<ol>
<li><p>Learning rate padding<br>‘constant’: eta &#x3D; eta0<br>‘optimal’: eta &#x3D; 1.0 &#x2F; (alpha * (t + t0)) </p>
</li>
<li><p>‘invscaling’: eta &#x3D; eta0 &#x2F; pow(t, power_t)</p>
<p>power_t&#x3D;0.25: present in <strong>parent class</strong></p>
</li>
<li><p>For a constant learning rate, you can use learning_rate&#x3D;’constant and use eta0 to specify the learning rate<br><strong>SGDRegressor.coef</strong>: regression coefficient 0<br><strong>SGDRegressorintercept_</strong>: bias</p>
</li>
</ol>
<h4 id="1-1-5-Boston-Home-Price-Forecast"><a href="#1-1-5-Boston-Home-Price-Forecast" class="headerlink" title="1.1.5 Boston Home Price Forecast"></a>1.1.5 Boston Home Price Forecast</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear1</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">A normal equations optimisation approach to forecasting house prices in Boston</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 1）Acquisition of data</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2）dataset division</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3）Normalisation</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4）estimator</span></span><br><span class="line">    estimator = LinearRegression()</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5）result</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The normal equation-weights are：\n&quot;</span>, estimator.coef_)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The normal equation-bias are：\n&quot;</span>, estimator.intercept_)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6）model evaluation</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predicted house price：\n&quot;</span>, y_predict)</span><br><span class="line">    error = mean_squared_error(y_test, y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;The normal equation-mean square error is：\n&quot;</span>, error)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear2</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Gradient descent optimisation approach to forecasting house prices in Boston</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 1）Acquisition of data</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;number of features：\n&quot;</span>, boston.data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2）dataset division</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3）Normalisation</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4）estimator</span></span><br><span class="line">    estimator = SGDRegressor(learning_rate=<span class="string">&quot;constant&quot;</span>, eta0=<span class="number">0.01</span>, max_iter=<span class="number">10000</span>, penalty=<span class="string">&quot;l1&quot;</span>)</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5）result</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Grandient Descent-weights：\n&quot;</span>, estimator.coef_)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Grandient Descent-bias：\n&quot;</span>, estimator.intercept_)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6）model evaluation</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;predicted house price：\n&quot;</span>, y_predict)</span><br><span class="line">    error = mean_squared_error(y_test, y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Grandient Descent-mean square error is：\n&quot;</span>, error)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    linear1()</span><br><span class="line">    linear2()</span><br></pre></td></tr></table></figure>

<h4 id="1-1-6-About-the-optimisation-methods-GD-SGD-SAG"><a href="#1-1-6-About-the-optimisation-methods-GD-SGD-SAG" class="headerlink" title="1.1.6 About the optimisation methods GD, SGD, SAG"></a>1.1.6 About the optimisation methods GD, SGD, SAG</h4><h5 id="1GD-Gradient-Descent"><a href="#1GD-Gradient-Descent" class="headerlink" title="1GD- Gradient Descent"></a>1GD- Gradient Descent</h5><p>the original gradient descent method needs to calculate the value of all samples to be able to derive the gradient, <strong>the amount of computation is large</strong>, so there will be a series of improvements later.</p>
<h5 id="2-SGD-Stochastic-gradient-descent"><a href="#2-SGD-Stochastic-gradient-descent" class="headerlink" title="2 SGD- Stochastic gradient descent"></a>2 SGD- Stochastic gradient descent</h5><p>It considers only one training sample in an iteration.<br><strong>Advantages:</strong><br>-Efficient<br>-Easy to implement<br><strong>Disadvantages:</strong><br>-SGD requires many hyperparameters: e.g., regular term parameters, number of iterations.<br>-SGD is sensitive to feature normalisation.</p>
<h5 id="3-SAG-Stochastic-Average-Gradient"><a href="#3-SAG-Stochastic-Average-Gradient" class="headerlink" title="3 SAG-Stochastic Average Gradient"></a>3 SAG-Stochastic Average Gradient</h5><p>due to the slow rate of convergence, gradient descent based algorithms such as SAG have been proposed.<br>Scikit-learn: ridge regression, logistic regression, etc. will have SAG optimisation</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">32</span></span></a></li><li><a class="level is-mobile" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Machine-learning/"><span class="level-start"><span class="level-item">Machine learning</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/Video-Compression/"><span class="level-start"><span class="level-item">Video Compression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-29T23:00:00.000Z">2023-09-30</time></p><p class="title"><a href="/2023/09/30/KD02/">Knowledge DistillationII</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-09-14T23:00:00.000Z">2023-09-15</time></p><p class="title"><a href="/2023/09/15/KD01/">Knowledge DistillationI</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-29T23:00:00.000Z">2023-08-30</time></p><p class="title"><a href="/2023/08/30/ModelCompressionPruning/">Model Compression - Pruning</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-14T23:00:00.000Z">2023-08-15</time></p><p class="title"><a href="/2023/08/15/Activation/">Activation Function</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-29T23:00:00.000Z">2023-07-30</time></p><p class="title"><a href="/2023/07/30/BasicsofNeuralNetworksII/">Basics of Neural NetworksII</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Array/"><span class="tag">Array</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BackTracking/"><span class="tag">BackTracking</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BinarySearchTree/"><span class="tag">BinarySearchTree</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BinaryTree/"><span class="tag">BinaryTree</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C-C/"><span class="tag">C/C++</span><span class="tag">32</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Combination/"><span class="tag">Combination</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CrossValidation/"><span class="tag">CrossValidation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DecisionTree/"><span class="tag">DecisionTree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepLearning/"><span class="tag">DeepLearning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DoublePointers/"><span class="tag">DoublePointers</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DynamicProgramming/"><span class="tag">DynamicProgramming</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeatureExtraction/"><span class="tag">FeatureExtraction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeaturePreprocessing/"><span class="tag">FeaturePreprocessing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GreedyAlgorithm/"><span class="tag">GreedyAlgorithm</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Iteration/"><span class="tag">Iteration</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KNN/"><span class="tag">KNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KnowledgeDistillation/"><span class="tag">KnowledgeDistillation</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinearRegression/"><span class="tag">LinearRegression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinkedList/"><span class="tag">LinkedList</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linkedlist/"><span class="tag">Linkedlist</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NaiveBayes/"><span class="tag">NaiveBayes</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pruning/"><span class="tag">Pruning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Resursion/"><span class="tag">Resursion</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RidgeRegression/"><span class="tag">RidgeRegression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/String/"><span class="tag">String</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TraversalOfBinaryTree/"><span class="tag">TraversalOfBinaryTree</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VideoCompression/"><span class="tag">VideoCompression</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deeplearning/"><span class="tag">deeplearning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/loop-invariant/"><span class="tag">loop invariant</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machinelearning/"><span class="tag">machinelearning</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/model-compression/"><span class="tag">model compression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/modelcompression/"><span class="tag">modelcompression</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quantization/"><span class="tag">quantization</span><span class="tag">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="Tianhao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Tianhao</p><p class="is-size-6 is-block">a MATH and CS student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Bristol, United Kingdom</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">46</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/jasminepp/jasminepp.github.io" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/jasminepp/jasminepp.github.io"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">September 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Jasmine&#039;Site</a><p class="is-size-7"><span>&copy; 2023 Tianhao Peng</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="jasmine"></script><script src="jasmine"></script><script src="jasmine" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="jasmine" defer></script><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>