<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: Classification - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hexo"><meta name="msapplication-TileImage" content="/img/avatar.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hexo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Hexo"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Hexo"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="John Doe"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Hexo","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":{"text":"Tianhao'Site"}}},"description":""}</script><link rel="icon" href="/img/avatar.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="jasmine"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="jasmine"></script><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Tianhao&#039;Site</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">Classification</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-04-29T23:00:00.000Z" title="30/04/2023, 00:00:00">2023-04-30</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-learning/">Machine learning</a></span><span class="level-item">11 minutes read (About 1592 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/04/30/MachineLearning04/">Machine Learning - ClassificationII</a></h1><div class="content"><h3 id="1-4-Navie-Bayes"><a href="#1-4-Navie-Bayes" class="headerlink" title="1.4 Navie Bayes"></a>1.4 Navie Bayes</h3><h4 id="1-4-1-What-is-a-Navie-Bayesian"><a href="#1-4-1-What-is-a-Navie-Bayesian" class="headerlink" title="1.4.1 What is a Navie Bayesian?"></a>1.4.1 What is a Navie Bayesian?</h4><p>Naive Bayes is a probabilistic machine learning algorithm based on the Bayes Theorem, used in a wide variety of classification tasks.   </p>
<h4 id="1-4-2-What-is-Conditional-Probability"><a href="#1-4-2-What-is-Conditional-Probability" class="headerlink" title="1.4.2 What is Conditional Probability?"></a>1.4.2 What is Conditional Probability?</h4><p> <strong>Coin Toss and Fair Dice Example</strong> When you flip a fair coin, there is an equal chance of getting either heads or tails. So you can say the probability of getting heads is 50%. Similarly what would be the probability of getting a 1 when you roll a dice with 6 faces? Assuming the dice is fair, the probability of 1&#x2F;6 &#x3D; 0.166.</p>
<p><strong>Playing Cards Example</strong> If you pick a card from the deck, can you guess the probability of getting a queen given the card is a spade? Well, I have already set a condition that the card is a spade.</p>
<p>So, the <strong>denominator</strong> (eligible population) is 13 and not 52. And since there is only one queen in spades, the probability it is a queen given the card is a spade is 1&#x2F;13 &#x3D; 0.077</p>
<p>This is a classic example of conditional probability.</p>
<p>So, when you say the conditional probability of A given B, <strong>it denotes the probability of A occurring given that B has already occurred</strong>.</p>
<p>Mathematically, Conditional probability of A given B can be computed as: P(A|B) &#x3D; P(A AND B) &#x2F; P(B)</p>
<h4 id="1-4-3-The-Bayes-Rule"><a href="#1-4-3-The-Bayes-Rule" class="headerlink" title="1.4.3 The Bayes Rule"></a>1.4.3 The Bayes Rule</h4><p>​          <img src="/2023/04/30/MachineLearning04/img02.png">           <img src="/2023/04/30/MachineLearning04/img01.png"><br>​    </p>
<h4 id="1-4-4-The-Naive-Bayes"><a href="#1-4-4-The-Naive-Bayes" class="headerlink" title="1.4.4 The Naive Bayes"></a>1.4.4 The Naive Bayes</h4><p>The Bayes Rule provides the formula for the probability of Y given X.</p>
<p>But, in real-world problems, you typically have multiple X variables.</p>
<p><em>When the features are independent, we can extend the Bayes Rule to what is called Naive Bayes</em>.</p>
<p>It is called ‘<strong>Naive</strong>’ because of the naive assumption that the X’s are <strong>independent</strong> of each other.</p>
<p><img src="/2023/04/30/MachineLearning04/img03.png"></p>
<h4 id="1-4-5-Naive-Bayes-Example-by-Hand"><a href="#1-4-5-Naive-Bayes-Example-by-Hand" class="headerlink" title="1.4.5 Naive Bayes Example by Hand"></a>1.4.5 Naive Bayes Example by Hand</h4><p>Say you have 1000 fruits which could be either ‘banana’, ‘orange’ or ‘other’. These are the 3 possible classes of the Y variable. We have data for the following X variables, all of which are binary (1 or 0).</p>
<ul>
<li>Long</li>
<li>Sweet</li>
<li>Yellow</li>
</ul>
<p>The first few rows of the training dataset look like this:</p>
<p><img src="/2023/04/30/MachineLearning04/img04.png"></p>
<p>For the sake of computing the probabilities, let’s aggregate the training data to form a counts table like this.</p>
<p><img src="/2023/04/30/MachineLearning04/img05.png"></p>
<p>So the objective of the classifier is to predict if a given fruit is a ‘Banana’ or ‘Orange’ or ‘Other’ when only the 3 features (long, sweet and yellow) are known.</p>
<p>Let’s say you are given a fruit that is: Long, Sweet and Yellow, <strong>can you predict what fruit it is?</strong></p>
<p><strong>Step 1: Compute the ‘Prior’ probabilities for each of the class of fruits.</strong> That is, the proportion of each fruit class out of all the fruits from the population.</p>
<p>You can provide the ‘Priors’ from prior information about the population. Otherwise, it can be computed from the training data. For this case, let’s compute from the training data. Out of 1000 records in training data, you have 500 Bananas, 300 Oranges and 200 Others.</p>
<p>So the respective priors are 0.5, 0.3 and 0.2. P(Y&#x3D;Banana) &#x3D; 500 &#x2F; 1000 &#x3D; 0.50 P(Y&#x3D;Orange) &#x3D; 300 &#x2F; 1000 &#x3D; 0.30 P(Y&#x3D;Other) &#x3D; 200 &#x2F; 1000 &#x3D; 0.20</p>
<p><strong>Step 2: Compute the probability of evidence that goes in the denominator.</strong> This is nothing but the product of P of Xs for all X. This is an optional step because the denominator is the same for all the classes and so will not affect the probabilities. P(x1&#x3D;Long) &#x3D; 500 &#x2F; 1000 &#x3D; 0.50 P(x2&#x3D;Sweet) &#x3D; 650 &#x2F; 1000 &#x3D; 0.65 P(x3&#x3D;Yellow) &#x3D; 800 &#x2F; 1000 &#x3D; 0.80</p>
<p><strong>Step 3: Compute the probability of likelihood of evidences that goes in the numerator.</strong> It is the product of conditional probabilities of the 3 features. If you refer back to the formula, it says P(X1 |Y&#x3D;k).</p>
<p>Here X1 is ‘Long’ and k is ‘Banana’.</p>
<p>That means the probability the fruit is ‘Long’ given that it is a Banana. In the above table, you have 500 Bananas. Out of that 400 is long.</p>
<p>So, P(Long | Banana) &#x3D; 400&#x2F;500 &#x3D; 0.8. Here, I have done it for Banana alone.</p>
<p><strong>Probability of Likelihood for Banana</strong> P(x1&#x3D;Long | Y&#x3D;Banana) &#x3D; 400 &#x2F; 500 &#x3D; 0.80 P(x2&#x3D;Sweet | Y&#x3D;Banana) &#x3D; 350 &#x2F; 500 &#x3D; 0.70 P(x3&#x3D;Yellow | Y&#x3D;Banana) &#x3D; 450 &#x2F; 500 &#x3D; 0.90.</p>
<p>So, the overall probability of Likelihood of evidence for Banana &#x3D; 0.8 * 0.7 * 0.9 &#x3D; 0.504</p>
<p><strong>Step 4: Substitute all the 3 equations into the Naive Bayes formula, to get the probability that it is a banana.</strong></p>
<p><img src="/2023/04/30/MachineLearning04/img06.png"></p>
<p>Similarly, you can compute the probabilities for ‘Orange’ and ‘Other fruit’. The denominator is the same for all 3 cases, so it’s optional to compute. Clearly, Banana gets the highest probability, so that will be our predicted class.</p>
<h4 id="1-4-6-What-is-Laplace-Correction"><a href="#1-4-6-What-is-Laplace-Correction" class="headerlink" title="1.4.6  What is Laplace Correction?"></a>1.4.6  What is Laplace Correction?</h4><p>The value of P(Orange | Long, Sweet and Yellow) was <strong>zero</strong> in the above example, because, P(Long | Orange) was zero.</p>
<p>That is, there were no ‘Long’ oranges in the training data.</p>
<p>It makes sense, but when you have a model with many features, the entire probability will become zero because one of the feature’s value was zero. To avoid this, <strong>we increase the count of the variable with zero to a small value (usually 1) in the numerator, so that the overall probability doesn’t become zero</strong>. This approach is called <strong>‘Laplace Correction’</strong>.</p>
<p>Most Naive Bayes model implementations accept this or an equivalent form of correction as a parameter.</p>
<h4 id="1-4-7-Case-Classifying-News"><a href="#1-4-7-Case-Classifying-News" class="headerlink" title="1.4.7 Case: Classifying News"></a>1.4.7 Case: Classifying News</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nb_news</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 1）Acquisition of data</span></span><br><span class="line">    news = fetch_20newsgroups(subset=<span class="string">&quot;all&quot;</span>)</span><br><span class="line"><span class="comment"># 2）Dataset division</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(news.data, news.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3）Feature engineering-tfidf</span></span><br><span class="line">transfer = TfidfVectorizer()</span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4）Navie Bayes predictor process</span></span><br><span class="line">estimator = MultinomialNB()</span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5）model evaluation</span></span><br><span class="line"><span class="comment">#Method 1: Direct comparison of true and predicted values</span></span><br><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_predict:\n&quot;</span>, y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Direct comparison of true and predicted values:\n&quot;</span>, y_test == y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Method 2: Calculation of accuracy</span></span><br><span class="line">score = estimator.score(x_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuracy：\n&quot;</span>, score)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    nb_news()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="1-5-Decision-Tree"><a href="#1-5-Decision-Tree" class="headerlink" title="1.5 Decision Tree"></a>1.5 Decision Tree</h2><h4 id="1-5-1-What-are-Decision-Tree-Classifiers"><a href="#1-5-1-What-are-Decision-Tree-Classifiers" class="headerlink" title="1.5.1 What are Decision Tree Classifiers?"></a>1.5.1 What are Decision Tree Classifiers?</h4><p>Decision tree classifiers are <strong>supervised machine learning models</strong>. This means that they use prelabelled data in order to train an algorithm that can be used to make a prediction. </p>
<p>Decision tree classifiers work like <strong>flowcharts</strong>. Each <em>node</em> of a decision tree represents a decision point that splits into two leaf nodes. Each of these nodes represents the outcome of the decision and each of the decisions can also turn into decision nodes. Eventually, the different decisions will lead to a final classification.</p>
<p>The diagram below demonstrates how decision trees work to make decisions. The top node is called the <strong>root node</strong>. Each of the decision points are called <strong>decision nodes</strong>. The final decision point is referred to as a <strong>leaf node</strong>.</p>
<p><img src="/2023/04/30/MachineLearning04/img07.png"></p>
<h4 id="1-5-2-How-do-Decision-Tree-Classifiers-Work"><a href="#1-5-2-How-do-Decision-Tree-Classifiers-Work" class="headerlink" title="1.5.2 How do Decision Tree Classifiers Work?"></a>1.5.2 How do Decision Tree Classifiers Work?</h4><p>Decision trees work by splitting data into a series of binary decisions. These decisions allow you to traverse down the tree based on these decisions. You continue moving through the decisions until you end at a leaf node, which will return the predicted classification.</p>
<p>The image below shows a decision tree being used to make a classification decision:</p>
<p><img src="/2023/04/30/MachineLearning04/img08.png"></p>
<p>How does a decision tree algorithm know which decisions to make? </p>
<ul>
<li><p><strong>The definition of entropy:</strong> H is called information entropy in <strong>bits</strong>.</p>
<p><img src="/2023/04/30/MachineLearning04/img09.jpg"></p>
</li>
<li><p><strong>Information gain</strong><br>  The <strong>information gain g(D,A)</strong> of feature A on the training dataset D, is defined as the difference between the information entropy H(D) of the set D and the information conditional entropy H(D|A) of D under the given conditions of feature A</p>
</li>
</ul>
<p><img src="/2023/04/30/MachineLearning04/img10.png"></p>
<h4 id="1-5-3-Example-Bank-Loan"><a href="#1-5-3-Example-Bank-Loan" class="headerlink" title="1.5.3 Example: Bank Loan"></a>1.5.3 Example: Bank Loan</h4><p><img src="/2023/04/30/MachineLearning04/img11.png"></p>
<p><img src="/2023/04/30/MachineLearning04/img12.jpg"></p>
<p>We use A1, A2, A3, and A4 to represent age, having a job, owning a house, and loan status. The final calculation is <strong>g(D,A1) &#x3D; 0.313</strong>, <strong>g(D,A2) &#x3D; 0.324</strong>, <strong>g(D,A3) &#x3D; 0.420</strong>, g(D,A4) &#x3D; 0.363. so we choose <strong>A3</strong> as the first feature for the division. This way we can slowly build up one tree at a time</p>
<h4 id="1-5-4-Titanic-Survival-Prediction"><a href="#1-5-4-Titanic-Survival-Prediction" class="headerlink" title="1.5.4 Titanic Survival Prediction"></a>1.5.4 Titanic Survival Prediction</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, export_graphviz</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">titanic</span>():</span><br><span class="line"><span class="comment"># 1 Acquisition of data</span></span><br><span class="line">    path = <span class="string">&quot;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt&quot;</span></span><br><span class="line">    titanic = pd.read_csv(path)</span><br><span class="line">   <span class="comment"># Filtering eigenvalues and target values</span></span><br><span class="line">    x = titanic[[<span class="string">&quot;pclass&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;sex&quot;</span>]]</span><br><span class="line">    y = titanic[<span class="string">&quot;survived&quot;</span>]</span><br><span class="line">    <span class="comment"># 2、data process</span></span><br><span class="line">    <span class="comment"># 1）handle missed value</span></span><br><span class="line">    x[<span class="string">&quot;age&quot;</span>].fillna(x[<span class="string">&quot;age&quot;</span>].mean(), inplace=<span class="literal">True</span>) <span class="comment">#fill average value</span></span><br><span class="line">    <span class="comment"># 2) Convert to Dictionary</span></span><br><span class="line">    x = x.to_dict(orient=<span class="string">&quot;records&quot;</span>)</span><br><span class="line">    <span class="comment"># 3、dataset division</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">22</span>)</span><br><span class="line">    <span class="comment"># 4、Dictionary Features Extraction</span></span><br><span class="line">    transfer = DictVectorizer()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.transform(x_test)</span><br><span class="line">    <span class="comment"># 3）Decision Tree Predictor</span></span><br><span class="line">    estimator = DecisionTreeClassifier(criterion=<span class="string">&quot;entropy&quot;</span>, max_depth=<span class="number">8</span>)</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line"><span class="comment"># 4）model evaluation</span></span><br><span class="line"><span class="comment"># Method 1: Direct comparison of true and predicted values</span></span><br><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_predict:\n&quot;</span>, y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; Direct comparison of true and predicted values:\n&quot;</span>, y_test == y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Method 2: Calculating accuracy</span></span><br><span class="line">score = estimator.score(x_test, y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;accuracy：\n&quot;</span>, score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualise Decision Tree</span></span><br><span class="line">export_graphviz(estimator, out_file=<span class="string">&quot;titanic_tree.dot&quot;</span>, feature_names=transfer.get_feature_names())</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    titanic()</span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-04-14T23:00:00.000Z" title="15/04/2023, 00:00:00">2023-04-15</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-learning/">Machine learning</a></span><span class="level-item">7 minutes read (About 1012 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/04/15/MachineLearning03/">Machine Learning - ClassificationI</a></h1><div class="content"><h3 id="1-1-sklearn-Converters-and-Estimators"><a href="#1-1-sklearn-Converters-and-Estimators" class="headerlink" title="1.1 sklearn Converters and Estimators"></a>1.1 sklearn Converters and Estimators</h3><h4 id="1-1-1-Transformer-parent-class-of-feature-engineering"><a href="#1-1-1-Transformer-parent-class-of-feature-engineering" class="headerlink" title="1.1.1 Transformer - parent class of feature engineering"></a>1.1.1 Transformer - parent class of feature engineering</h4><p>Think about the steps of <strong>feature engineering</strong> done before?<br> <strong>1 Instantiate</strong> (instantiated a transformer class (Transformer). </p>
<p><strong>2 Call fit_transform</strong> (for documents to build the classification word frequency matrix, can not be called at the same time) We call the <em><strong>interface* of feature engineering as a *transformer</strong></em>, where the transformer call there are so many forms of:</p>
<ul>
<li>fit transform</li>
<li>fit</li>
<li>fit transform</li>
</ul>
<p>What is the difference between these methods? Let’s take a look at the following code to get a clear picture:</p>
<p><img src="/2023/04/15/MachineLearning03/img1.png"></p>
<p>​        </p>
<h4 id="1-1-2-Estimator-implementation-of-sklearn-machine-learning-algorithm"><a href="#1-1-2-Estimator-implementation-of-sklearn-machine-learning-algorithm" class="headerlink" title="1.1.2 Estimator (implementation of sklearn machine learning algorithm)"></a>1.1.2 Estimator (implementation of sklearn machine learning algorithm)</h4><p>​        An important role in sklearn is played by estimators<br><strong>1 Estimators for classification:</strong></p>
<ul>
<li><strong>k-nearest neighbour</strong>: <em>sklearn.neighbors</em> </li>
<li><strong>Bayesian</strong>: <em>sklearn.naive_bayes</em> </li>
<li><strong>Logistic Regression</strong>: <em>sklearn.linear_model.LogisticRegression</em> </li>
<li><strong>Decision Trees and Random Forest</strong>: <em>sklearn.linear_model. sklearn.tree</em></li>
</ul>
<p><strong>2 Estimators for regression</strong></p>
<ul>
<li><p><strong>Linear regression</strong>: <em>sklearn.linear_model.LinearRegression</em>  </p>
</li>
<li><p><strong>Ridge Regression</strong>: <em>sklearn.linear_model.Ridge</em></p>
</li>
</ul>
<p><strong>3 Estimators for Unsupervised Learning</strong></p>
<ul>
<li><strong>clustering</strong>: sklearn.linear_model. sklearn.cluster.KMeans</li>
</ul>
<h5 id="Workflow-of-Estimator"><a href="#Workflow-of-Estimator" class="headerlink" title="Workflow of Estimator:"></a>Workflow of Estimator:</h5><p><img src="/2023/04/15/MachineLearning03/img2.png"></p>
<h3 id="1-2-K-Nearest-Neighbour-Algorithm"><a href="#1-2-K-Nearest-Neighbour-Algorithm" class="headerlink" title="1.2 K-Nearest Neighbour Algorithm"></a>1.2 K-Nearest Neighbour Algorithm</h3><p>​    1.2.1 What is K-Nearest Neighbour Algorithm</p>
<p><strong>sklearn.neighbours.KNeighborsClassifier(n_neighbors&#x3D;5,algorithm&#x3D;’auto’)</strong>       </p>
<p> KNN core idea:<br>            <em><u>Your “neighbours” infer your category.</u></em><br>        1 How to determine who is a neighbour?<br>            <strong>Calculate the distance</strong>:<br>                Distance formula</p>
<ul>
<li>Euclidean distance</li>
<li>Manhattan distance Absolute distance</li>
<li>Minkowski distance</li>
</ul>
<p>​        2 Analysis of film genres</p>
<p><strong>n_neighbors: value of k</strong></p>
<p>​            k &#x3D; 1 Romance<br>​            k &#x3D; 2 Romance<br>​            ……<br>​            k &#x3D; 6 Undetermined<br>​            k &#x3D; 7 Action film</p>
<p>What if the number of recent films taken is different? What would be the result?<br>If <strong>k is too small</strong>, it is vulnerable to outliers.<br>If <strong>k is too large,</strong> the sample will be unbalanced.<br>Combine with the previous data of dating objects, analyse what kind of processing needs to be done in:</p>
<ul>
<li>Handling of dimensionless quantities</li>
<li>Standardisation</li>
</ul>
<h4 id="1-2-2-Case-1-Iris-species-prediction"><a href="#1-2-2-Case-1-Iris-species-prediction" class="headerlink" title="1.2.2 Case 1: Iris species prediction"></a>1.2.2 Case 1: Iris species prediction</h4><p>Acquisition of data</p>
<p>Data set division</p>
<p>Feature engineering - Standardisation</p>
<p>KNN predictor process</p>
<p>Model evaluation</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span>  train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span>  StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">knn_iris</span>():</span><br><span class="line">    <span class="comment">#Acquisition of data</span></span><br><span class="line">    iris  = load_iris()</span><br><span class="line">    <span class="comment">#Dataset division</span></span><br><span class="line">    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=<span class="number">22</span>)</span><br><span class="line">    <span class="comment">#Feature engineering - Standardisation</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line">    <span class="comment">#KNN predictor process</span></span><br><span class="line">    estimator = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">    estimator.fit(x_train,y_train)</span><br><span class="line">    <span class="comment">#model evaluation</span></span><br><span class="line">    <span class="comment">#Method 1: Direct comparison of true and predicted values</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y_predict:\n&quot;</span>,y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Direct comparison of true and predicted values:\n&quot;</span>,y_test == y_predict)</span><br><span class="line">    <span class="comment">#Method 2: Calculation of accuracy</span></span><br><span class="line">    score = estimator.score(x_test,y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;accuracy:\n&quot;</span>,score)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    knn_iris()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="1-2-3-K-Nearest-Neighbour-Summary"><a href="#1-2-3-K-Nearest-Neighbour-Summary" class="headerlink" title="1.2.3 K-Nearest Neighbour Summary"></a>1.2.3 K-Nearest Neighbour Summary</h4><p>​    <strong>Pros</strong>: simple, easy to understand, easy to implement, no training required<br>​    <strong>Disadvantages:</strong></p>
<ul>
<li><p>​            1) K value must be specified, improper choice of K value then classification accuracy cannot be guaranteed</p>
</li>
<li><p>​            2) Lazy algorithm, large amount of computation and <strong>memory overhead</strong> when classifying test samples</p>
<p><strong>Usage scenarios</strong>: small data scenarios, thousands to tens of thousands of samples, specific scenarios and specific business to test</p>
</li>
</ul>
<h3 id="1-3-Model-Selection-and-Tuning"><a href="#1-3-Model-Selection-and-Tuning" class="headerlink" title="1.3 Model Selection and Tuning"></a>1.3 Model Selection and Tuning</h3><h4 id="1-3-1-What-is-cross-validation"><a href="#1-3-1-What-is-cross-validation" class="headerlink" title="1.3.1 What is cross validation ?"></a>1.3.1 What is cross validation ?</h4><p><strong>Cross-validation</strong>: Take the training data and divide it into <strong>training and validation sets</strong>. Take the following figure as an <em>example</em>: The data is divided into 4 parts, one of which is used as the validation set. Then after 4 times (groups) of tests, <u><em>each time to replace a different validation set.</em></u> That is to say, the results of the 4 sets of models are obtained, and the average value is taken as the final result. Also known as <em><strong>4 fold cross validation</strong></em>.</p>
<p><img src="/2023/04/15/MachineLearning03/img3.png"></p>
<p>We knew before that the data is divided into training set and test set, but in order to get <strong>more accurate model results</strong> from training. Do the following: </p>
<ul>
<li>Training set: training set + validation set.</li>
<li>Test set: test set</li>
</ul>
<h4 id="1-3-2-Hyperparametric-Search-Grid-Search"><a href="#1-3-2-Hyperparametric-Search-Grid-Search" class="headerlink" title="1.3.2 Hyperparametric Search - Grid Search"></a>1.3.2 Hyperparametric Search - Grid Search</h4><p>Usually, there are many parameters that need to be specified manually (e.g., the value of K in the k-nearest neighbour algorithm), which are called <strong>hyperparameters</strong>. However, the manual process is tedious, so several combinations of hyperparameters need to be <strong>preset</strong> for the model. Each set of hyperparameters is evaluated using <strong>cross-validation</strong>. Finally the optimal parameter combination is selected to build the model</p>
<p><img src="/2023/04/15/MachineLearning03/img4.png"></p>
<p>  <strong>sklearn.model_selection.GridSearchCV(estimator, param_grid&#x3D;None,cv&#x3D;None)</strong></p>
<ul>
<li>Perform an exhaustive search for the specified parameter values of the estimator</li>
<li><strong>estimator</strong>: estimator object</li>
<li><strong>param_grid</strong>: estimator parameter (dict) {“n_neighbors”:[1,3,5]}</li>
<li><strong>cv</strong>: specify several folds of cross-validation</li>
<li><strong>fit()</strong>: input training data</li>
<li><strong>score()</strong>: accuracy</li>
<li><strong>Result Analysis</strong>:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Best parameters: best_params_</span><br><span class="line">Best result: best_score_</span><br><span class="line">Best estimator: best_estimator_</span><br><span class="line">Cross-validation results: cv_results_</span><br></pre></td></tr></table></figure>

<p><strong>Selective Tuning of Iris  Instances</strong> :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span>  train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span>  StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">knn_iris_gscv</span>():</span><br><span class="line">    <span class="comment">#=add grid search and cross validation</span></span><br><span class="line">    <span class="comment">#get data</span></span><br><span class="line">    iris  = load_iris()</span><br><span class="line">    <span class="comment">#split the dataset</span></span><br><span class="line">    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=<span class="number">22</span>)</span><br><span class="line">    <span class="comment">#feature engneering - Standard</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line">    <span class="comment">#KNN estimator</span></span><br><span class="line">    estimator = KNeighborsClassifier() <span class="comment">#don&#x27;t need extra K</span></span><br><span class="line">    <span class="comment">#grid search and CV</span></span><br><span class="line">    <span class="comment">#prepare data</span></span><br><span class="line">    param_data = &#123;<span class="string">&quot;n_neighbors&quot;</span>:[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">11</span>]&#125;</span><br><span class="line">    estimator = GridSearchCV(estimator,param_grid=param_data,cv=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    estimator.fit(x_train,y_train)</span><br><span class="line">    <span class="comment">#model evaluation</span></span><br><span class="line">    <span class="comment">#Method 1: Direct comparison of true and predicted values</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y_predict:\n&quot;</span>,y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Direct comparison of true and predicted values:\n&quot;</span>,y_test == y_predict)</span><br><span class="line">    <span class="comment">#Method 2: calculate the accuracy</span></span><br><span class="line">    score = estimator.score(x_test,y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;accuracy:\n&quot;</span>,score)</span><br><span class="line">    <span class="comment">#optimal parameter</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;optimal parameter:\n&quot;</span>,estimator.best_params_)</span><br><span class="line">    <span class="comment">#optimal result</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;optimal result:\n&quot;</span>,estimator.best_score_)</span><br><span class="line">    <span class="comment">#optimal estimator</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;optimal estimator:\n&quot;</span>,estimator.best_estimator_)</span><br><span class="line">    <span class="comment">#result of CV</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;result of CV:\n&quot;</span>,estimator.cv_results_)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    knn_iris_gscv()</span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-04-14T23:00:00.000Z" title="15/04/2023, 00:00:00">2023-04-15</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-learning/">Machine learning</a></span><span class="level-item">8 minutes read (About 1165 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/04/15/Activation%20Function/">Machine Learning - Classification</a></h1><div class="content"><h3 id="1-4-Feature-Preprocessing"><a href="#1-4-Feature-Preprocessing" class="headerlink" title="1.4 Feature Preprocessing"></a>1.4 Feature Preprocessing</h3><p>:  Understand numerical data, categorical data characteristics Apply <strong>MinMaxScaler</strong> and <strong>StandardScaler</strong> to implement <strong>Normalisation</strong> of feature data</p>
<p>Feature preprocessing:  the process of <strong>converting feature data into feature data</strong> more suitable for the algorithmic model by means of some transformation functions.</p>
<p><img src="/2023/04/15/Activation%20Function/img1.png"></p>
<h4 id="Why-do-we-need-to-normalise-x2F-standardise"><a href="#Why-do-we-need-to-normalise-x2F-standardise" class="headerlink" title="Why do we need to normalise&#x2F;standardise?"></a>Why do we need to normalise&#x2F;standardise?</h4><p>Large differences in the units or sizes of features, or a feature whose variance is orders of magnitude larger than other features, can easily <strong>influence (dominate) the target result</strong>, preventing some algorithms from learning other features.</p>
<h4 id="1-41-Normalisation-Maps-the-data-by-transforming-the-original-data-between-default-0-1"><a href="#1-41-Normalisation-Maps-the-data-by-transforming-the-original-data-between-default-0-1" class="headerlink" title="1.41 Normalisation: Maps the data by transforming the original data between (default [0,1])"></a>1.41 Normalisation: Maps the data by transforming the original data between (default [0,1])</h4><p><img src="/2023/04/15/Activation%20Function/img2.png"></p>
<p><strong>sklearn.preprocessing.MinMaxScaler (feature_range&#x3D;(0,1)…)</strong></p>
<ul>
<li><p><strong>MinMaxScalar.fit_transform(X)</strong></p>
<p>​      X: numpy array format data[n_samples,n_features]</p>
</li>
<li><p><strong>return value</strong>: transformed array of the same shape</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">minmax_demo</span>():</span><br><span class="line">    data = pd.read_csv(<span class="string">&quot;dating.txt&quot;</span>)</span><br><span class="line">    data = data.iloc[:,:<span class="number">3</span>]  <span class="comment">#只取数据前三列</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data=\n&quot;</span>,data)</span><br><span class="line">    transfer = MinMaxScaler() <span class="comment">#默认0-1</span></span><br><span class="line">    data_new = transfer.fit_transform(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_new=\n&quot;</span>,data_new)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    minmax_demo()</span><br></pre></td></tr></table></figure>



<p><img src="/2023/04/15/Activation%20Function/img3.png"></p>
<p>Note that the maximum and minimum values are changing, in addition, the maximum and minimum values are very <strong>susceptible</strong> to anomalies so this method is <strong>less robust</strong> and only suitable for traditional accurate small data scenarios</p>
<h4 id="1-4-2-Standardisation-Transform-the-data-to-a-mean-of-0-and-standard-deviation-of-1-by-transforming-the-original-data"><a href="#1-4-2-Standardisation-Transform-the-data-to-a-mean-of-0-and-standard-deviation-of-1-by-transforming-the-original-data" class="headerlink" title="1.4.2 Standardisation:Transform the data to a mean of 0 and standard deviation of 1 by transforming the original data."></a>1.4.2 Standardisation:Transform the data to a mean of 0 and standard deviation of 1 by transforming the original data.</h4><p>For <strong>normalisation</strong>: If outliers occur that affects the maximum and minimum values, then the result will obviously change<br>For standarlisation:If outliers occur, due to the fact that there is a With a fixed amount of data, a small number of anomalies <strong>do not have a significant effect on the mean</strong>, and thus the variance changes less.</p>
<p><img src="/2023/04/15/Activation%20Function/img4.png"></p>
<ul>
<li><strong>sklearn.preprocessing.StandardScaler()</strong><br>After processing, for each column, all data are clustered around a mean of 0 and a standard deviation of 1</li>
<li><strong>StandardScaler.fit transform(X)</strong><br>X: numpyarray format data In amples,n_features]</li>
<li><strong>return value</strong>:transformed shape of the same array</li>
</ul>
<h3 id="1-5-Feature-Reduction"><a href="#1-5-Feature-Reduction" class="headerlink" title="1.5 Feature Reduction"></a>1.5 Feature Reduction</h3><h4 id="1-5-1-Dimensionality-reduction"><a href="#1-5-1-Dimensionality-reduction" class="headerlink" title="1.5.1 Dimensionality reduction"></a>1.5.1 Dimensionality reduction</h4><p><strong>Dimensionality reduction</strong> is the process of reducing the number of random variables (features) to obtain a set of “<strong>uncorrelated</strong>“ principal variables, subject to certain constraints.</p>
<ul>
<li><strong>Reduce the number of random variables</strong></li>
</ul>
<p><img src="/2023/04/15/Activation%20Function/img5.png"></p>
<ul>
<li><strong>Correlated Feature</strong></li>
</ul>
<p>eg: Correlation between relative humidity and rainfall</p>
<p><u><em>It is because we are using features for learning when we are doing training. If there is a problem with the features themselves or if there is a strong correlation between the features, it will have a greater impact on the algorithm’s ability to learn the predictions</em></u></p>
<h4 id="1-5-2-Two-approaches-to-dimensionality-reduction"><a href="#1-5-2-Two-approaches-to-dimensionality-reduction" class="headerlink" title="1.5.2  Two approaches to dimensionality reduction"></a>1.5.2  Two approaches to dimensionality reduction</h4><ul>
<li><strong>Feature selection</strong></li>
<li><strong>Principal Component Analysis</strong> (can be understood as a form of feature extraction)</li>
</ul>
<h4 id="1-5-3-What-is-feature-selection"><a href="#1-5-3-What-is-feature-selection" class="headerlink" title="1.5.3 What is feature selection"></a>1.5.3 What is feature selection</h4><ol>
<li><strong>Definition</strong>: Data containing redundant or correlated variables (or features, attributes, metrics, etc.) designed to <strong>identify the main features</strong> from the original ones.</li>
<li><strong>Method:</strong></li>
</ol>
<ul>
<li><p><strong>Filter</strong>:  focuses on exploring the characteristics of the feature itself, the association between the feature and the feature and the target value.</p>
<ul>
<li><strong>Variance Selection</strong>: low variance feature filtering. Variance selection method: low variance feature filtering.</li>
<li><strong>Correlation Coefficient</strong></li>
</ul>
</li>
<li><p><strong>Embedded</strong>: algorithm automatically selects features (correlation between features and target value).</p>
<ul>
<li><p><strong>Decision Tree</strong>:Information Entropy, Information Gain9</p>
</li>
<li><p><strong>Regularisation</strong>:L1, L2</p>
</li>
<li><p><strong>Deep learning</strong>: convolution, etc.</p>
</li>
</ul>
</li>
</ul>
<p><strong>3. Filter</strong></p>
<p><strong>3.1</strong> <strong>Low variance feature filtering</strong><br>Remove some features with <strong>low variance</strong>, the significance of variance was covered earlier. Combined with the size of the variance to consider the perspective of this approach:</p>
<ul>
<li><strong>feature variance is small</strong>: the value of most samples of a feature is relatively similar </li>
<li><strong>feature variance is large</strong>: the value of many samples of a feature are different</li>
</ul>
<p><strong>3.1.1</strong><br><strong>sklearn.feature_selection.VarianceThreshold(threshold &#x3D; 0.0)</strong></p>
<ul>
<li>Remove all low variance features</li>
<li><strong>Variance.fit_transform(X)</strong><ul>
<li>X:numpy array format data [n_samplesn_features]</li>
<li>Return value:Features with training set variance below threshold are removed. The default value is to keep all non-zero variance features, i.e., remove all features with the same value in all samples.<br>4.1.2 Data computation<br>We perform a filter between the indicator features of certain stocks, the data is in the file “factor_regression_data&#x2F;factor_returns.csv” excluding the indexdate’return columns (these types do not match and are not required indicators).<br>These are the characteristics in total</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">variance_demo</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Low variance feature filtering</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1、load data</span></span><br><span class="line"></span><br><span class="line">​    data = pd.read_csv(<span class="string">&quot;factor_returns.csv&quot;</span>)</span><br><span class="line">​    data = data.iloc[:, <span class="number">1</span>:-<span class="number">2</span>]</span><br><span class="line">​    <span class="built_in">print</span>(<span class="string">&quot;data:\n&quot;</span>, data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、initialise transer()</span></span><br><span class="line"></span><br><span class="line">transfer = VarianceThreshold(threshold=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、use fit_transform</span></span><br><span class="line"></span><br><span class="line">data_new = transfer.fit_transform(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;data_new:\n&quot;</span>, data_new, data_new.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">r1 = pearsonr(data[<span class="string">&quot;pe_ratio&quot;</span>], data[<span class="string">&quot;pb_ratio&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;correlation coefficients：\n&quot;</span>, r1)</span><br><span class="line">r2 = pearsonr(data[<span class="string">&#x27;revenue&#x27;</span>], data[<span class="string">&#x27;total_expense&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;the correlation between revenue and total_expense：\n&quot;</span>, r2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p><strong>3.2</strong> <strong>Correlation coefficients</strong></p>
<p><img src="/2023/04/15/Activation%20Function/img6.png"></p>
<p><strong>3.21 Characteristics</strong><br>The value of correlation coefficient lies between -1 and +1 i.e. <strong>-1&lt;&#x3D;r&lt;&#x3D;1</strong>. The properties are as follows:</p>
<ul>
<li><p>When <strong>r&gt;0</strong>, it means that the two variables are positively planted off, <strong>r&lt;0</strong>, the two variables are negatively correlated </p>
</li>
<li><p>When <strong>|r|&#x3D;1</strong>, it means that the two variables are perfectly correlated, when <strong>r&#x3D;0</strong>, it means that there is no correlation between the two variables </p>
</li>
<li><p>When <strong>0&lt;|r|&lt;1</strong>, it means that there is a certain degree of correlation between the two variables. The closer r is to 1, the closer the linear relationship between the two variables; the closer r is to 0, the weaker the linear correlation between the two variables.</p>
</li>
<li><p>Generally can be divided into <strong>three levels</strong>: <strong>r &lt; 0.4</strong> for low degree of correlation; <strong>0.4&lt;|rl &lt; 0.7</strong> for significant correlation; <strong>0.7&lt;&#x3D;|r| &lt; 1</strong> for high degree of linear correlation</p>
<figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from scipy.stats import pearsonr</span><br><span class="line">x <span class="symbol">:</span>(<span class="built_in">N</span>,)array_like</span><br><span class="line">y <span class="symbol">:</span>(<span class="built_in">N</span>,) array_like Retur<span class="symbol">ns:</span> (<span class="built_in">Pearson</span>&#x27;s correlation coefficient, p-<span class="built_in">value</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/2023/04/15/Activation%20Function/img7.png"></p>
</li>
</ul>
<h3 id="1-6-PCA"><a href="#1-6-PCA" class="headerlink" title="1.6 PCA"></a>1.6 PCA</h3><h4 id="1-6-1-What-is-Principal-Component-Analysis-PCA"><a href="#1-6-1-What-is-Principal-Component-Analysis-PCA" class="headerlink" title="1.6.1 What is Principal Component Analysis (PCA)?"></a>1.6.1 What is Principal Component Analysis (PCA)?</h4><p><strong>Definition</strong>:The process of transforming high-dimensional data into low-dimensional data, in which the original data may be discarded and new variables may be created.<br><strong>Role</strong>: is the dimensionality of the data compression, as far as possible to reduce the dimensionality of the original data (complexity), the loss of a small amount of information.<br><strong>Application</strong>: regression analysis or cluster analysis.</p>
<p>​                        Given:</p>
<img src="/2023/04/15/Activation%20Function/img8.png" style="zoom:67%;">

<p><strong>Now place the data in a two-dimensional spatial Cartesian coordinate system</strong></p>
<img src="/2023/04/15/Activation%20Function/img9.png" style="zoom:67%;">

<p><strong>Now let’s figure out how to reduce the two-dimensional data to one dimension (a straight line)</strong></p>
<img src="/2023/04/15/Activation%20Function/img10.png" style="zoom:67%;">

<p> <strong>We can see that although it is reduced to one dimension, there is a loss of data from the original five points to three, and so:</strong></p>
<img src="/2023/04/15/Activation%20Function/img11.png" style="zoom:67%;">

<p><strong>sklearn.decomposition.PCA(n_components&#x3D;None</strong></p>
<ul>
<li><strong>Decompose data into lower dimensional spaces</strong></li>
<li><strong>n_components</strong></li>
</ul>
<p>​       Fractional: Indicates what percent of the information is retained</p>
<p>​     Integer: Reduces to how many features. </p>
<ul>
<li>**Pca.fit_transform(X)**X:numpy array format data [n_samples,n_features]</li>
<li><strong>Return value</strong>: array of the specified dimension after transformation</li>
</ul>
<p><strong>For example:</strong></p>
<img src="/2023/04/15/Activation%20Function/img12.png" style="zoom:67%;">

<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition  import PCA</span><br><span class="line"></span><br><span class="line">def pca_demo():</span><br><span class="line">    data = <span class="string">[[2,8,4,5],[6,3,0,8],[5,4,9,1]]</span></span><br><span class="line">    transfer = PCA(n_components=<span class="number">2</span>) #reduce to <span class="number">2</span> features</span><br><span class="line">    data_new = transfer.fit_transform(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;data_new:\n&quot;</span>,data_new)</span><br><span class="line">    <span class="keyword">return</span> None</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    pca_demo()</span><br></pre></td></tr></table></figure>

<p><img src="/2023/04/15/Activation%20Function/img13.png"></p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">32</span></span></a></li><li><a class="level is-mobile" href="/categories/Deep-Learning/"><span class="level-start"><span class="level-item">Deep Learning</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/Machine-learning/"><span class="level-start"><span class="level-item">Machine learning</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Video-Compression/"><span class="level-start"><span class="level-item">Video Compression</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-29T23:00:00.000Z">2023-07-30</time></p><p class="title"><a href="/2023/07/30/ModelCompressionPruning/">Model Compression - Pruning</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-07-14T23:00:00.000Z">2023-07-15</time></p><p class="title"><a href="/2023/07/15/BasicsofNeuralNetworksII/">Basics of Neural NetworksII</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-29T23:00:00.000Z">2023-06-30</time></p><p class="title"><a href="/2023/06/30/BasicsofNeuralNetworksI/">Basics of Neural NetworksI</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-29T23:00:00.000Z">2023-06-30</time></p><p class="title"><a href="/2023/06/30/Activation/">Activation Function</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-29T23:00:00.000Z">2023-06-30</time></p><p class="title"><a href="/2023/06/30/Quantization01/">Model compression-QuantizationI</a></p><p class="categories"><a href="/categories/Deep-Learning/">Deep Learning</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Array/"><span class="tag">Array</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BackTracking/"><span class="tag">BackTracking</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BinarySearchTree/"><span class="tag">BinarySearchTree</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BinaryTree/"><span class="tag">BinaryTree</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C-C/"><span class="tag">C/C++</span><span class="tag">32</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Classification/"><span class="tag">Classification</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Combination/"><span class="tag">Combination</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CrossValidation/"><span class="tag">CrossValidation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DecisionTree/"><span class="tag">DecisionTree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepLearning/"><span class="tag">DeepLearning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DoublePointers/"><span class="tag">DoublePointers</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DynamicProgramming/"><span class="tag">DynamicProgramming</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeatureExtraction/"><span class="tag">FeatureExtraction</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/FeaturePreprocessing/"><span class="tag">FeaturePreprocessing</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GreedyAlgorithm/"><span class="tag">GreedyAlgorithm</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Iteration/"><span class="tag">Iteration</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">30</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KNN/"><span class="tag">KNN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KnowledgeDistillation/"><span class="tag">KnowledgeDistillation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinearRegression/"><span class="tag">LinearRegression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LinkedList/"><span class="tag">LinkedList</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linkedlist/"><span class="tag">Linkedlist</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NaiveBayes/"><span class="tag">NaiveBayes</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pruning/"><span class="tag">Pruning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Regression/"><span class="tag">Regression</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Resursion/"><span class="tag">Resursion</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RidgeRegression/"><span class="tag">RidgeRegression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/String/"><span class="tag">String</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/TraversalOfBinaryTree/"><span class="tag">TraversalOfBinaryTree</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VideoCompression/"><span class="tag">VideoCompression</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deeplearning/"><span class="tag">deeplearning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/loop-invariant/"><span class="tag">loop invariant</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machinelearning/"><span class="tag">machinelearning</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/model-compression/"><span class="tag">model compression</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/modelcompression/"><span class="tag">modelcompression</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/quantization/"><span class="tag">quantization</span><span class="tag">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="Tianhao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Tianhao</p><p class="is-size-6 is-block">a MATH and CS student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Bristol, United Kingdom</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">46</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">38</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/jasminepp/jasminepp.github.io" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/jasminepp/jasminepp.github.io"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">September 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Tianhao&#039;Site</a><p class="is-size-7"><span>&copy; 2023 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="jasmine"></script><script src="jasmine"></script><script src="jasmine" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>